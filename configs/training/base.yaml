training:
  seed: 42
  learning_rate: 5.0e-5
  batch_size: 8
  epochs: 3
  max_epochs: ${training.epochs}
  device: cpu
  dtype: float32
  gradient_accumulation_steps: 1
  gradient_accumulation: ${training.gradient_accumulation_steps}
  grad_accum: ${training.gradient_accumulation_steps}
  mixed_precision: false
  optimizer:
    name: adamw_torch
    weight_decay: 0.01
    betas: [0.9, 0.999]
    eps: 1.0e-08
  scheduler:
    name: linear
    warmup_steps: 0
    num_cycles: 1.0
  checkpoint:
    directory: .codex/checkpoints
    best_k: 2
    monitor: val_loss
    mode: min
    save_optimizer: true
  checkpoint_dir: ${training.checkpoint.directory}
  checkpoint_keep: ${training.checkpoint.best_k}
  logging:
    enable_tensorboard: false
    tensorboard_log_dir: .codex/tensorboard
    enable_mlflow: false
    mlflow_run_name: codex-training
    mlflow_tracking_uri: null
    mlflow_offline: true
  tensorboard: ${training.logging.enable_tensorboard}
  mlflow_enable: ${training.logging.enable_mlflow}
  trainer:
    epochs: ${training.epochs}
    gradient_accumulation_steps: ${training.gradient_accumulation_steps}
    mixed_precision: ${training.mixed_precision}
    max_grad_norm: null
    log_every_n_steps: 0
    checkpoint: ${training.checkpoint}
    logging: ${training.logging}

# Backwards compatibility aliases
epochs: ${training.epochs}
batch_size: ${training.batch_size}
learning_rate: ${training.learning_rate}
gradient_accumulation_steps: ${training.gradient_accumulation_steps}
mixed_precision: ${training.mixed_precision}
checkpoint_dir: ${training.checkpoint.directory}
keep_best_k: ${training.checkpoint.best_k}
