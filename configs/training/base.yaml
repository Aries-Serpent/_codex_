training:
  seed: 42
  learning_rate: 5.0e-5
  batch_size: 8
  epochs: 3
  max_epochs: ${training.epochs}
  device: cpu
  dtype: float32
  deterministic: true
  optimizer:
    name: adamw_torch
    weight_decay: 0.01
    betas: [0.9, 0.999]
    eps: 1.0e-08
  scheduler:
    name: linear
    warmup_steps: 0
    num_cycles: 1.0
  warmup_steps: 0
  gradient_accumulation: 1
  grad_accum: 1
  mixed_precision: false
  tensorboard: true
  mlflow_enable: false
  model: minilm
  output_dir: runs/default
  checkpoint:
    dir: runs/default/checkpoints
    every_n_steps: 50
    keep_best_k: 1
  checkpoint_dir: runs/default/checkpoints
  checkpoint_every_n_steps: 50
  checkpoint_keep: 1
  dataset:
    train_path: data/train_samples.jsonl
    eval_path: null
    format: jsonl
    train_texts: []
    eval_texts: []
    generate_manifest: false
    split_ratio: null
  lora:
    enable: false
    r: 8
    lora_alpha: 16
    lora_dropout: 0.05
    task_type: CAUSAL_LM
  eval_split: null
  logging:
    enable_tensorboard: true
    mlflow_enable: false

epochs: ${training.max_epochs}
batch_size: ${training.batch_size}
learning_rate: ${training.learning_rate}
seed: ${training.seed}
deterministic: ${training.deterministic}
gradient_accumulation_steps: ${training.gradient_accumulation}
mixed_precision: ${training.mixed_precision}

checkpoint:
  directory: ${training.checkpoint.dir}
  keep_best_k: ${training.checkpoint.keep_best_k}
  maximize_metric: false

logging:
  enable_tensorboard: ${training.logging.enable_tensorboard}
  enable_mlflow: ${training.logging.mlflow_enable}
