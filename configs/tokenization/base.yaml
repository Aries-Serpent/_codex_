# Tokenizer pipeline defaults
# Defines dataset layout, cache directories, normalization, and training knobs.
datasets:
  - id: train
    path: data/tokenizer/train.txt
    checksum: null
  - id: validation
    path: data/tokenizer/validation.txt
    checksum: null
cache:
  dir: artifacts/tokenizers
  manifest_name: manifest.json
normalizer:
  unicode_form: NFKC
  lowercase: true
  strip_accents: false
  custom_rules: []
tokenizer:
  backend: sentencepiece
  model_type: unigram
  vocab_size: 32000
  min_frequency: 2
  limit_alphabet: 6000
  special_tokens:
    - "<unk>"
    - "<pad>"
    - "<s>"
    - "</s>"
    - "<mask>"
training:
  seed: 1337
  shuffle: true
  batch_size: 2000
  max_sentence_length: 256
  character_coverage: 0.9995
  dropout: 0.0
  num_workers: 4
  pretokenizer: whitespace
  
