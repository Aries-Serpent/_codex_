# Offline TinyLLaMA model configuration.
# Resolves local checkpoints for the `tinyllama-offline` registry entry.
# Expected local layout (relative to the repo root):
#   artifacts/models/tinyllama/{config.json,pytorch_model.bin,tokenizer.model,...}
defaults:
  - override /model: base

model:
  name: tinyllama-offline
  local_files_only: true
  local_path: ${oc.env:CODEX_ML_TINYLLAMA_PATH,${oc.env:CODEX_ML_OFFLINE_MODELS_DIR,${hydra:runtime.cwd}/artifacts/models/tinyllama}}
  dtype: float32
  device: cpu
