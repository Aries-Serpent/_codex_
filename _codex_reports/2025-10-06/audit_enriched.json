{
  "files_count": 4028,
  "root": "/workspace/_codex_",
  "search_hits": {
    "checkpoints": [
      "src/utils/checkpoint.py"
    ],
    "ci": [
      ".github/CODEOWNERS",
      ".github/Copilot.md",
      ".github/PULL_REQUEST_TEMPLATE.md",
      ".github/README.md",
      ".github/_workflows_disabled/README.md",
      ".github/_workflows_disabled/_policy.yml",
      ".github/_workflows_disabled/lint.yml",
      ".github/_workflows_disabled/manual_ci.yml",
      ".github/_workflows_disabled/nightly.yml.disabled",
      ".github/_workflows_disabled/release-upload.yml",
      ".github/_workflows_disabled/vuln_scan.yml.disabled",
      ".github/chatmodes/template.chatmode.md",
      ".github/copilot-instructions.md",
      ".github/docs/CrossRepoPatterns_Analysis.md",
      ".github/docs/DeepResearch_GitHooksAutoFix.md",
      ".github/docs/Implementation_LLMAutoFixHook.md",
      ".github/docs/PoC_Repo_Layout_Copilot.md",
      ".github/workflows/ci.yml.disabled",
      ".github/workflows/validate.yml.disabled",
      "noxfile.py",
      "tox.ini"
    ],
    "docker": [
      "Dockerfile"
    ],
    "logging": [
      "src/utils/logging_factory.py"
    ],
    "model": [
      ".venv/lib/python3.12/site-packages/pip/_internal/models/__init__.py",
      ".venv/lib/python3.12/site-packages/pip/_internal/models/candidate.py",
      ".venv/lib/python3.12/site-packages/pip/_internal/models/direct_url.py",
      ".venv/lib/python3.12/site-packages/pip/_internal/models/format_control.py",
      ".venv/lib/python3.12/site-packages/pip/_internal/models/index.py",
      ".venv/lib/python3.12/site-packages/pip/_internal/models/installation_report.py",
      ".venv/lib/python3.12/site-packages/pip/_internal/models/link.py",
      ".venv/lib/python3.12/site-packages/pip/_internal/models/pylock.py",
      ".venv/lib/python3.12/site-packages/pip/_internal/models/scheme.py",
      ".venv/lib/python3.12/site-packages/pip/_internal/models/search_scope.py",
      ".venv/lib/python3.12/site-packages/pip/_internal/models/selection_prefs.py",
      ".venv/lib/python3.12/site-packages/pip/_internal/models/target_python.py",
      ".venv/lib/python3.12/site-packages/pip/_internal/models/wheel.py",
      ".venv/lib/python3.12/site-packages/pip/_vendor/pygments/modeline.py",
      ".venv/lib/python3.12/site-packages/pip/_vendor/requests/models.py",
      "agents/codex_client/codex_client/models.py",
      "examples/plugins/toy_model/__init__.py",
      "services/ita/app/models.py",
      "src/codex_ml/interfaces/reward_model.py",
      "src/codex_ml/modeling/__init__.py",
      "src/codex_ml/modeling/codex_model_loader.py",
      "src/codex_ml/modeling/factory.py",
      "src/codex_ml/models/__init__.py",
      "src/codex_ml/models/activations.py",
      "src/codex_ml/models/decoder_only.py",
      "src/codex_ml/models/factory.py",
      "src/codex_ml/models/generate.py",
      "src/codex_ml/models/loader_registry.py",
      "src/codex_ml/models/minilm.py",
      "src/codex_ml/models/offline_tiny.py",
      "src/codex_ml/models/peft_hooks.py",
      "src/codex_ml/models/registry.py",
      "src/codex_ml/models/utils/__init__.py",
      "src/codex_ml/models/utils/peft.py",
      "src/codex_ml/registry/models.py",
      "src/codex_ml/reward_models/__init__.py",
      "src/codex_ml/reward_models/rlhf.py",
      "src/codex_ml/reward_models/simple.py",
      "src/codex_ml/utils/modeling.py",
      "tests/codex_ml/test_model_factory.py",
      "tests/modeling/conftest.py",
      "tests/modeling/test_decoder_only.py",
      "tests/modeling/test_model_registry.py",
      "tests/models/conftest.py",
      "tests/models/test_lora_integration.py",
      "tests/models/test_models_registry_api.py",
      "tests/models/test_peft_optional.py",
      "tests/models/test_peft_smoke.py",
      "tests/models/test_registry_dtype.py",
      "tests/reward_models/test_length_reward_model.py",
      "tests/test_model_factory.py",
      "tests/test_model_forward.py",
      "tests/test_model_loader.py",
      "tests/test_model_registry.py",
      "tests/test_model_registry_invalid.py",
      "tests/test_modeling_utils.py",
      "tests/test_reward_models_rlhf.py",
      "tests/utils/test_modeling.py",
      "tools/codex_apply_modeling_monitoring_api.py"
    ],
    "notebooks": [
      "examples/notebooks/chat_finetune.ipynb",
      "examples/notebooks/demo_infer.ipynb",
      "examples/notebooks/demo_train_eval.ipynb",
      "examples/notebooks/tokenizer_quickstart.ipynb",
      "notebooks/gpu_training_example.ipynb",
      "notebooks/quick_start.ipynb"
    ],
    "requirements": [
      "pyproject.toml",
      "requirements.txt"
    ],
    "tests": [
      "tests/__init__.py",
      "tests/__pycache__/__init__.cpython-312.pyc",
      "tests/__pycache__/_codex_introspect.cpython-312.pyc",
      "tests/__pycache__/conftest.cpython-312-pytest-8.4.1.pyc",
      "tests/__pycache__/test_activations.cpython-312-pytest-8.4.1.pyc",
      "tests/__pycache__/test_batch_metrics_shim.cpython-312-pytest-8.4.1.pyc",
      "tests/__pycache__/test_bestk_retention.cpython-312-pytest-8.4.1.pyc",
      "tests/__pycache__/test_callbacks.cpython-312-pytest-8.4.1.pyc",
      "tests/__pycache__/test_chat_env_cleanup.cpython-312-pytest-8.4.1.pyc",
      "tests/__pycache__/test_chat_session.cpython-312-pytest-8.4.1.pyc",
      "tests/__pycache__/test_chat_session_exit.cpython-312-pytest-8.4.1.pyc",
      "tests/__pycache__/test_checkpoint_bundle.cpython-312-pytest-8.4.1.pyc",
      "tests/__pycache__/test_checkpoint_checksum.cpython-312-pytest-8.4.1.pyc",
      "tests/__pycache__/test_checkpoint_checksum_and_retention.cpython-312-pytest-8.4.1.pyc",
      "tests/__pycache__/test_checkpoint_commit_meta.cpython-312-pytest-8.4.1.pyc",
      "tests/__pycache__/test_checkpoint_corrupt_load.cpython-312-pytest-8.4.1.pyc",
      "tests/__pycache__/test_checkpoint_corruption.cpython-312-pytest-8.4.1.pyc",
      "tests/__pycache__/test_checkpoint_integrity.cpython-312-pytest-8.4.1.pyc",
      "tests/__pycache__/test_checkpoint_integrity_corruption.cpython-312-pytest-8.4.1.pyc",
      "tests/__pycache__/test_checkpoint_manager.cpython-312-pytest-8.4.1.pyc",
      "tests/__pycache__/test_checkpoint_metadata.cpython-312-pytest-8.4.1.pyc",
      "tests/__pycache__/test_checkpoint_provenance.cpython-312-pytest-8.4.1.pyc",
      "tests/__pycache__/test_checkpoint_restore_rng_torch.cpython-312-pytest-8.4.1.pyc",
      "tests/__pycache__/test_checkpoint_roundtrip.cpython-312-pytest-8.4.1.pyc",
      "tests/__pycache__/test_checkpoint_save_resume.cpython-312-pytest-8.4.1.pyc",
      "tests/__pycache__/test_checkpoint_sha_rng.cpython-312-pytest-8.4.1.pyc",
      "tests/__pycache__/test_checkpoint_system_meta.cpython-312-pytest-8.4.1.pyc",
      "tests/__pycache__/test_checkpoint_util.cpython-312-pytest-8.4.1.pyc",
      "tests/__pycache__/test_checkpointing.cpython-312-pytest-8.4.1.pyc",
      "tests/__pycache__/test_checkpointing_utils_extra.cpython-312-pytest-8.4.1.pyc",
      "tests/__pycache__/test_checksum_sha256.cpython-312-pytest-8.4.1.pyc",
      "tests/__pycache__/test_ci_smoke.cpython-312-pytest-8.4.1.pyc",
      "tests/__pycache__/test_cli.cpython-312-pytest-8.4.1.pyc",
      "tests/__pycache__/test_cli_help.cpython-312-pytest-8.4.1.pyc",
      "tests/__pycache__/test_cli_help_paths.cpython-312-pytest-8.4.1.pyc",
      "tests/__pycache__/test_cli_hydra_validation.cpython-312-pytest-8.4.1.pyc",
      "tests/__pycache__/test_cli_pool.cpython-312-pytest-8.4.1.pyc",
      "tests/__pycache__/test_cli_smoke.cpython-312-pytest-8.4.1.pyc",
      "tests/__pycache__/test_cli_train_command.cpython-312-pytest-8.4.1.pyc",
      "tests/__pycache__/test_codex_best_effort.cpython-312-pytest-8.4.1.pyc",
      "tests/__pycache__/test_codex_cli.cpython-312-pytest-8.4.1.pyc",
      "tests/__pycache__/test_codex_maintenance.cpython-312-pytest-8.4.1.pyc",
      "tests/__pycache__/test_codex_ml_cli_version.cpython-312-pytest-8.4.1.pyc",
      "tests/__pycache__/test_codex_run_tasks.cpython-312-pytest-8.4.1.pyc",
      "tests/__pycache__/test_codex_sequence_validations.cpython-312-pytest-8.4.1.pyc",
      "tests/__pycache__/test_config_validation.cpython-312-pytest-8.4.1.pyc",
      "tests/__pycache__/test_conversation_logger.cpython-312-pytest-8.4.1.pyc",
      "tests/__pycache__/test_crc32_file.cpython-312-pytest-8.4.1.pyc",
      "tests/__pycache__/test_data_cache_sharding.cpython-312-pytest-8.4.1.pyc",
      "tests/__pycache__/test_data_loader.cpython-312-pytest-8.4.1.pyc",
      "tests/__pycache__/test_data_loaders.cpython-312-pytest-8.4.1.pyc",
      "tests/__pycache__/test_data_loaders_extended.cpython-312-pytest-8.4.1.pyc",
      "tests/__pycache__/test_data_split_utils.cpython-312-pytest-8.4.1.pyc",
      "tests/__pycache__/test_data_splits.cpython-312-pytest-8.4.1.pyc",
      "tests/__pycache__/test_data_utils.cpython-312-pytest-8.4.1.pyc",
      "tests/__pycache__/test_dataset_checksums.cpython-312-pytest-8.4.1.pyc",
      "tests/__pycache__/test_dataset_manifest.cpython-312-pytest-8.4.1.pyc",
      "tests/__pycache__/test_db_utils.cpython-312-pytest-8.4.1.pyc",
      "tests/__pycache__/test_db_utils_table_name.cpython-312-pytest-8.4.1.pyc",
      "tests/__pycache__/test_deep_research_task_process.cpython-312-pytest-8.4.1.pyc",
      "tests/__pycache__/test_deploy_codex_pipeline.cpython-312-pytest-8.4.1.pyc",
      "tests/__pycache__/test_deterministic_split.cpython-312-pytest-8.4.1.pyc",
      "tests/__pycache__/test_docs_examples_paths.cpython-312-pytest-8.4.1.pyc",
      "tests/__pycache__/test_engine_hf_trainer_lora.cpython-312-pytest-8.4.1.pyc",
      "tests/__pycache__/test_env_logging.cpython-312-pytest-8.4.1.pyc",
      "tests/__pycache__/test_error_log.cpython-312-pytest-8.4.1.pyc",
      "tests/__pycache__/test_eval_loop_cpu.cpython-312-pytest-8.4.1.pyc",
      "tests/__pycache__/test_eval_loop_minimal.cpython-312-pytest-8.4.1.pyc",
      "tests/__pycache__/test_eval_loop_resilience.cpython-312-pytest-8.4.1.pyc",
      "tests/__pycache__/test_eval_runner.cpython-312-pytest-8.4.1.pyc",
      "tests/__pycache__/test_eval_with_metrics.cpython-312-pytest-8.4.1.pyc",
      "tests/__pycache__/test_evaluate_cli.cpython-312-pytest-8.4.1.pyc",
      "tests/__pycache__/test_evaluation.cpython-312-pytest-8.4.1.pyc",
      "tests/__pycache__/test_export.cpython-312-pytest-8.4.1.pyc",
      "tests/__pycache__/test_fences_tool.cpython-312-pytest-8.4.1.pyc",
      "tests/__pycache__/test_fetch_messages.cpython-312-pytest-8.4.1.pyc",
      "tests/__pycache__/test_fetch_messages_missing_db.cpython-312-pytest-8.4.1.pyc",
      "tests/__pycache__/test_git_tag.cpython-312-pytest-8.4.1.pyc",
      "tests/__pycache__/test_gradient_accumulation_equivalence.cpython-312-pytest-8.4.1.pyc",
      "tests/__pycache__/test_gradient_accumulation_tail_flush.cpython-312-pytest-8.4.1.pyc",
      "tests/__pycache__/test_hf_loader_amp.cpython-312-pytest-8.4.1.pyc",
      "tests/__pycache__/test_hf_loader_peft_guard.cpython-312-pytest-8.4.1.pyc",
      "tests/__pycache__/test_hf_loader_registry.cpython-312-pytest-8.4.1.pyc",
      "tests/__pycache__/test_hydra_cli.cpython-312-pytest-8.4.1.pyc",
      "tests/__pycache__/test_hydra_compose.cpython-312-pytest-8.4.1.pyc",
      "tests/__pycache__/test_import_codex.cpython-312-pytest-8.4.1.pyc",
      "tests/__pycache__/test_import_ndjson.cpython-312-pytest-8.4.1.pyc",
      "tests/__pycache__/test_import_ndjson_cli.cpython-312-pytest-8.4.1.pyc",
      "tests/__pycache__/test_import_ndjson_dedup.cpython-312-pytest-8.4.1.pyc",
      "tests/__pycache__/test_ingestion_auto_encoding.cpython-312-pytest-8.4.1.pyc",
      "tests/__pycache__/test_ingestion_encoding_coverage.cpython-312-pytest-8.4.1.pyc",
      "tests/__pycache__/test_ingestion_encodings_matrix.cpython-312-pytest-8.4.1.pyc",
      "tests/__pycache__/test_ingestion_family_encoding.cpython-312-pytest-8.4.1.pyc",
      "tests/__pycache__/test_ingestion_io.cpython-312-pytest-8.4.1.pyc",
      "tests/__pycache__/test_ingestion_read_text.cpython-312-pytest-8.4.1.pyc",
      "tests/__pycache__/test_ingestion_seeded_shuffle.cpython-312-pytest-8.4.1.pyc",
      "tests/__pycache__/test_ingestion_split_cache.cpython-312-pytest-8.4.1.pyc",
      "tests/__pycache__/test_interface_loader_env.cpython-312-pytest-8.4.1.pyc",
      "tests/__pycache__/test_interfaces_compat.cpython-312-pytest-8.4.1.pyc",
      "tests/__pycache__/test_interfaces_hf_tokenizer.cpython-312-pytest-8.4.1.pyc",
      "tests/__pycache__/test_iter_jsonl.cpython-312-pytest-8.4.1.pyc",
      "tests/__pycache__/test_json_report.cpython-312-pytest-8.4.1.pyc",
      "tests/__pycache__/test_jsonl_writer.cpython-312-pytest-8.4.1.pyc",
      "tests/__pycache__/test_label_policy_lint.cpython-312-pytest-8.4.1.pyc",
      "tests/__pycache__/test_loader_registry.cpython-312-pytest-8.4.1.pyc",
      "tests/__pycache__/test_loaders.cpython-312-pytest-8.4.1.pyc",
      "tests/__pycache__/test_log_adapters.cpython-312-pytest-8.4.1.pyc",
      "tests/__pycache__/test_logging_bootstrap.cpython-312-pytest-8.4.1.pyc",
      "tests/__pycache__/test_logging_viewer_cli.cpython-312-pytest-8.4.1.pyc",
      "tests/__pycache__/test_make_generator_worker_seed.cpython-312-pytest-8.4.1.pyc",
      "tests/__pycache__/test_metric_curves.cpython-312-pytest-8.4.1.pyc",
      "tests/__pycache__/test_metric_registry.cpython-312-pytest-8.4.1.pyc",
      "tests/__pycache__/test_metrics.cpython-312-pytest-8.4.1.pyc",
      "tests/__pycache__/test_metrics_correctness.cpython-312-pytest-8.4.1.pyc",
      "tests/__pycache__/test_metrics_logging.cpython-312-pytest-8.4.1.pyc",
      "tests/__pycache__/test_metrics_perplexity.cpython-312-pytest-8.4.1.pyc",
      "tests/__pycache__/test_metrics_registry_alias.cpython-312-pytest-8.4.1.pyc",
      "tests/__pycache__/test_metrics_tb.cpython-312-pytest-8.4.1.pyc",
      "tests/__pycache__/test_metrics_token_accuracy.cpython-312-pytest-8.4.1.pyc",
      "tests/__pycache__/test_microhelpers.cpython-312-pytest-8.4.1.pyc",
      "tests/__pycache__/test_minilm_forward.cpython-312-pytest-8.4.1.pyc",
      "tests/__pycache__/test_mlflow_adapter.cpython-312-pytest-8.4.1.pyc",
      "tests/__pycache__/test_mlflow_step_logging.cpython-312-pytest-8.4.1.pyc",
      "tests/__pycache__/test_mlflow_utils.cpython-312-pytest-8.4.1.pyc",
      "tests/__pycache__/test_mlflow_utils_root.cpython-312-pytest-8.4.1.pyc",
      "tests/__pycache__/test_model_factory.cpython-312-pytest-8.4.1.pyc",
      "tests/__pycache__/test_model_forward.cpython-312-pytest-8.4.1.pyc",
      "tests/__pycache__/test_model_loader.cpython-312-pytest-8.4.1.pyc",
      "tests/__pycache__/test_model_registry.cpython-312-pytest-8.4.1.pyc",
      "tests/__pycache__/test_model_registry_invalid.cpython-312-pytest-8.4.1.pyc",
      "tests/__pycache__/test_modeling_utils.cpython-312-pytest-8.4.1.pyc",
      "tests/__pycache__/test_monitoring.cpython-312-pytest-8.4.1.pyc",
      "tests/__pycache__/test_monitoring_cli.cpython-312-pytest-8.4.1.pyc",
      "tests/__pycache__/test_monitoring_thread.cpython-312-pytest-8.4.1.pyc",
      "tests/__pycache__/test_ndjson_db_parity.cpython-312-pytest-8.4.1.pyc",
      "tests/__pycache__/test_ndjson_logger.cpython-312-pytest-8.4.1.pyc",
      "tests/__pycache__/test_ndjson_logging.cpython-312-pytest-8.4.1.pyc",
      "tests/__pycache__/test_ndjson_writer.cpython-312-pytest-8.4.1.pyc",
      "tests/__pycache__/test_nox_tests_delegation.cpython-312-pytest-8.4.1.pyc",
      "tests/__pycache__/test_offline_repo_auditor.cpython-312-pytest-8.4.1.pyc",
      "tests/__pycache__/test_parse_when.cpython-312-pytest-8.4.1.pyc",
      "tests/__pycache__/test_peft_adapter.cpython-312-pytest-8.4.1.pyc",
      "tests/__pycache__/test_peft_integration.cpython-312-pytest-8.4.1.pyc",
      "tests/__pycache__/test_pipeline.cpython-312-pytest-8.4.1.pyc",
      "tests/__pycache__/test_plugin_loader.cpython-312-pytest-8.4.1.pyc",
      "tests/__pycache__/test_plugins_cli.cpython-312-pytest-8.4.1.pyc",
      "tests/__pycache__/test_precommit_config_exists.cpython-312-pytest-8.4.1.pyc",
      "tests/__pycache__/test_pyproject_metadata.cpython-312-pytest-8.4.1.pyc",
      "tests/__pycache__/test_query_logs_build_query.cpython-312-pytest-8.4.1.pyc",
      "tests/__pycache__/test_query_logs_tail.cpython-312-pytest-8.4.1.pyc",
      "tests/__pycache__/test_readme_examples.cpython-312-pytest-8.4.1.pyc",
      "tests/__pycache__/test_readme_has_quickstart.cpython-312-pytest-8.4.1.pyc",
      "tests/__pycache__/test_registry.cpython-312-pytest-8.4.1.pyc",
      "tests/__pycache__/test_repo_option_a.cpython-312-pytest-8.4.1.pyc",
      "tests/__pycache__/test_repro_branches.cpython-312-pytest-8.4.1.pyc",
      "tests/__pycache__/test_repro_cli.cpython-312-pytest-8.4.1.pyc",
      "tests/__pycache__/test_repro_determinism.cpython-312-pytest-8.4.1.pyc",
      "tests/__pycache__/test_repro_helper.cpython-312-pytest-8.4.1.pyc",
      "tests/__pycache__/test_repro_rng_roundtrip.cpython-312-pytest-8.4.1.pyc",
      "tests/__pycache__/test_repro_seed_consistency.cpython-312-pytest-8.4.1.pyc",
      "tests/__pycache__/test_reproducibility.cpython-312-pytest-8.4.1.pyc",
      "tests/__pycache__/test_requirements_lock.cpython-312-pytest-8.4.1.pyc",
      "tests/__pycache__/test_resume.cpython-312-pytest-8.4.1.pyc",
      "tests/__pycache__/test_resume_training.cpython-312-pytest-8.4.1.pyc",
      "tests/__pycache__/test_reward_models_rlhf.cpython-312-pytest-8.4.1.pyc",
      "tests/__pycache__/test_run_functional_training_tokenizer.cpython-312-pytest-8.4.1.pyc",
      "tests/__pycache__/test_runner_doctor.cpython-312-pytest-8.4.1.pyc",
      "tests/__pycache__/test_safety.cpython-312-pytest-8.4.1.pyc",
      "tests/__pycache__/test_safety_filters_integration.cpython-312-pytest-8.4.1.pyc",
      "tests/__pycache__/test_safety_import_no_crash.cpython-312-pytest-8.4.1.pyc",
      "tests/__pycache__/test_search_providers.cpython-312-pytest-8.4.1.pyc",
      "tests/__pycache__/test_secrets_scanner.cpython-312-pytest-8.4.1.pyc",
      "tests/__pycache__/test_seeded_shuffle.cpython-312-pytest-8.4.1.pyc",
      "tests/__pycache__/test_semparser.cpython-312-pytest-8.4.1.pyc",
      "tests/__pycache__/test_session_hooks.cpython-312-pytest-8.4.1.pyc",
      "tests/__pycache__/test_session_hooks_warnings.cpython-312-pytest-8.4.1.pyc",
      "tests/__pycache__/test_session_logger_log_adapters.cpython-312-pytest-8.4.1.pyc",
      "tests/__pycache__/test_session_logger_wal.cpython-312-pytest-8.4.1.pyc",
      "tests/__pycache__/test_session_logging.cpython-312-pytest-8.4.1.pyc",
      "tests/__pycache__/test_session_logging_mirror.cpython-312-pytest-8.4.1.pyc",
      "tests/__pycache__/test_session_query_cli.cpython-312-pytest-8.4.1.pyc",
      "tests/__pycache__/test_session_query_smoke.cpython-312-pytest-8.4.1.pyc",
      "tests/__pycache__/test_setup_sync_groups.cpython-312-pytest-8.4.1.pyc",
      "tests/__pycache__/test_split_indices.cpython-312-pytest-8.4.1.pyc",
      "tests/__pycache__/test_splits.cpython-312-pytest-8.4.1.pyc",
      "tests/__pycache__/test_sqlite_pool.cpython-312-pytest-8.4.1.pyc",
      "tests/__pycache__/test_sqlite_pool_close.cpython-312-pytest-8.4.1.pyc",
      "tests/__pycache__/test_sqlite_sanitize.cpython-312-pytest-8.4.1.pyc",
      "tests/__pycache__/test_sqlite_wal.cpython-312-pytest-8.4.1.pyc",
      "tests/__pycache__/test_static_code_analysis_step.cpython-312-pytest-8.4.1.pyc",
      "tests/__pycache__/test_symbolic_pipeline.cpython-312-pytest-8.4.1.pyc",
      "tests/__pycache__/test_system_metrics_logging.cpython-312-pytest-8.4.1.pyc",
      "tests/__pycache__/test_system_metrics_sampler.cpython-312-pytest-8.4.1.pyc",
      "tests/__pycache__/test_tb_writer_noop.cpython-312-pytest-8.4.1.pyc",
      "tests/__pycache__/test_telemetry_degrade.cpython-312-pytest-8.4.1.pyc",
      "tests/__pycache__/test_token_cache.cpython-312-pytest-8.4.1.pyc",
      "tests/__pycache__/test_tokenization_roundtrip.cpython-312-pytest-8.4.1.pyc",
      "tests/__pycache__/test_tokenizer_encode_decode.cpython-312-pytest-8.4.1.pyc",
      "tests/__pycache__/test_tokenizer_roundtrip.cpython-312-pytest-8.4.1.pyc",
      "tests/__pycache__/test_train_helpers.cpython-312-pytest-8.4.1.pyc",
      "tests/__pycache__/test_train_loop_import_sideeffects.cpython-312-pytest-8.4.1.pyc",
      "tests/__pycache__/test_train_loop_smoke.cpython-312-pytest-8.4.1.pyc",
      "tests/__pycache__/test_train_smoke.cpython-312-pytest-8.4.1.pyc",
      "tests/__pycache__/test_train_tokenizer.cpython-312-pytest-8.4.1.pyc",
      "tests/__pycache__/test_trainer_reward_registry.cpython-312-pytest-8.4.1.pyc",
      "tests/__pycache__/test_training_callbacks.cpython-312-pytest-8.4.1.pyc",
      "tests/__pycache__/test_training_config_yaml.cpython-312-pytest-8.4.1.pyc",
      "tests/__pycache__/test_training_lr_history_and_eval.cpython-312-pytest-8.4.1.pyc",
      "tests/__pycache__/test_training_resume.cpython-312-pytest-8.4.1.pyc",
      "tests/__pycache__/test_trainloop_grad_accum.cpython-312-pytest-8.4.1.pyc",
      "tests/__pycache__/test_utils_training_callbacks.cpython-312-pytest-8.4.1.pyc",
      "tests/__pycache__/test_vendor_audit_scripts.cpython-312-pytest-8.4.1.pyc",
      "tests/__pycache__/test_wandb_shim.cpython-312-pytest-8.4.1.pyc",
      "tests/_codex_introspect.py",
      "tests/addons/__pycache__/test_metrics_collector_mlflow.cpython-312-pytest-8.4.1.pyc",
      "tests/addons/test_metrics_collector_mlflow.py",
      "tests/analysis/__pycache__/test_audit_pipeline.cpython-312-pytest-8.4.1.pyc",
      "tests/analysis/__pycache__/test_docs_links_audit.cpython-312-pytest-8.4.1.pyc",
      "tests/analysis/__pycache__/test_external_search.cpython-312-pytest-8.4.1.pyc",
      "tests/analysis/__pycache__/test_external_web_search.cpython-312-pytest-8.4.1.pyc",
      "tests/analysis/__pycache__/test_providers.cpython-312-pytest-8.4.1.pyc",
      "tests/analysis/test_audit_pipeline.py",
      "tests/analysis/test_docs_links_audit.py",
      "tests/analysis/test_external_search.py",
      "tests/analysis/test_external_web_search.py",
      "tests/analysis/test_providers.py",
      "tests/assets/README.md",
      "tests/assets/corpus_tiny.txt",
      "tests/breadcrumbs/__pycache__/test_catalog_db.cpython-312-pytest-8.4.1.pyc",
      "tests/breadcrumbs/__pycache__/test_ledger.cpython-312-pytest-8.4.1.pyc",
      "tests/breadcrumbs/test_bundle_and_integrity.py",
      "tests/breadcrumbs/test_catalog_db.py",
      "tests/breadcrumbs/test_compaction.py",
      "tests/breadcrumbs/test_ledger.py",
      "tests/checkpoint/__pycache__/test_state_providers.cpython-312-pytest-8.4.1.pyc",
      "tests/checkpoint/test_state_providers.py",
      "tests/checkpointing/__pycache__/test_atomicity_and_resume.cpython-312-pytest-8.4.1.pyc",
      "tests/checkpointing/__pycache__/test_best_not_pruned.cpython-312-pytest-8.4.1.pyc",
      "tests/checkpointing/__pycache__/test_best_promotion.cpython-312-pytest-8.4.1.pyc",
      "tests/checkpointing/__pycache__/test_checkpoint_json_event.cpython-312-pytest-8.4.1.pyc",
      "tests/checkpointing/__pycache__/test_corrupt_checkpoint_load.cpython-312-pytest-8.4.1.pyc",
      "tests/checkpointing/__pycache__/test_load_latest.cpython-312-pytest-8.4.1.pyc",
      "tests/checkpointing/__pycache__/test_periodic_and_trim.cpython-312-pytest-8.4.1.pyc",
      "tests/checkpointing/__pycache__/test_rng_state_checkpoint.cpython-312-pytest-8.4.1.pyc",
      "tests/checkpointing/__pycache__/test_roundtrip.cpython-312-pytest-8.4.1.pyc",
      "tests/checkpointing/test_atomicity_and_resume.py",
      "tests/checkpointing/test_best_not_pruned.py",
      "tests/checkpointing/test_best_promotion.py",
      "tests/checkpointing/test_checkpoint_json_event.py",
      "tests/checkpointing/test_corrupt_checkpoint_load.py",
      "tests/checkpointing/test_load_latest.py",
      "tests/checkpointing/test_periodic_and_trim.py",
      "tests/checkpointing/test_rng_state_checkpoint.py",
      "tests/checkpointing/test_roundtrip.py",
      "tests/cli/__pycache__/conftest.cpython-312-pytest-8.4.1.pyc",
      "tests/cli/conftest.py",
      "tests/cli/test_cli_viewer.py",
      "tests/cli/test_codex_cli.py",
      "tests/cli/test_codex_export_env.py",
      "tests/cli/test_codex_ml_cli_guard.py",
      "tests/cli/test_codex_train_cli.py",
      "tests/cli/test_codexml_cli_fallback.py",
      "tests/cli/test_evaluation_cli.py",
      "tests/cli/test_generate_safety.py",
      "tests/cli/test_infer_cli_lora.py",
      "tests/cli/test_monitoring_cli.py",
      "tests/cli/test_ndjson_summary_cli.py",
      "tests/cli/test_plugins_cli.py",
      "tests/cli/test_repo_cli.py",
      "tests/cli/test_subcommands.py",
      "tests/cli/test_tokenizer_cli.py",
      "tests/codex_ml/__pycache__/test_model_factory.cpython-312-pytest-8.4.1.pyc",
      "tests/codex_ml/data/__pycache__/test_jsonl_loader.cpython-312-pytest-8.4.1.pyc",
      "tests/codex_ml/data/test_jsonl_loader.py",
      "tests/codex_ml/test_model_factory.py",
      "tests/config/__pycache__/conftest.cpython-312-pytest-8.4.1.pyc",
      "tests/config/__pycache__/test_hydra_defaults.cpython-312-pytest-8.4.1.pyc",
      "tests/config/__pycache__/test_override_propagation.cpython-312-pytest-8.4.1.pyc",
      "tests/config/__pycache__/test_provenance_snapshot.cpython-312-pytest-8.4.1.pyc",
      "tests/config/__pycache__/test_sweep_expand.cpython-312-pytest-8.4.1.pyc",
      "tests/config/__pycache__/test_training_config_yaml.cpython-312-pytest-8.4.1.pyc",
      "tests/config/__pycache__/test_validate_config.cpython-312-pytest-8.4.1.pyc",
      "tests/config/conftest.py",
      "tests/config/test_hydra_defaults.py",
      "tests/config/test_override_propagation.py",
      "tests/config/test_provenance_snapshot.py",
      "tests/config/test_sweep_expand.py",
      "tests/config/test_training_config_yaml.py",
      "tests/config/test_validate_config.py",
      "tests/conftest.py",
      "tests/connectors/__pycache__/conftest.cpython-312-pytest-8.4.1.pyc",
      "tests/connectors/conftest.py",
      "tests/connectors/test_local_connector.py",
      "tests/connectors/test_registry.py",
      "tests/connectors/test_remote.py",
      "tests/data/__pycache__/test_cache_flush_threshold.cpython-312-pytest-8.4.1.pyc",
      "tests/data/__pycache__/test_load_dataset.cpython-312-pytest-8.4.1.pyc",
      "tests/data/__pycache__/test_manifest.cpython-312-pytest-8.4.1.pyc",
      "tests/data/__pycache__/test_prepare_data_reproducible.cpython-312-pytest-8.4.1.pyc",
      "tests/data/__pycache__/test_registry_manifest.cpython-312-pytest-8.4.1.pyc",
      "tests/data/__pycache__/test_safety_filter.cpython-312-pytest-8.4.1.pyc",
      "tests/data/__pycache__/test_safety_filter_split.cpython-312-pytest-8.4.1.pyc",
      "tests/data/__pycache__/test_split_dataset.cpython-312-pytest-8.4.1.pyc",
      "tests/data/__pycache__/test_split_dataset_checksum.cpython-312-pytest-8.4.1.pyc",
      "tests/data/__pycache__/test_split_dataset_deterministic.cpython-312-pytest-8.4.1.pyc",
      "tests/data/__pycache__/test_split_seed.cpython-312-pytest-8.4.1.pyc",
      "tests/data/__pycache__/test_streaming_memory.cpython-312-pytest-8.4.1.pyc",
      "tests/data/test_cache_flush_threshold.py",
      "tests/data/test_cache_roundtrip.py",
      "tests/data/test_hf_factory_compat.py",
      "tests/data/test_load_dataset.py",
      "tests/data/test_loaders.py",
      "tests/data/test_manifest.py",
      "tests/data/test_prepare_data_reproducible.py",
      "tests/data/test_registry_manifest.py",
      "tests/data/test_safety_filter.py",
      "tests/data/test_safety_filter_split.py",
      "tests/data/test_split_dataset.py",
      "tests/data/test_split_dataset_checksum.py",
      "tests/data/test_split_dataset_deterministic.py",
      "tests/data/test_split_seed.py",
      "tests/data/test_streaming_memory.py",
      "tests/data/validate_fences_sample.md",
      "tests/deployment/__pycache__/test_cloud.cpython-312-pytest-8.4.1.pyc",
      "tests/deployment/test_cloud.py",
      "tests/docs/__pycache__/test_link_audit.cpython-312-pytest-8.4.1.pyc",
      "tests/docs/__pycache__/test_status_update_template.cpython-312-pytest-8.4.1.pyc",
      "tests/docs/test_link_audit.py",
      "tests/docs/test_status_update_template.py",
      "tests/eval/__pycache__/conftest.cpython-312-pytest-8.4.1.pyc",
      "tests/eval/conftest.py",
      "tests/eval/test_bleu_rouge_fallbacks.py",
      "tests/eval/test_datasets_hf_disk.py",
      "tests/eval/test_eval_runner_max_samples.py",
      "tests/eval/test_eval_runner_smoke.py",
      "tests/eval/test_evaluation_reproducible.py",
      "tests/eval/test_hf_dataset_loader.py",
      "tests/eval/test_metrics.py",
      "tests/eval/test_metrics_correctness.py",
      "tests/eval/test_metrics_text_values.py",
      "tests/eval/test_registry_determinism.py",
      "tests/eval/test_run_unit_tests.py",
      "tests/eval/test_schema_compat.py",
      "tests/fixtures/.gitkeep",
      "tests/fixtures/data/sample.csv",
      "tests/fixtures/data/sample.jsonl",
      "tests/gates/__pycache__/conftest.cpython-312-pytest-8.4.1.pyc",
      "tests/gates/conftest.py",
      "tests/gates/test_quality_gates.py",
      "tests/helpers/__init__.py",
      "tests/helpers/__pycache__/__init__.cpython-312.pyc",
      "tests/helpers/__pycache__/optional_dependencies.cpython-312.pyc",
      "tests/helpers/optional_dependencies.py",
      "tests/hf_loader/__pycache__/test_amp_dtype_map.cpython-312-pytest-8.4.1.pyc",
      "tests/hf_loader/__pycache__/test_bf16_probe.cpython-312-pytest-8.4.1.pyc",
      "tests/hf_loader/test_amp_dtype_map.py",
      "tests/hf_loader/test_bf16_probe.py",
      "tests/ingestion/__init__.py",
      "tests/ingestion/__pycache__/__init__.cpython-312.pyc",
      "tests/ingestion/__pycache__/test_io_text.cpython-312-pytest-8.4.1.pyc",
      "tests/ingestion/test_io_text.py",
      "tests/interfaces/__init__.py",
      "tests/interfaces/__pycache__/__init__.cpython-312.pyc",
      "tests/interfaces/__pycache__/conftest.cpython-312-pytest-8.4.1.pyc",
      "tests/interfaces/conftest.py",
      "tests/interfaces/test_apply_config.py",
      "tests/interfaces/test_env_tokenizer_component.py",
      "tests/interfaces/test_get_component_env_var.py",
      "tests/interfaces/test_loader_tokenizer_env.py",
      "tests/interfaces/test_registry_loader.py",
      "tests/interfaces/test_tokenizer_hf.py",
      "tests/interfaces/test_tokenizer_loader_env.py",
      "tests/logging/__pycache__/test_file_logger.cpython-312-pytest-8.4.1.pyc",
      "tests/logging/__pycache__/test_public_api_docstrings.cpython-312-pytest-8.4.1.pyc",
      "tests/logging/__pycache__/test_public_docstrings.cpython-312-pytest-8.4.1.pyc",
      "tests/logging/__pycache__/test_run_schema.cpython-312-pytest-8.4.1.pyc",
      "tests/logging/test_file_logger.py",
      "tests/logging/test_public_api_docstrings.py",
      "tests/logging/test_public_docstrings.py",
      "tests/logging/test_run_schema.py",
      "tests/metrics/__pycache__/test_metrics_jsonschema.cpython-312-pytest-8.4.1.pyc",
      "tests/metrics/__pycache__/test_record_metrics_local.cpython-312-pytest-8.4.1.pyc",
      "tests/metrics/__pycache__/test_writers.cpython-312-pytest-8.4.1.pyc",
      "tests/metrics/test_metrics_jsonschema.py",
      "tests/metrics/test_record_metrics_local.py",
      "tests/metrics/test_writers.py",
      "tests/modeling/__pycache__/conftest.cpython-312-pytest-8.4.1.pyc",
      "tests/modeling/conftest.py",
      "tests/modeling/test_decoder_only.py",
      "tests/modeling/test_model_registry.py",
      "tests/models/__pycache__/conftest.cpython-312-pytest-8.4.1.pyc",
      "tests/models/conftest.py",
      "tests/models/test_lora_integration.py",
      "tests/models/test_models_registry_api.py",
      "tests/models/test_peft_optional.py",
      "tests/models/test_peft_smoke.py",
      "tests/models/test_registry_dtype.py",
      "tests/monitoring/__init__.py",
      "tests/monitoring/__pycache__/__init__.cpython-312.pyc",
      "tests/monitoring/__pycache__/conftest.cpython-312-pytest-8.4.1.pyc",
      "tests/monitoring/__pycache__/test_async_writer_rotation.cpython-312-pytest-8.4.1.pyc",
      "tests/monitoring/__pycache__/test_codex_logging_bootstrap.cpython-312-pytest-8.4.1.pyc",
      "tests/monitoring/__pycache__/test_codex_logging_cfg.cpython-312-pytest-8.4.1.pyc",
      "tests/monitoring/__pycache__/test_codex_logging_degraded_warning.cpython-312-pytest-8.4.1.pyc",
      "tests/monitoring/__pycache__/test_codex_logging_offline.cpython-312-pytest-8.4.1.pyc",
      "tests/monitoring/__pycache__/test_engine_bootstrap.cpython-312-pytest-8.4.1.pyc",
      "tests/monitoring/__pycache__/test_logging_bootstrap_initialization.cpython-312-pytest-8.4.1.pyc",
      "tests/monitoring/__pycache__/test_logging_bootstrap_real.cpython-312-pytest-8.4.1.pyc",
      "tests/monitoring/__pycache__/test_mlflow_monitoring_utils.cpython-312-pytest-8.4.1.pyc",
      "tests/monitoring/__pycache__/test_monitoring_mlflow_utils.cpython-312-pytest-8.4.1.pyc",
      "tests/monitoring/__pycache__/test_nvml_optional.cpython-312-pytest-8.4.1.pyc",
      "tests/monitoring/__pycache__/test_prometheus_fallback.cpython-312-pytest-8.4.1.pyc",
      "tests/monitoring/__pycache__/test_sampler_multi_gpu.cpython-312-pytest-8.4.1.pyc",
      "tests/monitoring/__pycache__/test_schema_and_redaction.cpython-312-pytest-8.4.1.pyc",
      "tests/monitoring/__pycache__/test_system_metrics.cpython-312-pytest-8.4.1.pyc",
      "tests/monitoring/__pycache__/test_system_snapshot.cpython-312-pytest-8.4.1.pyc",
      "tests/monitoring/conftest.py",
      "tests/monitoring/test_async_writer_rotation.py",
      "tests/monitoring/test_codex_logging_bootstrap.py",
      "tests/monitoring/test_codex_logging_cfg.py",
      "tests/monitoring/test_codex_logging_degraded_warning.py",
      "tests/monitoring/test_codex_logging_offline.py",
      "tests/monitoring/test_engine_bootstrap.py",
      "tests/monitoring/test_logging_bootstrap_initialization.py",
      "tests/monitoring/test_logging_bootstrap_real.py",
      "tests/monitoring/test_mlflow_monitoring_utils.py",
      "tests/monitoring/test_monitoring_mlflow_utils.py",
      "tests/monitoring/test_nvml_optional.py",
      "tests/monitoring/test_prometheus_fallback.py",
      "tests/monitoring/test_sampler_multi_gpu.py",
      "tests/monitoring/test_schema_and_redaction.py",
      "tests/monitoring/test_system_metrics.py",
      "tests/monitoring/test_system_snapshot.py",
      "tests/multilingual/__pycache__/conftest.cpython-312-pytest-8.4.1.pyc",
      "tests/multilingual/conftest.py",
      "tests/multilingual/test_data_loader.py",
      "tests/multilingual/test_tokenizer.py",
      "tests/perf/__pycache__/test_perf_smoke.cpython-312-pytest-8.4.1.pyc",
      "tests/perf/test_perf_smoke.py",
      "tests/pipeline/__pycache__/conftest.cpython-312-pytest-8.4.1.pyc",
      "tests/pipeline/conftest.py",
      "tests/pipeline/test_pipeline_config.py",
      "tests/pipeline/test_pipeline_smoke.py",
      "tests/plugins/__pycache__/test_entry_point_collision.cpython-312-pytest-8.4.1.pyc",
      "tests/plugins/__pycache__/test_entry_point_discovery.cpython-312-pytest-8.4.1.pyc",
      "tests/plugins/__pycache__/test_factory_vs_class.cpython-312-pytest-8.4.1.pyc",
      "tests/plugins/__pycache__/test_registry_basic.cpython-312-pytest-8.4.1.pyc",
      "tests/plugins/_sandbox_pkg/codex_dummy_plugin/__init__.py",
      "tests/plugins/_sandbox_pkg/pyproject.toml",
      "tests/plugins/test_entry_point_collision.py",
      "tests/plugins/test_entry_point_discovery.py",
      "tests/plugins/test_factory_vs_class.py",
      "tests/plugins/test_registry_basic.py",
      "tests/plugins/test_registry_entries.py",
      "tests/privacy/__pycache__/conftest.cpython-312-pytest-8.4.1.pyc",
      "tests/privacy/conftest.py",
      "tests/privacy/test_dp_training.py",
      "tests/registry/__pycache__/test_registry_documentation.cpython-312-pytest-8.4.1.pyc",
      "tests/registry/test_registry_documentation.py",
      "tests/reward_models/__pycache__/test_length_reward_model.cpython-312-pytest-8.4.1.pyc",
      "tests/reward_models/test_length_reward_model.py",
      "tests/safety/__pycache__/test_filters.cpython-312-pytest-8.4.1.pyc",
      "tests/safety/__pycache__/test_filters_regressions.cpython-312-pytest-8.4.1.pyc",
      "tests/safety/__pycache__/test_filters_runtime.cpython-312-pytest-8.4.1.pyc",
      "tests/safety/__pycache__/test_license_checker.cpython-312-pytest-8.4.1.pyc",
      "tests/safety/__pycache__/test_risk_score.cpython-312-pytest-8.4.1.pyc",
      "tests/safety/__pycache__/test_safety_filter_integration.cpython-312-pytest-8.4.1.pyc",
      "tests/safety/test_filters.py",
      "tests/safety/test_filters_regressions.py",
      "tests/safety/test_filters_runtime.py",
      "tests/safety/test_license_checker.py",
      "tests/safety/test_risk_score.py",
      "tests/safety/test_safety_filter_integration.py",
      "tests/security/__pycache__/test_log_redaction.cpython-312-pytest-8.4.1.pyc",
      "tests/security/__pycache__/test_offline_scans.cpython-312-pytest-8.4.1.pyc",
      "tests/security/__pycache__/test_safety_filters.cpython-312-pytest-8.4.1.pyc",
      "tests/security/__pycache__/test_semgrep_rules.cpython-312-pytest-8.4.1.pyc",
      "tests/security/__pycache__/test_verify_pins.cpython-312-pytest-8.4.1.pyc",
      "tests/security/test_log_redaction.py",
      "tests/security/test_offline_scans.py",
      "tests/security/test_safety_filters.py",
      "tests/security/test_semgrep_rules.py",
      "tests/security/test_verify_pins.py",
      "tests/smoke/__pycache__/conftest.cpython-312-pytest-8.4.1.pyc",
      "tests/smoke/__pycache__/test_artifacts_hash.cpython-312-pytest-8.4.1.pyc",
      "tests/smoke/__pycache__/test_checkpoint_hashing.cpython-312-pytest-8.4.1.pyc",
      "tests/smoke/__pycache__/test_cli_determinism_wiring.cpython-312-pytest-8.4.1.pyc",
      "tests/smoke/__pycache__/test_config_validate_cli.cpython-312-pytest-8.4.1.pyc",
      "tests/smoke/__pycache__/test_determinism.cpython-312-pytest-8.4.1.pyc",
      "tests/smoke/__pycache__/test_logging_flags_end_to_end.cpython-312-pytest-8.4.1.pyc",
      "tests/smoke/__pycache__/test_mlflow_utils_noop.cpython-312-pytest-8.4.1.pyc",
      "tests/smoke/__pycache__/test_plugin_registry.cpython-312-pytest-8.4.1.pyc",
      "tests/smoke/conftest.py",
      "tests/smoke/test_artifacts_hash.py",
      "tests/smoke/test_checkpoint_hashing.py",
      "tests/smoke/test_cli_determinism_wiring.py",
      "tests/smoke/test_config_validate_cli.py",
      "tests/smoke/test_determinism.py",
      "tests/smoke/test_hf_trainer_hello.py",
      "tests/smoke/test_logging_flags_end_to_end.py",
      "tests/smoke/test_mlflow_utils_noop.py",
      "tests/smoke/test_plugin_registry.py",
      "tests/telemetry/__pycache__/test_instrumentation.cpython-312-pytest-8.4.1.pyc",
      "tests/telemetry/__pycache__/test_json_disable_env.cpython-312-pytest-8.4.1.pyc",
      "tests/telemetry/__pycache__/test_metrics_server.cpython-312-pytest-8.4.1.pyc",
      "tests/telemetry/__pycache__/test_ndjson_disable_env.cpython-312-pytest-8.4.1.pyc",
      "tests/telemetry/__pycache__/test_sample_rate_gate.cpython-312-pytest-8.4.1.pyc",
      "tests/telemetry/__pycache__/test_telemetry_event_schema.cpython-312-pytest-8.4.1.pyc",
      "tests/telemetry/test_instrumentation.py",
      "tests/telemetry/test_json_disable_env.py",
      "tests/telemetry/test_metrics_server.py",
      "tests/telemetry/test_ndjson_disable_env.py",
      "tests/telemetry/test_sample_rate_gate.py",
      "tests/telemetry/test_telemetry_event_schema.py",
      "tests/test_accelerate_shim.py",
      "tests/test_activations.py",
      "tests/test_api_infer.py",
      "tests/test_api_infer_masking.py",
      "tests/test_api_infer_tokenizer.py",
      "tests/test_api_rate_limit.py",
      "tests/test_api_secret_filter.py",
      "tests/test_batch_metrics_shim.py",
      "tests/test_bestk_retention.py",
      "tests/test_callbacks.py",
      "tests/test_chat_env_cleanup.py",
      "tests/test_chat_session.py",
      "tests/test_chat_session_exit.py",
      "tests/test_checkpoint_bundle.py",
      "tests/test_checkpoint_checksum.py",
      "tests/test_checkpoint_checksum_and_retention.py",
      "tests/test_checkpoint_commit_meta.py",
      "tests/test_checkpoint_corrupt_load.py",
      "tests/test_checkpoint_corruption.py",
      "tests/test_checkpoint_integrity.py",
      "tests/test_checkpoint_integrity_corruption.py",
      "tests/test_checkpoint_manager.py",
      "tests/test_checkpoint_metadata.py",
      "tests/test_checkpoint_provenance.py",
      "tests/test_checkpoint_restore_rng_torch.py",
      "tests/test_checkpoint_roundtrip.py",
      "tests/test_checkpoint_save_resume.py",
      "tests/test_checkpoint_sha_rng.py",
      "tests/test_checkpoint_system_meta.py",
      "tests/test_checkpoint_util.py",
      "tests/test_checkpointing.py",
      "tests/test_checkpointing_utils_extra.py",
      "tests/test_checksum_sha256.py",
      "tests/test_ci_smoke.py",
      "tests/test_cli.py",
      "tests/test_cli_help.py",
      "tests/test_cli_help_paths.py",
      "tests/test_cli_hydra_validation.py",
      "tests/test_cli_pool.py",
      "tests/test_cli_smoke.py",
      "tests/test_cli_train_command.py",
      "tests/test_cli_train_engine.py",
      "tests/test_codex_best_effort.py",
      "tests/test_codex_cli.py",
      "tests/test_codex_maintenance.py",
      "tests/test_codex_ml_cli_version.py",
      "tests/test_codex_run_tasks.py",
      "tests/test_codex_sequence_validations.py",
      "tests/test_codexml_cli.py",
      "tests/test_config_validation.py",
      "tests/test_conversation_logger.py",
      "tests/test_crc32_file.py",
      "tests/test_data_cache_sharding.py",
      "tests/test_data_loader.py",
      "tests/test_data_loaders.py",
      "tests/test_data_loaders_extended.py",
      "tests/test_data_registry.py",
      "tests/test_data_split_utils.py",
      "tests/test_data_splits.py",
      "tests/test_data_utils.py",
      "tests/test_dataset_checksums.py",
      "tests/test_dataset_manifest.py",
      "tests/test_db_utils.py",
      "tests/test_db_utils_table_name.py",
      "tests/test_deep_research_task_process.py",
      "tests/test_deploy_codex_pipeline.py",
      "tests/test_determinism.py",
      "tests/test_deterministic_split.py",
      "tests/test_docs_examples_paths.py",
      "tests/test_engine_hf_trainer.py",
      "tests/test_engine_hf_trainer_grad_accum.py",
      "tests/test_engine_hf_trainer_lora.py",
      "tests/test_env_logging.py",
      "tests/test_error_log.py",
      "tests/test_eval_loop_cpu.py",
      "tests/test_eval_loop_minimal.py",
      "tests/test_eval_loop_resilience.py",
      "tests/test_eval_runner.py",
      "tests/test_eval_with_metrics.py",
      "tests/test_evaluate_cli.py",
      "tests/test_evaluation.py",
      "tests/test_export.py",
      "tests/test_fences_tool.py",
      "tests/test_fetch_messages.py",
      "tests/test_fetch_messages_missing_db.py",
      "tests/test_git_tag.py",
      "tests/test_grad_accumulation_path.py",
      "tests/test_gradient_accumulation_equivalence.py",
      "tests/test_gradient_accumulation_tail_flush.py",
      "tests/test_hf_loader_amp.py",
      "tests/test_hf_loader_peft_guard.py",
      "tests/test_hf_loader_registry.py",
      "tests/test_hf_tokenizer_padding.py",
      "tests/test_hf_trainer_lora_config.py",
      "tests/test_hydra_cli.py",
      "tests/test_hydra_compose.py",
      "tests/test_import_codex.py",
      "tests/test_import_ndjson.py",
      "tests/test_import_ndjson_cli.py",
      "tests/test_import_ndjson_dedup.py",
      "tests/test_ingestion_auto_encoding.py",
      "tests/test_ingestion_encoding_coverage.py",
      "tests/test_ingestion_encodings_matrix.py",
      "tests/test_ingestion_family_encoding.py",
      "tests/test_ingestion_io.py",
      "tests/test_ingestion_read_text.py",
      "tests/test_ingestion_seeded_shuffle.py",
      "tests/test_ingestion_split_cache.py",
      "tests/test_interface_loader_env.py",
      "tests/test_interfaces_compat.py",
      "tests/test_interfaces_hf_tokenizer.py",
      "tests/test_iter_jsonl.py",
      "tests/test_json_report.py",
      "tests/test_jsonl_writer.py",
      "tests/test_label_policy_lint.py",
      "tests/test_loader_registry.py",
      "tests/test_loaders.py",
      "tests/test_log_adapters.py",
      "tests/test_logging_bootstrap.py",
      "tests/test_logging_viewer_cli.py",
      "tests/test_make_generator_worker_seed.py",
      "tests/test_metric_curves.py",
      "tests/test_metric_registry.py",
      "tests/test_metrics.py",
      "tests/test_metrics_correctness.py",
      "tests/test_metrics_logging.py",
      "tests/test_metrics_perplexity.py",
      "tests/test_metrics_registry_alias.py",
      "tests/test_metrics_tb.py",
      "tests/test_metrics_token_accuracy.py",
      "tests/test_microhelpers.py",
      "tests/test_minilm_forward.py",
      "tests/test_mlflow_adapter.py",
      "tests/test_mlflow_step_logging.py",
      "tests/test_mlflow_utils.py",
      "tests/test_mlflow_utils_root.py",
      "tests/test_model_factory.py",
      "tests/test_model_forward.py",
      "tests/test_model_loader.py",
      "tests/test_model_registry.py",
      "tests/test_model_registry_invalid.py",
      "tests/test_modeling_utils.py",
      "tests/test_monitoring.py",
      "tests/test_monitoring_cli.py",
      "tests/test_monitoring_thread.py",
      "tests/test_ndjson_db_parity.py",
      "tests/test_ndjson_logger.py",
      "tests/test_ndjson_logging.py",
      "tests/test_ndjson_writer.py",
      "tests/test_nox_tests_delegation.py",
      "tests/test_offline_repo_auditor.py",
      "tests/test_parse_when.py",
      "tests/test_peft_adapter.py",
      "tests/test_peft_integration.py",
      "tests/test_pipeline.py",
      "tests/test_plugin_loader.py",
      "tests/test_plugins_cli.py",
      "tests/test_precommit_config_exists.py",
      "tests/test_pyproject_metadata.py",
      "tests/test_query_logs_build_query.py",
      "tests/test_query_logs_tail.py",
      "tests/test_readme_examples.py",
      "tests/test_readme_has_quickstart.py",
      "tests/test_registry.py",
      "tests/test_repo_option_a.py",
      "tests/test_repo_option_b.py",
      "tests/test_repro_branches.py",
      "tests/test_repro_cli.py",
      "tests/test_repro_determinism.py",
      "tests/test_repro_helper.py",
      "tests/test_repro_rng_roundtrip.py",
      "tests/test_repro_seed_consistency.py",
      "tests/test_reproducibility.py",
      "tests/test_requirements_lock.py",
      "tests/test_resume.py",
      "tests/test_resume_training.py",
      "tests/test_reward_models_rlhf.py",
      "tests/test_run_eval_cli.py",
      "tests/test_run_functional_training_tokenizer.py",
      "tests/test_runner_doctor.py",
      "tests/test_safety.py",
      "tests/test_safety_filters_integration.py",
      "tests/test_safety_import_no_crash.py",
      "tests/test_search_providers.py",
      "tests/test_secrets_scanner.py",
      "tests/test_seeded_shuffle.py",
      "tests/test_semparser.py",
      "tests/test_sentencepiece_adapter.py",
      "tests/test_session_hooks.py",
      "tests/test_session_hooks_warnings.py",
      "tests/test_session_logger_log_adapters.py",
      "tests/test_session_logger_wal.py",
      "tests/test_session_logging.py",
      "tests/test_session_logging_mirror.py",
      "tests/test_session_query_cli.py",
      "tests/test_session_query_smoke.py",
      "tests/test_setup_sync_groups.py",
      "tests/test_split_indices.py",
      "tests/test_splits.py",
      "tests/test_sqlite_pool.py",
      "tests/test_sqlite_pool_close.py",
      "tests/test_sqlite_sanitize.py",
      "tests/test_sqlite_wal.py",
      "tests/test_static_code_analysis_step.py",
      "tests/test_symbolic_pipeline.py",
      "tests/test_system_metrics_logging.py",
      "tests/test_system_metrics_sampler.py",
      "tests/test_tb_writer_noop.py",
      "tests/test_telemetry_degrade.py",
      "tests/test_token_cache.py",
      "tests/test_tokenization.py",
      "tests/test_tokenization_roundtrip.py",
      "tests/test_tokenizer.py",
      "tests/test_tokenizer_batch_encode.py",
      "tests/test_tokenizer_encode_decode.py",
      "tests/test_tokenizer_ids.py",
      "tests/test_tokenizer_roundtrip.py",
      "tests/test_tokenizer_sp.py",
      "tests/test_tokenizer_wrapper.py",
      "tests/test_train_helpers.py",
      "tests/test_train_loop.py",
      "tests/test_train_loop_import_sideeffects.py",
      "tests/test_train_loop_smoke.py",
      "tests/test_train_smoke.py",
      "tests/test_train_tokenizer.py",
      "tests/test_trainer_reward_registry.py",
      "tests/test_training_arguments_flags.py",
      "tests/test_training_callbacks.py",
      "tests/test_training_config_yaml.py",
      "tests/test_training_eval.py",
      "tests/test_training_integration_flags.py",
      "tests/test_training_lr_history_and_eval.py",
      "tests/test_training_metadata_logging.py",
      "tests/test_training_resume.py",
      "tests/test_trainloop_grad_accum.py",
      "tests/test_utils_training_callbacks.py",
      "tests/test_vendor_audit_scripts.py",
      "tests/test_wandb_shim.py",
      "tests/tokenization/__pycache__/conftest.cpython-312-pytest-8.4.1.pyc",
      "tests/tokenization/__pycache__/test_adapter.cpython-312-pytest-8.4.1.pyc",
      "tests/tokenization/__pycache__/test_cli_inspect_export.cpython-312-pytest-8.4.1.pyc",
      "tests/tokenization/__pycache__/test_encode_decode_roundtrip.cpython-312-pytest-8.4.1.pyc",
      "tests/tokenization/__pycache__/test_load_tokenizer_use_fast.cpython-312-pytest-8.4.1.pyc",
      "tests/tokenization/__pycache__/test_padding_truncation_ext.cpython-312-pytest-8.4.1.pyc",
      "tests/tokenization/conftest.py",
      "tests/tokenization/test_adapter.py",
      "tests/tokenization/test_cli_inspect_export.py",
      "tests/tokenization/test_encode_decode_roundtrip.py",
      "tests/tokenization/test_load_tokenizer_use_fast.py",
      "tests/tokenization/test_padding_truncation_ext.py",
      "tests/tokenization/test_roundtrip.py",
      "tests/tokenization/test_roundtrip_basic.py",
      "tests/tokenization/test_sentencepiece_adapter.py",
      "tests/tokenization/test_sentencepiece_adapter_edges.py",
      "tests/tokenization/test_sentencepiece_adapter_prefix.py",
      "tests/tokenization/test_sentencepiece_adapter_stub.py",
      "tests/tokenization/test_sentencepiece_adapter_train.py",
      "tests/tokenization/test_sentencepiece_roundtrip.py",
      "tests/tokenization/test_sentencepiece_tokenizer.py",
      "tests/tokenization/test_sp_fixture_roundtrip.py",
      "tests/tokenization/test_streaming_ingest.py",
      "tests/tokenization/test_tiny_vocab_roundtrip.py",
      "tests/tokenization/test_tokenizer_basic.py",
      "tests/tokenization/test_tokenizer_cli.py",
      "tests/tokenization/test_tokenizer_training_streaming_equivalence.py",
      "tests/tokenization/test_train_tokenizer_smoke.py",
      "tests/tokenization/test_train_tokenizer_streaming.py",
      "tests/tools/.gitkeep",
      "tests/tools/test_export_security.py",
      "tests/tools/test_post_check_validation.py",
      "tests/tools/test_validate_fences.py",
      "tests/tools/test_validate_fences_samples.py",
      "tests/tracking/__init__.py",
      "tests/tracking/test_composite_writer_degrades.py",
      "tests/tracking/test_default_file_backend.py",
      "tests/tracking/test_git_tag.py",
      "tests/tracking/test_git_tag_decode.py",
      "tests/tracking/test_init_experiment_tags.py",
      "tests/tracking/test_mlflow_defaults_and_noop.py",
      "tests/tracking/test_mlflow_entrypoints.py",
      "tests/tracking/test_mlflow_guard.py",
      "tests/tracking/test_mlflow_noop_default.py",
      "tests/tracking/test_mlflow_offline_cli.py",
      "tests/tracking/test_mlflow_offline_guard.py",
      "tests/tracking/test_mlflow_utils_py.old",
      "tests/tracking/test_ndjson_logger_core.py",
      "tests/tracking/test_ndjson_summarizer.py",
      "tests/tracking/test_ndjson_writer.py",
      "tests/tracking/test_noop_logger.py",
      "tests/tracking/test_seed_snapshot_local.py",
      "tests/tracking/test_tracking_summary_rotation.py",
      "tests/tracking/test_tracking_writers_offline.py",
      "tests/train_loop/test_bf16_matmul_guard.py",
      "tests/train_loop/test_bf16_require_flag.py",
      "tests/train_loop/test_dataset_cast_policy_event.py",
      "tests/train_loop/test_dtype_logging_and_dataset_gate.py",
      "tests/train_loop/test_resolve_dtype_and_device.py",
      "tests/train_loop/test_telemetry_rollover.py",
      "tests/training/conftest.py",
      "tests/training/test_base_config.py",
      "tests/training/test_callbacks.py",
      "tests/training/test_checkpoint_manager_basic.py",
      "tests/training/test_checkpoint_manager_callback.py",
      "tests/training/test_checkpoint_resume.py",
      "tests/training/test_config_loading.py",
      "tests/training/test_custom_loop_overfit.py",
      "tests/training/test_engine_hf_trainer_lora_cfg.py",
      "tests/training/test_functional_training_evaluation.py",
      "tests/training/test_functional_training_main.py",
      "tests/training/test_lora_optional.py",
      "tests/training/test_overfit_smoke.py",
      "tests/training/test_run_custom_trainer_metadata.py",
      "tests/training/test_run_functional_training_resume.py",
      "tests/training/test_seed_util.py",
      "tests/training/test_split_determinism.py",
      "tests/training/test_strict_determinism.py",
      "tests/training/test_tiny_overfit.py",
      "tests/utils/__pycache__/cli_runner.cpython-312.pyc",
      "tests/utils/cli_runner.py",
      "tests/utils/test_checkpoint_rng.py",
      "tests/utils/test_checkpointing.py",
      "tests/utils/test_checkpointing_core.py",
      "tests/utils/test_checkpointing_safe_load.py",
      "tests/utils/test_codex_utils_offline.py",
      "tests/utils/test_error_log.py",
      "tests/utils/test_modeling.py",
      "tests/utils/test_provenance_export.py",
      "tests/utils/test_repro_rng.py"
    ],
    "tokenizer": [
      ".venv/lib/python3.12/site-packages/packaging/_tokenizer.py",
      ".venv/lib/python3.12/site-packages/pip/_vendor/packaging/_tokenizer.py",
      "codex_digest/tokenizer.py",
      "examples/plugins/toy_tokenizer/__init__.py",
      "interfaces/tokenizer.py",
      "src/codex_ml/interfaces/tokenizer.py",
      "src/codex_ml/registry/tokenizers.py",
      "src/codex_ml/tokenization/hf_tokenizer.py",
      "src/codex_ml/tokenization/train_tokenizer.py",
      "src/tokenization/train_tokenizer.py",
      "src/tokenizer/__init__.py",
      "src/tokenizer/__pycache__/__init__.cpython-312.pyc",
      "src/tokenizer/__pycache__/fast_tokenizer.cpython-312.pyc",
      "src/tokenizer/fast_tokenizer.py",
      "tests/cli/test_tokenizer_cli.py",
      "tests/interfaces/test_env_tokenizer_component.py",
      "tests/interfaces/test_loader_tokenizer_env.py",
      "tests/interfaces/test_tokenizer_hf.py",
      "tests/interfaces/test_tokenizer_loader_env.py",
      "tests/multilingual/test_tokenizer.py",
      "tests/test_api_infer_tokenizer.py",
      "tests/test_hf_tokenizer_padding.py",
      "tests/test_interfaces_hf_tokenizer.py",
      "tests/test_run_functional_training_tokenizer.py",
      "tests/test_tokenizer.py",
      "tests/test_tokenizer_batch_encode.py",
      "tests/test_tokenizer_encode_decode.py",
      "tests/test_tokenizer_ids.py",
      "tests/test_tokenizer_roundtrip.py",
      "tests/test_tokenizer_sp.py",
      "tests/test_tokenizer_wrapper.py",
      "tests/test_train_tokenizer.py",
      "tests/tokenization/test_load_tokenizer_use_fast.py",
      "tests/tokenization/test_sentencepiece_tokenizer.py",
      "tests/tokenization/test_tokenizer_basic.py",
      "tests/tokenization/test_tokenizer_cli.py",
      "tests/tokenization/test_tokenizer_training_streaming_equivalence.py",
      "tests/tokenization/test_train_tokenizer_smoke.py",
      "tests/tokenization/test_train_tokenizer_streaming.py",
      "tokenization/__init__.py",
      "tokenization/__pycache__/__init__.cpython-312.pyc",
      "tokenization/cli.py"
    ],
    "train": [
      "src/training/__init__.py",
      "src/training/__pycache__/__init__.cpython-312.pyc",
      "src/training/__pycache__/simple_trainer.cpython-312.pyc",
      "src/training/simple_trainer.py",
      "training/__init__.py",
      "training/cache.py",
      "training/checkpoint_manager.py",
      "training/data_utils.py",
      "training/datasets.py",
      "training/engine_hf_trainer.py",
      "training/functional_training.py",
      "training/seed.py",
      "training/streaming.py"
    ]
  },
  "stub_density": {
    "stubs_per_kloc": 28.51262010680376,
    "total_python_lines": 316281,
    "total_stub_count": 9018
  },
  "stubs": [
    {
      "context": "\"\"\n        log_change(f\"- {category}: {preview}{suffix}\")\n\n    stub_markers = (\"TODO\", \"NotImplementedError\", \"pass  # TODO\")\n    try:\n        for path in repo_root",
      "file": "codex_update_runner.py",
      "line": 130,
      "match": "\\bTODO\\b"
    },
    {
      "context": "preview}{suffix}\")\n\n    stub_markers = (\"TODO\", \"NotImplementedError\", \"pass  # TODO\")\n    try:\n        for path in repo_root.rglob(\"*.py\"):\n            if \".git\" i",
      "file": "codex_update_runner.py",
      "line": 130,
      "match": "\\bTODO\\b"
    },
    {
      "context": "   log_change(f\"- {category}: {preview}{suffix}\")\n\n    stub_markers = (\"TODO\", \"NotImplementedError\", \"pass  # TODO\")\n    try:\n        for path in repo_root.rglob(\"*.py\"):\n       ",
      "file": "codex_update_runner.py",
      "line": 130,
      "match": "NotImplementedError"
    },
    {
      "context": "gory}: {preview}{suffix}\")\n\n    stub_markers = (\"TODO\", \"NotImplementedError\", \"pass  # TODO\")\n    try:\n        for path in repo_root.rglob(\"*.py\"):\n            if \".git\" i",
      "file": "codex_update_runner.py",
      "line": 130,
      "match": "\\bpass\\s*#\\s*TODO\\b"
    },
    {
      "context": "can (run tools/offline_repo_auditor.py)\n- Stub detection targets:\n  - Keywords: TODO, FIXME, TBD, NotImplementedError, pass (empty bodies), raise NotImplementedErro",
      "file": "CODEBASE_AUDIT_2025-08-26_203612.md",
      "line": 26,
      "match": "\\bTODO\\b"
    },
    {
      "context": "un tools/offline_repo_auditor.py)\n- Stub detection targets:\n  - Keywords: TODO, FIXME, TBD, NotImplementedError, pass (empty bodies), raise NotImplementedError\n  - P",
      "file": "CODEBASE_AUDIT_2025-08-26_203612.md",
      "line": 26,
      "match": "\\bFIXME\\b"
    },
    {
      "context": "line_repo_auditor.py)\n- Stub detection targets:\n  - Keywords: TODO, FIXME, TBD, NotImplementedError, pass (empty bodies), raise NotImplementedError\n  - Placeholder configs: exampl",
      "file": "CODEBASE_AUDIT_2025-08-26_203612.md",
      "line": 26,
      "match": "NotImplementedError"
    },
    {
      "context": "  - Keywords: TODO, FIXME, TBD, NotImplementedError, pass (empty bodies), raise NotImplementedError\n  - Placeholder configs: example\\_*.yaml, template\\_*.json, .env.example\n- Next",
      "file": "CODEBASE_AUDIT_2025-08-26_203612.md",
      "line": 26,
      "match": "NotImplementedError"
    },
    {
      "context": "/cli/test_subcommands.py\n```\n\n**Stub/Placeholder signals (sample)**\n```text\n(no TODO/FIXME/NotImplementedError/pass signals sampled in small scan)\n```\n\n---\n\n## 2) C",
      "file": "_codex_status_update-0C_base_-2025-09-27.md",
      "line": 109,
      "match": "\\bTODO\\b"
    },
    {
      "context": "test_subcommands.py\n```\n\n**Stub/Placeholder signals (sample)**\n```text\n(no TODO/FIXME/NotImplementedError/pass signals sampled in small scan)\n```\n\n---\n\n## 2) Capabil",
      "file": "_codex_status_update-0C_base_-2025-09-27.md",
      "line": 109,
      "match": "\\bFIXME\\b"
    },
    {
      "context": "ubcommands.py\n```\n\n**Stub/Placeholder signals (sample)**\n```text\n(no TODO/FIXME/NotImplementedError/pass signals sampled in small scan)\n```\n\n---\n\n## 2) Capability Audit Table\n| Ca",
      "file": "_codex_status_update-0C_base_-2025-09-27.md",
      "line": 109,
      "match": "NotImplementedError"
    },
    {
      "context": "ted automation assets.\n\n## 3. Stub & Placeholder Histogram (unchanged counts)\n- TODO markers: **4,534**\n- `NotImplementedError` raises: **4,192**\n- Bare `pass` plac",
      "file": "CODEBASE_AUDIT_2025-09-27_ITERATION3.md",
      "line": 14,
      "match": "\\bTODO\\b"
    },
    {
      "context": "3. Stub & Placeholder Histogram (unchanged counts)\n- TODO markers: **4,534**\n- `NotImplementedError` raises: **4,192**\n- Bare `pass` placeholders: **365**\n- Concentrated hotspots ",
      "file": "CODEBASE_AUDIT_2025-09-27_ITERATION3.md",
      "line": 15,
      "match": "NotImplementedError"
    },
    {
      "context": "ning automation hotspots (esp. `codex_ast_upgrade.py`) with focus on replacing `NotImplementedError` paths that still gate orchestrators.\n5. **Documentation Sync**: document offli",
      "file": "CODEBASE_AUDIT_2025-09-27_ITERATION3.md",
      "line": 36,
      "match": "NotImplementedError"
    },
    {
      "context": "ell_type\":\"markdown\",\"metadata\":{},\"source\":[\"# GPU Training Example (Stub)\\n\",\"TODO: Fill with end-to-end training demo.\"]},\n  {\"cell_type\":\"code\",\"metadata\":{},\"e",
      "file": "codex_script.py",
      "line": 595,
      "match": "\\bTODO\\b"
    },
    {
      "context": "nto a temporary staging area for parsing. Extract all code blocks, commands and TODO markers to form an initial change backlog.\n4. Capture the baseline environment:",
      "file": "AUDIT_PROMPT.md",
      "line": 119,
      "match": "\\bTODO\\b"
    },
    {
      "context": "ere possible to confirm loadability.\n2. Scan for stubs or placeholders such as `TODO`, `pass`, or `NotImplementedError` and associate them with the capability index",
      "file": "AUDIT_PROMPT.md",
      "line": 125,
      "match": "\\bTODO\\b"
    },
    {
      "context": "ontent\": code_block})\n        for line in content.splitlines():\n            if \"TODO\" in line:\n                tasks.append({\"type\": \"todo\", \"content\": line.strip()",
      "file": "AUDIT_PROMPT.md",
      "line": 238,
      "match": "\\bTODO\\b"
    },
    {
      "context": "merate(f, start=1):\n                        if any(token in line for token in [\"TODO\", \"pass\", \"NotImplementedError\"]):\n                            capability_map.s",
      "file": "AUDIT_PROMPT.md",
      "line": 255,
      "match": "\\bTODO\\b"
    },
    {
      "context": "firm loadability.\n2. Scan for stubs or placeholders such as `TODO`, `pass`, or `NotImplementedError` and associate them with the capability index. Log these in a machine-readable ",
      "file": "AUDIT_PROMPT.md",
      "line": 125,
      "match": "NotImplementedError"
    },
    {
      "context": "1):\n                        if any(token in line for token in [\"TODO\", \"pass\", \"NotImplementedError\"]):\n                            capability_map.setdefault(path, []).append({\"li",
      "file": "AUDIT_PROMPT.md",
      "line": 255,
      "match": "NotImplementedError"
    },
    {
      "context": "> List[Dict[str, Any]]:\n    entries: List[Dict[str, Any]] = []\n    patterns = [\"TODO\", \"NotImplementedError\"]\n    for file_path in _iter_python_files(ctx.root):\n   ",
      "file": "codex_task_sequence.py",
      "line": 222,
      "match": "\\bTODO\\b"
    },
    {
      "context": "ict[str, Any]]:\n    entries: List[Dict[str, Any]] = []\n    patterns = [\"TODO\", \"NotImplementedError\"]\n    for file_path in _iter_python_files(ctx.root):\n        try:\n            t",
      "file": "codex_task_sequence.py",
      "line": 222,
      "match": "NotImplementedError"
    },
    {
      "context": "data\": {},\n   \"source\": [\n    \"# GPU Training Example (Stub)\\n\",\n    \"\\n\",\n    \"TODO: Fill with end-to-end training demo.\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   ",
      "file": "notebooks/gpu_training_example.ipynb",
      "line": 10,
      "match": "\\bTODO\\b"
    },
    {
      "context": "args: Any, **_kwargs: Any) -> Tensor:  # pragma: no cover - guard\n        raise NotImplementedError(\"torch operations not available in offline stub\")\n\n    def topk(*args: Any, **k",
      "file": "torch/__init__.py",
      "line": 114,
      "match": "NotImplementedError"
    },
    {
      "context": " overall (`artifacts/metrics/docstring_coverage.json`).\n- Stub sweep highlights TODO clusters in analysis utilities (`artifacts/metrics/stub_summary.json`).\n\n## Cap",
      "file": "reports/_codex_status_update-2025-09-30.md",
      "line": 17,
      "match": "\\bTODO\\b"
    },
    {
      "context": "pass\\s*(#.*)?$] pass\n  - codex_update_runner.py:134 [\\bTODO\\b] stub_markers = (\"TODO\", \"NotImplementedError\", \"pass  # TODO\")\n  - codex_task_sequence.py:207 [\\bTODO",
      "file": "reports/iteration1_audit.md",
      "line": 116,
      "match": "\\bTODO\\b"
    },
    {
      "context": "unner.py:134 [\\bTODO\\b] stub_markers = (\"TODO\", \"NotImplementedError\", \"pass  # TODO\")\n  - codex_task_sequence.py:207 [\\bTODO\\b] patterns = [\"TODO\", \"NotImplemented",
      "file": "reports/iteration1_audit.md",
      "line": 116,
      "match": "\\bTODO\\b"
    },
    {
      "context": "dError\", \"pass  # TODO\")\n  - codex_task_sequence.py:207 [\\bTODO\\b] patterns = [\"TODO\", \"NotImplementedError\"]\n  - codex_task_sequence.py:448 [NotImplementedError] r",
      "file": "reports/iteration1_audit.md",
      "line": 117,
      "match": "\\bTODO\\b"
    },
    {
      "context": " codex_ready_task_sequence.yaml:42 [\\bTODO\\b] description: \"Scan repository for TODO, NotImplementedError, and bare pass statements.\"\n  - codex_ready_task_sequence.",
      "file": "reports/iteration1_audit.md",
      "line": 123,
      "match": "\\bTODO\\b"
    },
    {
      "context": "essages and link to deferred.md.\"\n  - mkdocs.yml:78 [\\bTODO\\b] strict: false  # TODO: enable strict once nav paths are verified\n  - codex_setup.py:50 [^\\s*pass\\s*(#",
      "file": "reports/iteration1_audit.md",
      "line": 125,
      "match": "\\bTODO\\b"
    },
    {
      "context": "#.*)?$] pass\n  - _codex_status_update-0C_base_-2025-09-27.md:109 [\\bTODO\\b] (no TODO/FIXME/NotImplementedError/pass signals sampled in small scan)\n  - codex_ast_upg",
      "file": "reports/iteration1_audit.md",
      "line": 128,
      "match": "\\bTODO\\b"
    },
    {
      "context": "(#.*)?$] pass\n  - CODEBASE_AUDIT_2025-08-26_203612.md:26 [\\bTODO\\b] - Keywords: TODO, FIXME, TBD, NotImplementedError, pass (empty bodies), raise NotImplementedErro",
      "file": "reports/iteration1_audit.md",
      "line": 136,
      "match": "\\bTODO\\b"
    },
    {
      "context": "ell_type\":\"markdown\",\"metadata\":{},\"source\":[\"# GPU Training Example (Stub)\\n\",\"TODO: Fill with end-to-end training demo.\"]},\n  - codex_workflow.py:50 [^\\s*pass\\s*(",
      "file": "reports/iteration1_audit.md",
      "line": 151,
      "match": "\\bTODO\\b"
    },
    {
      "context": "$] pass\n  - _codex_status_update-0C_base_-2025-09-27.md:109 [\\bTODO\\b] (no TODO/FIXME/NotImplementedError/pass signals sampled in small scan)\n  - codex_ast_upgrade.p",
      "file": "reports/iteration1_audit.md",
      "line": 128,
      "match": "\\bFIXME\\b"
    },
    {
      "context": "$] pass\n  - CODEBASE_AUDIT_2025-08-26_203612.md:26 [\\bTODO\\b] - Keywords: TODO, FIXME, TBD, NotImplementedError, pass (empty bodies), raise NotImplementedError\n  - C",
      "file": "reports/iteration1_audit.md",
      "line": 136,
      "match": "\\bFIXME\\b"
    },
    {
      "context": "#.*)?$] pass\n  - codex_update_runner.py:134 [\\bTODO\\b] stub_markers = (\"TODO\", \"NotImplementedError\", \"pass  # TODO\")\n  - codex_task_sequence.py:207 [\\bTODO\\b] patterns = [\"TODO\",",
      "file": "reports/iteration1_audit.md",
      "line": 116,
      "match": "NotImplementedError"
    },
    {
      "context": " \"pass  # TODO\")\n  - codex_task_sequence.py:207 [\\bTODO\\b] patterns = [\"TODO\", \"NotImplementedError\"]\n  - codex_task_sequence.py:448 [NotImplementedError] raise NotImplementedErro",
      "file": "reports/iteration1_audit.md",
      "line": 117,
      "match": "NotImplementedError"
    },
    {
      "context": "O\\b] patterns = [\"TODO\", \"NotImplementedError\"]\n  - codex_task_sequence.py:448 [NotImplementedError] raise NotImplementedError(\n  - codex_task_sequence.py:453 [NotImplementedError",
      "file": "reports/iteration1_audit.md",
      "line": 118,
      "match": "NotImplementedError"
    },
    {
      "context": "otImplementedError\"]\n  - codex_task_sequence.py:448 [NotImplementedError] raise NotImplementedError(\n  - codex_task_sequence.py:453 [NotImplementedError] raise NotImplementedError",
      "file": "reports/iteration1_audit.md",
      "line": 118,
      "match": "NotImplementedError"
    },
    {
      "context": "NotImplementedError] raise NotImplementedError(\n  - codex_task_sequence.py:453 [NotImplementedError] raise NotImplementedError(\n  - codex_task_sequence.py:458 [NotImplementedError",
      "file": "reports/iteration1_audit.md",
      "line": 119,
      "match": "NotImplementedError"
    },
    {
      "context": "NotImplementedError(\n  - codex_task_sequence.py:453 [NotImplementedError] raise NotImplementedError(\n  - codex_task_sequence.py:458 [NotImplementedError] raise NotImplementedError",
      "file": "reports/iteration1_audit.md",
      "line": 119,
      "match": "NotImplementedError"
    },
    {
      "context": "NotImplementedError] raise NotImplementedError(\n  - codex_task_sequence.py:458 [NotImplementedError] raise NotImplementedError(\n  - codex_task_sequence.py:463 [NotImplementedError",
      "file": "reports/iteration1_audit.md",
      "line": 120,
      "match": "NotImplementedError"
    },
    {
      "context": "NotImplementedError(\n  - codex_task_sequence.py:458 [NotImplementedError] raise NotImplementedError(\n  - codex_task_sequence.py:463 [NotImplementedError] raise NotImplementedError",
      "file": "reports/iteration1_audit.md",
      "line": 120,
      "match": "NotImplementedError"
    },
    {
      "context": "NotImplementedError] raise NotImplementedError(\n  - codex_task_sequence.py:463 [NotImplementedError] raise NotImplementedError(\n  - codex_task_sequence.py:492 [NotImplementedError",
      "file": "reports/iteration1_audit.md",
      "line": 121,
      "match": "NotImplementedError"
    },
    {
      "context": "NotImplementedError(\n  - codex_task_sequence.py:463 [NotImplementedError] raise NotImplementedError(\n  - codex_task_sequence.py:492 [NotImplementedError] raise NotImplementedError",
      "file": "reports/iteration1_audit.md",
      "line": 121,
      "match": "NotImplementedError"
    },
    {
      "context": "NotImplementedError] raise NotImplementedError(\n  - codex_task_sequence.py:492 [NotImplementedError] raise NotImplementedError(\n  - codex_ready_task_sequence.yaml:42 [\\bTODO\\b] de",
      "file": "reports/iteration1_audit.md",
      "line": 122,
      "match": "NotImplementedError"
    },
    {
      "context": "NotImplementedError(\n  - codex_task_sequence.py:492 [NotImplementedError] raise NotImplementedError(\n  - codex_ready_task_sequence.yaml:42 [\\bTODO\\b] description: \"Scan repository",
      "file": "reports/iteration1_audit.md",
      "line": 122,
      "match": "NotImplementedError"
    },
    {
      "context": "_ready_task_sequence.yaml:42 [\\bTODO\\b] description: \"Scan repository for TODO, NotImplementedError, and bare pass statements.\"\n  - codex_ready_task_sequence.yaml:77 [NotImplement",
      "file": "reports/iteration1_audit.md",
      "line": 123,
      "match": "NotImplementedError"
    },
    {
      "context": "ementedError, and bare pass statements.\"\n  - codex_ready_task_sequence.yaml:77 [NotImplementedError] description: \"Annotate deferred modules in-code with explicit NotImplementedEr",
      "file": "reports/iteration1_audit.md",
      "line": 124,
      "match": "NotImplementedError"
    },
    {
      "context": "ImplementedError] description: \"Annotate deferred modules in-code with explicit NotImplementedError messages and link to deferred.md.\"\n  - mkdocs.yml:78 [\\bTODO\\b] strict: false  ",
      "file": "reports/iteration1_audit.md",
      "line": 124,
      "match": "NotImplementedError"
    },
    {
      "context": "s\n  - _codex_status_update-0C_base_-2025-09-27.md:109 [\\bTODO\\b] (no TODO/FIXME/NotImplementedError/pass signals sampled in small scan)\n  - codex_ast_upgrade.py:176 [^\\s*pass\\s*(#",
      "file": "reports/iteration1_audit.md",
      "line": 128,
      "match": "NotImplementedError"
    },
    {
      "context": "odex_ast_upgrade.py:370 [^\\s*pass\\s*(#.*)?$] pass\n  - codex_ast_upgrade.py:393 [NotImplementedError] raise NotImplementedError\n  - codex_ast_upgrade.py:830 [^\\s*pass\\s*(#.*)?$] pa",
      "file": "reports/iteration1_audit.md",
      "line": 134,
      "match": "NotImplementedError"
    },
    {
      "context": "s*pass\\s*(#.*)?$] pass\n  - codex_ast_upgrade.py:393 [NotImplementedError] raise NotImplementedError\n  - codex_ast_upgrade.py:830 [^\\s*pass\\s*(#.*)?$] pass\n  - CODEBASE_AUDIT_2025-",
      "file": "reports/iteration1_audit.md",
      "line": 134,
      "match": "NotImplementedError"
    },
    {
      "context": "CODEBASE_AUDIT_2025-08-26_203612.md:26 [\\bTODO\\b] - Keywords: TODO, FIXME, TBD, NotImplementedError, pass (empty bodies), raise NotImplementedError\n  - CODEBASE_AUDIT_2025-08-26_2",
      "file": "reports/iteration1_audit.md",
      "line": 136,
      "match": "NotImplementedError"
    },
    {
      "context": "] - Keywords: TODO, FIXME, TBD, NotImplementedError, pass (empty bodies), raise NotImplementedError\n  - CODEBASE_AUDIT_2025-08-26_203612.md:56 [\\bTBD\\b] | Tokenization (fast token",
      "file": "reports/iteration1_audit.md",
      "line": 136,
      "match": "NotImplementedError"
    },
    {
      "context": "ass\n  - ...(and 9082 more)\n\n- Stub counts by pattern:\n  - `\\bTODO\\b`: 4533\n  - `NotImplementedError`: 4188\n  - `^\\s*pass\\s*(#.*)?$`: 365\n  - `\\bTBD\\b`: 30\n  - `\\bFIXME\\b`: 16\n\n## ",
      "file": "reports/iteration1_audit.md",
      "line": 165,
      "match": "NotImplementedError"
    },
    {
      "context": "update_runner.py:134 [\\bTODO\\b] stub_markers = (\"TODO\", \"NotImplementedError\", \"pass  # TODO\")\n  - codex_task_sequence.py:207 [\\bTODO\\b] patterns = [\"TODO\", \"NotImplemented",
      "file": "reports/iteration1_audit.md",
      "line": 116,
      "match": "\\bpass\\s*#\\s*TODO\\b"
    },
    {
      "context": " `noxfile.py`, `codex_update_runner.py`)\n\n- No runtime `NotImplementedError` or TODO comments remain; the `TODO` hits in\n  `codex_update_runner.py` stem from the li",
      "file": "reports/POST_CHECK_VALIDATION_2025-09-27.md",
      "line": 24,
      "match": "\\bTODO\\b"
    },
    {
      "context": "e_runner.py`)\n\n- No runtime `NotImplementedError` or TODO comments remain; the `TODO` hits in\n  `codex_update_runner.py` stem from the literal sentinel list used by",
      "file": "reports/POST_CHECK_VALIDATION_2025-09-27.md",
      "line": 24,
      "match": "\\bTODO\\b"
    },
    {
      "context": " `noxfile.py` continues to expose the deterministic test sessions without new\n  TODO debt.\n\n## Repository Histogram Snapshot\n\n- Running the validation tool with `--",
      "file": "reports/POST_CHECK_VALIDATION_2025-09-27.md",
      "line": 56,
      "match": "\\bTODO\\b"
    },
    {
      "context": "Histogram Snapshot\n\n- Running the validation tool with `--full` enumerated `51` TODO strings,\n  `41` `NotImplementedError` occurrences, and `320` bare `pass` statem",
      "file": "reports/POST_CHECK_VALIDATION_2025-09-27.md",
      "line": 60,
      "match": "\\bTODO\\b"
    },
    {
      "context": "lding (`codex_setup.py`, `noxfile.py`, `codex_update_runner.py`)\n\n- No runtime `NotImplementedError` or TODO comments remain; the `TODO` hits in\n  `codex_update_runner.py` stem fr",
      "file": "reports/POST_CHECK_VALIDATION_2025-09-27.md",
      "line": 24,
      "match": "NotImplementedError"
    },
    {
      "context": "and deployment shim all scan\n  clean with no TODOs or placeholders. The single `NotImplementedError` match is\n  part of the connector base module documentation describing the old ",
      "file": "reports/POST_CHECK_VALIDATION_2025-09-27.md",
      "line": 41,
      "match": "NotImplementedError"
    },
    {
      "context": "Running the validation tool with `--full` enumerated `51` TODO strings,\n  `41` `NotImplementedError` occurrences, and `320` bare `pass` statements\n  across the Python codebase. Th",
      "file": "reports/POST_CHECK_VALIDATION_2025-09-27.md",
      "line": 61,
      "match": "NotImplementedError"
    },
    {
      "context": "d (tokenization, training, evaluation and logging) are implemented. No obvious `NotImplementedError` or `pass` stubs were discovered in the inspected modules. However, some featur",
      "file": "reports/_codex_status_update-2025-10-05.md",
      "line": 42,
      "match": "NotImplementedError"
    },
    {
      "context": "operational tooling.\n\nThe original codebase exposed an empty stub that raised\n``NotImplementedError`` at runtime which made even smoke tests fail once the\nmodule was imported.  Th",
      "file": "src/codex_ml/connectors/base.py",
      "line": 4,
      "match": "NotImplementedError"
    },
    {
      "context": "f write(self, record: Mapping[str, Any] | MetricsRecord) -> None:\n        raise NotImplementedError\n\n\nclass NDJSONMetricsWriter(BaseMetricsWriter):\n    \"\"\"Append structured metric",
      "file": "src/codex_ml/metrics/writers.py",
      "line": 72,
      "match": "NotImplementedError"
    },
    {
      "context": "iene(req: HygieneReq) -> Dict[str, Any]:\n    findings = []\n    if req.diff and \"TODO\" in req.diff:\n        findings.append({\"type\": \"lint\", \"message\": \"Found TODO i",
      "file": "temp/bridge_codex_copilot_bridge/services/ita/app/main.py",
      "line": 50,
      "match": "\\bTODO\\b"
    },
    {
      "context": " \"TODO\" in req.diff:\n        findings.append({\"type\": \"lint\", \"message\": \"Found TODO in diff\"})\n    return {\"ok\": len(findings) == 0, \"findings\": findings}\n\n\n@app.p",
      "file": "temp/bridge_codex_copilot_bridge/services/ita/app/main.py",
      "line": 51,
      "match": "\\bTODO\\b"
    },
    {
      "context": "Repo Map**\n   - List top-level directories and key files.\n   - Identify stubs (`TODO`, `NotImplementedError`, `pass`, placeholders) and unimplemented areas.\n\n2. **C",
      "file": "docs/status_update_prompt.md",
      "line": 28,
      "match": "\\bTODO\\b"
    },
    {
      "context": "**\n   - List top-level directories and key files.\n   - Identify stubs (`TODO`, `NotImplementedError`, `pass`, placeholders) and unimplemented areas.\n\n2. **Capability Audit Table**",
      "file": "docs/status_update_prompt.md",
      "line": 28,
      "match": "NotImplementedError"
    },
    {
      "context": "\n- Full training engine (AMP, optimizer, proper grad accumulation loop) remains TODO.\n- Additional API /infer unit tests for tokenization + secret redaction are rec",
      "file": "docs/Implementation_Update_merged.md",
      "line": 58,
      "match": "\\bTODO\\b"
    },
    {
      "context": "# Gap Analysis Report\n\n- .codex/codex_repo_scout.py:238: TODO\n- .codex/codex_repo_scout.py:266: TODO\n- .codex/codex_repo_scout.py:268: NotImp",
      "file": "docs/gaps_report.md",
      "line": 3,
      "match": "\\bTODO\\b"
    },
    {
      "context": "eport\n\n- .codex/codex_repo_scout.py:238: TODO\n- .codex/codex_repo_scout.py:266: TODO\n- .codex/codex_repo_scout.py:268: NotImplementedError\n- .codex/codex_repo_scout",
      "file": "docs/gaps_report.md",
      "line": 4,
      "match": "\\bTODO\\b"
    },
    {
      "context": "/codex_repo_scout.py:315: NotImplementedError\n- .codex/codex_repo_scout.py:335: TODO\n- .codex/run_repo_scout.py:236: TODO\n- .codex/run_repo_scout.py:248: NotImpleme",
      "file": "docs/gaps_report.md",
      "line": 7,
      "match": "\\bTODO\\b"
    },
    {
      "context": "tedError\n- .codex/codex_repo_scout.py:335: TODO\n- .codex/run_repo_scout.py:236: TODO\n- .codex/run_repo_scout.py:248: NotImplementedError\n- .codex/run_repo_scout.py:",
      "file": "docs/gaps_report.md",
      "line": 8,
      "match": "\\bTODO\\b"
    },
    {
      "context": "odex/run_repo_scout.py:248: NotImplementedError\n- .codex/run_repo_scout.py:253: TODO\n- .codex/run_repo_scout.py:258: TODO\n- .codex/run_repo_scout.py:263: TODO\n- .co",
      "file": "docs/gaps_report.md",
      "line": 10,
      "match": "\\bTODO\\b"
    },
    {
      "context": "entedError\n- .codex/run_repo_scout.py:253: TODO\n- .codex/run_repo_scout.py:258: TODO\n- .codex/run_repo_scout.py:263: TODO\n- .codex/run_repo_scout.py:264: TODO\n- cod",
      "file": "docs/gaps_report.md",
      "line": 11,
      "match": "\\bTODO\\b"
    },
    {
      "context": ":253: TODO\n- .codex/run_repo_scout.py:258: TODO\n- .codex/run_repo_scout.py:263: TODO\n- .codex/run_repo_scout.py:264: TODO\n- codex_ast_upgrade.py:248: NotImplemented",
      "file": "docs/gaps_report.md",
      "line": 12,
      "match": "\\bTODO\\b"
    },
    {
      "context": ":258: TODO\n- .codex/run_repo_scout.py:263: TODO\n- .codex/run_repo_scout.py:264: TODO\n- codex_ast_upgrade.py:248: NotImplementedError\n- codex_digest/tokenizer.py:20:",
      "file": "docs/gaps_report.md",
      "line": 13,
      "match": "\\bTODO\\b"
    },
    {
      "context": "rror\n- codex_digest/tokenizer.py:20: NotImplementedError\n- codex_script.py:534: TODO\n- scripts/run_codex_tasks.py:101: TODO\n- scripts/run_codex_tasks.py:61: NotImpl",
      "file": "docs/gaps_report.md",
      "line": 16,
      "match": "\\bTODO\\b"
    },
    {
      "context": "tImplementedError\n- codex_script.py:534: TODO\n- scripts/run_codex_tasks.py:101: TODO\n- scripts/run_codex_tasks.py:61: NotImplementedError\n- scripts/run_codex_tasks.",
      "file": "docs/gaps_report.md",
      "line": 17,
      "match": "\\bTODO\\b"
    },
    {
      "context": "pts/run_codex_tasks.py:61: NotImplementedError\n- scripts/run_codex_tasks.py:61: TODO\n- scripts/run_codex_tasks.py:62: NotImplementedError\n- scripts/run_codex_tasks.",
      "file": "docs/gaps_report.md",
      "line": 19,
      "match": "\\bTODO\\b"
    },
    {
      "context": "pts/run_codex_tasks.py:62: NotImplementedError\n- scripts/run_codex_tasks.py:62: TODO\n- scripts/run_codex_tasks.py:62: pass  # TODO\n- scripts/run_codex_tasks.py:9: T",
      "file": "docs/gaps_report.md",
      "line": 21,
      "match": "\\bTODO\\b"
    },
    {
      "context": "\n- scripts/run_codex_tasks.py:62: TODO\n- scripts/run_codex_tasks.py:62: pass  # TODO\n- scripts/run_codex_tasks.py:9: TODO\n- src/codex_ml/analysis/providers.py:17: N",
      "file": "docs/gaps_report.md",
      "line": 22,
      "match": "\\bTODO\\b"
    },
    {
      "context": "O\n- scripts/run_codex_tasks.py:62: pass  # TODO\n- scripts/run_codex_tasks.py:9: TODO\n- src/codex_ml/analysis/providers.py:17: NotImplementedError\n- src/codex_ml/int",
      "file": "docs/gaps_report.md",
      "line": 23,
      "match": "\\bTODO\\b"
    },
    {
      "context": "cking/writers.py:12: NotImplementedError\n- tests/test_interfaces_compat.py:106: TODO\n- tests/test_interfaces_compat.py:107: TODO\n- tests/test_interfaces_compat.py:2",
      "file": "docs/gaps_report.md",
      "line": 37,
      "match": "\\bTODO\\b"
    },
    {
      "context": "ests/test_interfaces_compat.py:106: TODO\n- tests/test_interfaces_compat.py:107: TODO\n- tests/test_interfaces_compat.py:28: TODO\n- tests/test_interfaces_compat.py:42",
      "file": "docs/gaps_report.md",
      "line": 38,
      "match": "\\bTODO\\b"
    },
    {
      "context": "tests/test_interfaces_compat.py:107: TODO\n- tests/test_interfaces_compat.py:28: TODO\n- tests/test_interfaces_compat.py:42: TODO\n- tests/test_interfaces_compat.py:52",
      "file": "docs/gaps_report.md",
      "line": 39,
      "match": "\\bTODO\\b"
    },
    {
      "context": " tests/test_interfaces_compat.py:28: TODO\n- tests/test_interfaces_compat.py:42: TODO\n- tests/test_interfaces_compat.py:52: TODO\n- tests/test_offline_repo_auditor.py",
      "file": "docs/gaps_report.md",
      "line": 40,
      "match": "\\bTODO\\b"
    },
    {
      "context": " tests/test_interfaces_compat.py:42: TODO\n- tests/test_interfaces_compat.py:52: TODO\n- tests/test_offline_repo_auditor.py:16: NotImplementedError\n- tests/test_offli",
      "file": "docs/gaps_report.md",
      "line": 41,
      "match": "\\bTODO\\b"
    },
    {
      "context": "epo_auditor.py:16: NotImplementedError\n- tests/test_offline_repo_auditor.py:31: TODO\n- tests/test_offline_repo_auditor.py:9: TODO\n- tests/test_session_logging.py:10",
      "file": "docs/gaps_report.md",
      "line": 43,
      "match": "\\bTODO\\b"
    },
    {
      "context": "s/test_offline_repo_auditor.py:31: TODO\n- tests/test_offline_repo_auditor.py:9: TODO\n- tests/test_session_logging.py:106: NotImplementedError\n- tools/apply_ci_preco",
      "file": "docs/gaps_report.md",
      "line": 44,
      "match": "\\bTODO\\b"
    },
    {
      "context": "t_session_logging.py:106: NotImplementedError\n- tools/apply_ci_precommit.py:14: TODO\n- tools/apply_hydra_scaffold.py:155: TODO\n- tools/apply_interfaces.py:104: NotI",
      "file": "docs/gaps_report.md",
      "line": 46,
      "match": "\\bTODO\\b"
    },
    {
      "context": "ror\n- tools/apply_ci_precommit.py:14: TODO\n- tools/apply_hydra_scaffold.py:155: TODO\n- tools/apply_interfaces.py:104: NotImplementedError\n- tools/apply_interfaces.p",
      "file": "docs/gaps_report.md",
      "line": 47,
      "match": "\\bTODO\\b"
    },
    {
      "context": "s/apply_interfaces.py:179: NotImplementedError\n- tools/apply_interfaces.py:214: TODO\n- tools/apply_interfaces.py:227: TODO\n- tools/apply_interfaces.py:236: TODO\n- t",
      "file": "docs/gaps_report.md",
      "line": 58,
      "match": "\\bTODO\\b"
    },
    {
      "context": "tedError\n- tools/apply_interfaces.py:214: TODO\n- tools/apply_interfaces.py:227: TODO\n- tools/apply_interfaces.py:236: TODO\n- tools/apply_interfaces.py:287: TODO\n- t",
      "file": "docs/gaps_report.md",
      "line": 59,
      "match": "\\bTODO\\b"
    },
    {
      "context": "14: TODO\n- tools/apply_interfaces.py:227: TODO\n- tools/apply_interfaces.py:236: TODO\n- tools/apply_interfaces.py:287: TODO\n- tools/apply_interfaces.py:288: TODO\n- t",
      "file": "docs/gaps_report.md",
      "line": 60,
      "match": "\\bTODO\\b"
    },
    {
      "context": "27: TODO\n- tools/apply_interfaces.py:236: TODO\n- tools/apply_interfaces.py:287: TODO\n- tools/apply_interfaces.py:288: TODO\n- tools/apply_interfaces.py:296: TODO\n- t",
      "file": "docs/gaps_report.md",
      "line": 61,
      "match": "\\bTODO\\b"
    },
    {
      "context": "36: TODO\n- tools/apply_interfaces.py:287: TODO\n- tools/apply_interfaces.py:288: TODO\n- tools/apply_interfaces.py:296: TODO\n- tools/apply_interfaces.py:95: NotImplem",
      "file": "docs/gaps_report.md",
      "line": 62,
      "match": "\\bTODO\\b"
    },
    {
      "context": "87: TODO\n- tools/apply_interfaces.py:288: TODO\n- tools/apply_interfaces.py:296: TODO\n- tools/apply_interfaces.py:95: NotImplementedError\n- tools/apply_stack_polish.",
      "file": "docs/gaps_report.md",
      "line": 63,
      "match": "\\bTODO\\b"
    },
    {
      "context": "/apply_interfaces.py:95: NotImplementedError\n- tools/apply_stack_polish.py:553: TODO\n- tools/codex_exec.py:90: NotImplementedError\n- tools/codex_exec.py:90: TODO\n- ",
      "file": "docs/gaps_report.md",
      "line": 65,
      "match": "\\bTODO\\b"
    },
    {
      "context": "3: TODO\n- tools/codex_exec.py:90: NotImplementedError\n- tools/codex_exec.py:90: TODO\n- tools/codex_exec.py:92: TODO\n- tools/codex_patch_session_logging.py:182: NotI",
      "file": "docs/gaps_report.md",
      "line": 67,
      "match": "\\bTODO\\b"
    },
    {
      "context": "0: NotImplementedError\n- tools/codex_exec.py:90: TODO\n- tools/codex_exec.py:92: TODO\n- tools/codex_patch_session_logging.py:182: NotImplementedError\n- tools/codex_p",
      "file": "docs/gaps_report.md",
      "line": 68,
      "match": "\\bTODO\\b"
    },
    {
      "context": "sion_logging.py:281: NotImplementedError\n- tools/codex_workflow_executor.py:52: TODO\n- tools/codex_workflow_executor.py:54: TODO\n- tools/codex_workflow_executor.py:",
      "file": "docs/gaps_report.md",
      "line": 71,
      "match": "\\bTODO\\b"
    },
    {
      "context": "ools/codex_workflow_executor.py:52: TODO\n- tools/codex_workflow_executor.py:54: TODO\n- tools/codex_workflow_executor.py:65: TODO\n- tools/codex_workflow_executor.py:",
      "file": "docs/gaps_report.md",
      "line": 72,
      "match": "\\bTODO\\b"
    },
    {
      "context": "ools/codex_workflow_executor.py:54: TODO\n- tools/codex_workflow_executor.py:65: TODO\n- tools/codex_workflow_executor.py:70: TODO\n- tools/offline_repo_auditor.py:71:",
      "file": "docs/gaps_report.md",
      "line": 73,
      "match": "\\bTODO\\b"
    },
    {
      "context": "ools/codex_workflow_executor.py:65: TODO\n- tools/codex_workflow_executor.py:70: TODO\n- tools/offline_repo_auditor.py:71: TODO\n- tools/offline_repo_auditor.py:74: No",
      "file": "docs/gaps_report.md",
      "line": 74,
      "match": "\\bTODO\\b"
    },
    {
      "context": "- tools/codex_workflow_executor.py:70: TODO\n- tools/offline_repo_auditor.py:71: TODO\n- tools/offline_repo_auditor.py:74: NotImplementedError\n- tools/offline_repo_au",
      "file": "docs/gaps_report.md",
      "line": 75,
      "match": "\\bTODO\\b"
    },
    {
      "context": "fline_repo_auditor.py:8: NotImplementedError\n- tools/offline_repo_auditor.py:8: TODO\n",
      "file": "docs/gaps_report.md",
      "line": 79,
      "match": "\\bTODO\\b"
    },
    {
      "context": ": TODO\n- .codex/codex_repo_scout.py:266: TODO\n- .codex/codex_repo_scout.py:268: NotImplementedError\n- .codex/codex_repo_scout.py:315: NotImplementedError\n- .codex/codex_repo_scout",
      "file": "docs/gaps_report.md",
      "line": 5,
      "match": "NotImplementedError"
    },
    {
      "context": "/codex_repo_scout.py:268: NotImplementedError\n- .codex/codex_repo_scout.py:315: NotImplementedError\n- .codex/codex_repo_scout.py:335: TODO\n- .codex/run_repo_scout.py:236: TODO\n- .",
      "file": "docs/gaps_report.md",
      "line": 6,
      "match": "NotImplementedError"
    },
    {
      "context": ":335: TODO\n- .codex/run_repo_scout.py:236: TODO\n- .codex/run_repo_scout.py:248: NotImplementedError\n- .codex/run_repo_scout.py:253: TODO\n- .codex/run_repo_scout.py:258: TODO\n- .co",
      "file": "docs/gaps_report.md",
      "line": 9,
      "match": "NotImplementedError"
    },
    {
      "context": "t.py:263: TODO\n- .codex/run_repo_scout.py:264: TODO\n- codex_ast_upgrade.py:248: NotImplementedError\n- codex_digest/tokenizer.py:20: NotImplementedError\n- codex_script.py:534: TODO",
      "file": "docs/gaps_report.md",
      "line": 14,
      "match": "NotImplementedError"
    },
    {
      "context": "- codex_ast_upgrade.py:248: NotImplementedError\n- codex_digest/tokenizer.py:20: NotImplementedError\n- codex_script.py:534: TODO\n- scripts/run_codex_tasks.py:101: TODO\n- scripts/ru",
      "file": "docs/gaps_report.md",
      "line": 15,
      "match": "NotImplementedError"
    },
    {
      "context": "4: TODO\n- scripts/run_codex_tasks.py:101: TODO\n- scripts/run_codex_tasks.py:61: NotImplementedError\n- scripts/run_codex_tasks.py:61: TODO\n- scripts/run_codex_tasks.py:62: NotImple",
      "file": "docs/gaps_report.md",
      "line": 18,
      "match": "NotImplementedError"
    },
    {
      "context": "tedError\n- scripts/run_codex_tasks.py:61: TODO\n- scripts/run_codex_tasks.py:62: NotImplementedError\n- scripts/run_codex_tasks.py:62: TODO\n- scripts/run_codex_tasks.py:62: pass  # ",
      "file": "docs/gaps_report.md",
      "line": 20,
      "match": "NotImplementedError"
    },
    {
      "context": "O\n- scripts/run_codex_tasks.py:9: TODO\n- src/codex_ml/analysis/providers.py:17: NotImplementedError\n- src/codex_ml/interfaces/reward_model.py:20: NotImplementedError\n- src/codex_m",
      "file": "docs/gaps_report.md",
      "line": 24,
      "match": "NotImplementedError"
    },
    {
      "context": "viders.py:17: NotImplementedError\n- src/codex_ml/interfaces/reward_model.py:20: NotImplementedError\n- src/codex_ml/interfaces/reward_model.py:38: NotImplementedError\n- src/codex_m",
      "file": "docs/gaps_report.md",
      "line": 25,
      "match": "NotImplementedError"
    },
    {
      "context": "_model.py:20: NotImplementedError\n- src/codex_ml/interfaces/reward_model.py:38: NotImplementedError\n- src/codex_ml/interfaces/rl.py:14: NotImplementedError\n- src/codex_ml/interfac",
      "file": "docs/gaps_report.md",
      "line": 26,
      "match": "NotImplementedError"
    },
    {
      "context": "ces/reward_model.py:38: NotImplementedError\n- src/codex_ml/interfaces/rl.py:14: NotImplementedError\n- src/codex_ml/interfaces/rl.py:19: NotImplementedError\n- src/codex_ml/interfac",
      "file": "docs/gaps_report.md",
      "line": 27,
      "match": "NotImplementedError"
    },
    {
      "context": "ml/interfaces/rl.py:14: NotImplementedError\n- src/codex_ml/interfaces/rl.py:19: NotImplementedError\n- src/codex_ml/interfaces/rl.py:24: NotImplementedError\n- src/codex_ml/interfac",
      "file": "docs/gaps_report.md",
      "line": 28,
      "match": "NotImplementedError"
    },
    {
      "context": "ml/interfaces/rl.py:19: NotImplementedError\n- src/codex_ml/interfaces/rl.py:24: NotImplementedError\n- src/codex_ml/interfaces/rl.py:29: NotImplementedError\n- src/codex_ml/interfac",
      "file": "docs/gaps_report.md",
      "line": 29,
      "match": "NotImplementedError"
    },
    {
      "context": "ml/interfaces/rl.py:24: NotImplementedError\n- src/codex_ml/interfaces/rl.py:29: NotImplementedError\n- src/codex_ml/interfaces/tokenizer.py:51: NotImplementedError\n- src/codex_ml/i",
      "file": "docs/gaps_report.md",
      "line": 30,
      "match": "NotImplementedError"
    },
    {
      "context": "rfaces/rl.py:29: NotImplementedError\n- src/codex_ml/interfaces/tokenizer.py:51: NotImplementedError\n- src/codex_ml/interfaces/tokenizer.py:70: NotImplementedError\n- src/codex_ml/i",
      "file": "docs/gaps_report.md",
      "line": 31,
      "match": "NotImplementedError"
    },
    {
      "context": "tokenizer.py:51: NotImplementedError\n- src/codex_ml/interfaces/tokenizer.py:70: NotImplementedError\n- src/codex_ml/interfaces/tokenizer.py:80: NotImplementedError\n- src/codex_ml/p",
      "file": "docs/gaps_report.md",
      "line": 32,
      "match": "NotImplementedError"
    },
    {
      "context": "tokenizer.py:70: NotImplementedError\n- src/codex_ml/interfaces/tokenizer.py:80: NotImplementedError\n- src/codex_ml/pipeline.py:33: NotImplementedError\n- src/codex_ml/pipeline.py:3",
      "file": "docs/gaps_report.md",
      "line": 33,
      "match": "NotImplementedError"
    },
    {
      "context": "/interfaces/tokenizer.py:80: NotImplementedError\n- src/codex_ml/pipeline.py:33: NotImplementedError\n- src/codex_ml/pipeline.py:39: NotImplementedError\n- src/codex_ml/tracking/writ",
      "file": "docs/gaps_report.md",
      "line": 34,
      "match": "NotImplementedError"
    },
    {
      "context": "src/codex_ml/pipeline.py:33: NotImplementedError\n- src/codex_ml/pipeline.py:39: NotImplementedError\n- src/codex_ml/tracking/writers.py:12: NotImplementedError\n- tests/test_interfa",
      "file": "docs/gaps_report.md",
      "line": 35,
      "match": "NotImplementedError"
    },
    {
      "context": "x_ml/pipeline.py:39: NotImplementedError\n- src/codex_ml/tracking/writers.py:12: NotImplementedError\n- tests/test_interfaces_compat.py:106: TODO\n- tests/test_interfaces_compat.py:1",
      "file": "docs/gaps_report.md",
      "line": 36,
      "match": "NotImplementedError"
    },
    {
      "context": "sts/test_interfaces_compat.py:52: TODO\n- tests/test_offline_repo_auditor.py:16: NotImplementedError\n- tests/test_offline_repo_auditor.py:31: TODO\n- tests/test_offline_repo_auditor",
      "file": "docs/gaps_report.md",
      "line": 42,
      "match": "NotImplementedError"
    },
    {
      "context": "tests/test_offline_repo_auditor.py:9: TODO\n- tests/test_session_logging.py:106: NotImplementedError\n- tools/apply_ci_precommit.py:14: TODO\n- tools/apply_hydra_scaffold.py:155: TOD",
      "file": "docs/gaps_report.md",
      "line": 45,
      "match": "NotImplementedError"
    },
    {
      "context": "TODO\n- tools/apply_hydra_scaffold.py:155: TODO\n- tools/apply_interfaces.py:104: NotImplementedError\n- tools/apply_interfaces.py:109: NotImplementedError\n- tools/apply_interfaces.p",
      "file": "docs/gaps_report.md",
      "line": 48,
      "match": "NotImplementedError"
    },
    {
      "context": "s/apply_interfaces.py:104: NotImplementedError\n- tools/apply_interfaces.py:109: NotImplementedError\n- tools/apply_interfaces.py:114: NotImplementedError\n- tools/apply_interfaces.p",
      "file": "docs/gaps_report.md",
      "line": 49,
      "match": "NotImplementedError"
    },
    {
      "context": "s/apply_interfaces.py:109: NotImplementedError\n- tools/apply_interfaces.py:114: NotImplementedError\n- tools/apply_interfaces.py:119: NotImplementedError\n- tools/apply_interfaces.p",
      "file": "docs/gaps_report.md",
      "line": 50,
      "match": "NotImplementedError"
    },
    {
      "context": "s/apply_interfaces.py:114: NotImplementedError\n- tools/apply_interfaces.py:119: NotImplementedError\n- tools/apply_interfaces.py:135: NotImplementedError\n- tools/apply_interfaces.p",
      "file": "docs/gaps_report.md",
      "line": 51,
      "match": "NotImplementedError"
    },
    {
      "context": "s/apply_interfaces.py:119: NotImplementedError\n- tools/apply_interfaces.py:135: NotImplementedError\n- tools/apply_interfaces.py:148: NotImplementedError\n- tools/apply_interfaces.p",
      "file": "docs/gaps_report.md",
      "line": 52,
      "match": "NotImplementedError"
    },
    {
      "context": "s/apply_interfaces.py:135: NotImplementedError\n- tools/apply_interfaces.py:148: NotImplementedError\n- tools/apply_interfaces.py:164: NotImplementedError\n- tools/apply_interfaces.p",
      "file": "docs/gaps_report.md",
      "line": 53,
      "match": "NotImplementedError"
    },
    {
      "context": "s/apply_interfaces.py:148: NotImplementedError\n- tools/apply_interfaces.py:164: NotImplementedError\n- tools/apply_interfaces.py:169: NotImplementedError\n- tools/apply_interfaces.p",
      "file": "docs/gaps_report.md",
      "line": 54,
      "match": "NotImplementedError"
    },
    {
      "context": "s/apply_interfaces.py:164: NotImplementedError\n- tools/apply_interfaces.py:169: NotImplementedError\n- tools/apply_interfaces.py:174: NotImplementedError\n- tools/apply_interfaces.p",
      "file": "docs/gaps_report.md",
      "line": 55,
      "match": "NotImplementedError"
    },
    {
      "context": "s/apply_interfaces.py:169: NotImplementedError\n- tools/apply_interfaces.py:174: NotImplementedError\n- tools/apply_interfaces.py:179: NotImplementedError\n- tools/apply_interfaces.p",
      "file": "docs/gaps_report.md",
      "line": 56,
      "match": "NotImplementedError"
    },
    {
      "context": "s/apply_interfaces.py:174: NotImplementedError\n- tools/apply_interfaces.py:179: NotImplementedError\n- tools/apply_interfaces.py:214: TODO\n- tools/apply_interfaces.py:227: TODO\n- t",
      "file": "docs/gaps_report.md",
      "line": 57,
      "match": "NotImplementedError"
    },
    {
      "context": "288: TODO\n- tools/apply_interfaces.py:296: TODO\n- tools/apply_interfaces.py:95: NotImplementedError\n- tools/apply_stack_polish.py:553: TODO\n- tools/codex_exec.py:90: NotImplemente",
      "file": "docs/gaps_report.md",
      "line": 64,
      "match": "NotImplementedError"
    },
    {
      "context": "lementedError\n- tools/apply_stack_polish.py:553: TODO\n- tools/codex_exec.py:90: NotImplementedError\n- tools/codex_exec.py:90: TODO\n- tools/codex_exec.py:92: TODO\n- tools/codex_pat",
      "file": "docs/gaps_report.md",
      "line": 66,
      "match": "NotImplementedError"
    },
    {
      "context": "TODO\n- tools/codex_exec.py:92: TODO\n- tools/codex_patch_session_logging.py:182: NotImplementedError\n- tools/codex_patch_session_logging.py:281: NotImplementedError\n- tools/codex_w",
      "file": "docs/gaps_report.md",
      "line": 69,
      "match": "NotImplementedError"
    },
    {
      "context": "logging.py:182: NotImplementedError\n- tools/codex_patch_session_logging.py:281: NotImplementedError\n- tools/codex_workflow_executor.py:52: TODO\n- tools/codex_workflow_executor.py:",
      "file": "docs/gaps_report.md",
      "line": 70,
      "match": "NotImplementedError"
    },
    {
      "context": "DO\n- tools/offline_repo_auditor.py:71: TODO\n- tools/offline_repo_auditor.py:74: NotImplementedError\n- tools/offline_repo_auditor.py:75: NotImplementedError\n- tools/offline_repo_au",
      "file": "docs/gaps_report.md",
      "line": 76,
      "match": "NotImplementedError"
    },
    {
      "context": "ine_repo_auditor.py:74: NotImplementedError\n- tools/offline_repo_auditor.py:75: NotImplementedError\n- tools/offline_repo_auditor.py:8: NotImplementedError\n- tools/offline_repo_aud",
      "file": "docs/gaps_report.md",
      "line": 77,
      "match": "NotImplementedError"
    },
    {
      "context": "line_repo_auditor.py:75: NotImplementedError\n- tools/offline_repo_auditor.py:8: NotImplementedError\n- tools/offline_repo_auditor.py:8: TODO\n",
      "file": "docs/gaps_report.md",
      "line": 78,
      "match": "NotImplementedError"
    },
    {
      "context": "tedError\n- scripts/run_codex_tasks.py:62: TODO\n- scripts/run_codex_tasks.py:62: pass  # TODO\n- scripts/run_codex_tasks.py:9: TODO\n- src/codex_ml/analysis/providers.py:17: N",
      "file": "docs/gaps_report.md",
      "line": 22,
      "match": "\\bpass\\s*#\\s*TODO\\b"
    },
    {
      "context": "eep Research Prompts for Repository TODOs\n\nThis document enumerates all current TODO comments in the repository and provides suggested prompts for ChatGPT-5 Deep Re",
      "file": "docs/deep_research_prompts.md",
      "line": 3,
      "match": "\\bTODO\\b"
    },
    {
      "context": "tions:** `tools/apply_hydra_scaffold.py:155`, `src/codex_ml/cli/main.py:46`\n- **TODO:** Implement real step handlers; currently the pipeline simply reports success.",
      "file": "docs/deep_research_prompts.md",
      "line": 8,
      "match": "\\bTODO\\b"
    },
    {
      "context": "sh.py:553`, `codex_script.py:516`, `notebooks/gpu_training_example.ipynb:4`\n- **TODO:** Fill the GPU training example with an end-to-end demo.\n- **Research Prompt:*",
      "file": "docs/deep_research_prompts.md",
      "line": 14,
      "match": "\\bTODO\\b"
    },
    {
      "context": " `tests/test_interfaces_compat.py:42`, `tests/test_interfaces_compat.py:50`\n- **TODO:** Supply constructor kwargs where needed when instantiating Tokenizer, RewardM",
      "file": "docs/deep_research_prompts.md",
      "line": 20,
      "match": "\\bTODO\\b"
    },
    {
      "context": "ons:** `configs/interfaces.example.yaml:3`, `tools/apply_interfaces.py:249`\n- **TODO:** Replace placeholder `yourpkg.tokenizers.hf:HFTokenizer` with actual module a",
      "file": "docs/deep_research_prompts.md",
      "line": 34,
      "match": "\\bTODO\\b"
    },
    {
      "context": "s.\"\n\n## README Badge Slug\n\n- **Location:** `tools/apply_ci_precommit.py:14`\n- **TODO:** README badges still reference a TODO repository slug.\n- **Research Prompt:**",
      "file": "docs/deep_research_prompts.md",
      "line": 40,
      "match": "\\bTODO\\b"
    },
    {
      "context": "** `tools/apply_ci_precommit.py:14`\n- **TODO:** README badges still reference a TODO repository slug.\n- **Research Prompt:** \"Determine the correct GitHub repositor",
      "file": "docs/deep_research_prompts.md",
      "line": 40,
      "match": "\\bTODO\\b"
    },
    {
      "context": "ADME.md`.\"\n\n## PEFT LoRA Integration\n\n- **Location:** `codex_script.py:244`\n- **TODO:** Wire `peft.get_peft_model(model, LoraConfig(**cfg))` into `apply_lora`.\n- **",
      "file": "docs/deep_research_prompts.md",
      "line": 46,
      "match": "\\bTODO\\b"
    },
    {
      "context": "th\n\n\ndef main() -> None:\n    \"\"\"Entry point for Codex remediation.\"\"\"\n    raise NotImplementedError(\"{{implement_context_loader}}\")\n\n\nif __name__ == \"__main__\":\n    main()\n    pri",
      "file": "docs/status_updates/status_update_{{date}}.md",
      "line": 133,
      "match": "NotImplementedError"
    },
    {
      "context": " return \"\\n\".join(\n            [\n                \"try:\",\n                \"    # TODO: place guarded logic here\",\n                \"    pass\",\n                \"except",
      "file": "analysis/intuitive_aptitude.py",
      "line": 658,
      "match": "\\bTODO\\b"
    },
    {
      "context": "   \"    pass\",\n                \"except Exception as e:\",\n                \"    # TODO: refine exception types and remediation\",\n                \"    raise\",\n        ",
      "file": "analysis/intuitive_aptitude.py",
      "line": 661,
      "match": "\\bTODO\\b"
    },
    {
      "context": "ell_type\":\"markdown\",\"metadata\":{},\"source\":[\"# GPU Training Example (Stub)\\n\",\"TODO: Fill with end-to-end training demo.\"]},\n  {\"cell_type\":\"code\",\"metadata\":{},\"e",
      "file": "tools/apply_stack_polish.py",
      "line": 553,
      "match": "\\bTODO\\b"
    },
    {
      "context": " \".markdown\", \".mdx\"),\n        \"yaml\": (\".yml\", \".yaml\"),\n    }\n    markers = (\"TODO\", \"NotImplemented\", \"pass\")\n    details: list[str] = []\n    for label, suffixes",
      "file": "tools/post_check_validation.py",
      "line": 314,
      "match": "\\bTODO\\b"
    },
    {
      "context": "hecks\": [check.to_dict() for check in self.checks]}\n\n\n_DEF_STUB_MARKER = \"raise NotImplementedError\"\n\n\ndef _no_notimplemented(name: str, paths: Sequence[Path]) -> CheckReport:\n   ",
      "file": "tools/post_check_validation.py",
      "line": 66,
      "match": "NotImplementedError"
    },
    {
      "context": "debases.\n\n- Walks a repository tree without any network access\n- Detects stubs (TODO/FIXME/TBD, NotImplementedError, pass placeholders)\n- Summarizes structure (dirs",
      "file": "tools/offline_repo_auditor.py",
      "line": 8,
      "match": "\\bTODO\\b"
    },
    {
      "context": "es.\n\n- Walks a repository tree without any network access\n- Detects stubs (TODO/FIXME/TBD, NotImplementedError, pass placeholders)\n- Summarizes structure (dirs/files",
      "file": "tools/offline_repo_auditor.py",
      "line": 8,
      "match": "\\bFIXME\\b"
    },
    {
      "context": "s a repository tree without any network access\n- Detects stubs (TODO/FIXME/TBD, NotImplementedError, pass placeholders)\n- Summarizes structure (dirs/files), languages, and pattern",
      "file": "tools/offline_repo_auditor.py",
      "line": 8,
      "match": "NotImplementedError"
    },
    {
      "context": "b\"),\n    re.compile(r\"\\bFIXME\\b\"),\n    re.compile(r\"\\bTBD\\b\"),\n    re.compile(r\"NotImplementedError\"),\n    re.compile(r\"\\braise\\s+NotImplementedError\\b\"),\n    re.compile(r\"^\\s*pas",
      "file": "tools/offline_repo_auditor.py",
      "line": 78,
      "match": "NotImplementedError"
    },
    {
      "context": "r\"\\bTBD\\b\"),\n    re.compile(r\"NotImplementedError\"),\n    re.compile(r\"\\braise\\s+NotImplementedError\\b\"),\n    re.compile(r\"^\\s*pass\\s*(#.*)?$\"),\n]\n\n\n@dataclass\nclass FileFinding:\n ",
      "file": "tools/offline_repo_auditor.py",
      "line": 79,
      "match": "NotImplementedError"
    },
    {
      "context": "isabled\n- pyproject.toml or pytest.ini (coverage gate)\n- README.md (badges with TODO repo slug)\n\nValidations (local, best-effort): pre-commit, black/isort/flake8/my",
      "file": "tools/apply_ci_precommit.py",
      "line": 14,
      "match": "\\bTODO\\b"
    },
    {
      "context": " dry_run={cfg.dry_run}\")\n        if cfg.dry_run:\n            continue\n        # TODO: Implement real step handlers; here we simulate success\n    return 0\n\n@hydra.ma",
      "file": "tools/apply_hydra_scaffold.py",
      "line": 153,
      "match": "\\bTODO\\b"
    },
    {
      "context": "ncrete implementations\ntokenizer:\n  path: yourpkg.tokenizers.hf:HFTokenizer   # TODO: replace with actual module:class\n  kwargs: {}\nreward_model:\n  path: yourpkg.re",
      "file": "tools/apply_interfaces.py",
      "line": 329,
      "match": "\\bTODO\\b"
    },
    {
      "context": "[int]:\n        \\\"\\\"\\\"Encode a single string into token ids.\\\"\\\"\\\"\n        raise NotImplementedError\n\n    def batch_encode(self, texts: Iterable[str], *, add_special_tokens: bool =",
      "file": "tools/apply_interfaces.py",
      "line": 95,
      "match": "NotImplementedError"
    },
    {
      "context": "True) -> str:\n        \\\"\\\"\\\"Decode token ids into a string.\\\"\\\"\\\"\n        raise NotImplementedError\n\n    @abstractmethod\n    def vocab_size(self) -> int:\n        \\\"\\\"\\\"Return size",
      "file": "tools/apply_interfaces.py",
      "line": 104,
      "match": "NotImplementedError"
    },
    {
      "context": "size(self) -> int:\n        \\\"\\\"\\\"Return size of vocabulary.\\\"\\\"\\\"\n        raise NotImplementedError\n\n    @abstractmethod\n    def pad_id(self) -> int:\n        \\\"\\\"\\\"Return padding ",
      "file": "tools/apply_interfaces.py",
      "line": 109,
      "match": "NotImplementedError"
    },
    {
      "context": "pad_id(self) -> int:\n        \\\"\\\"\\\"Return padding token id.\\\"\\\"\\\"\n        raise NotImplementedError\n\n    @abstractmethod\n    def eos_id(self) -> int:\n        \\\"\\\"\\\"Return end-of-s",
      "file": "tools/apply_interfaces.py",
      "line": 114,
      "match": "NotImplementedError"
    },
    {
      "context": "elf) -> int:\n        \\\"\\\"\\\"Return end-of-sequence token id.\\\"\\\"\\\"\n        raise NotImplementedError\n# END: CODEX_IFACE_TOKENIZER\n\"\"\"\n\nS_REWARD = \"# BEGIN: CODEX_IFACE_REWARD\"\nREWA",
      "file": "tools/apply_interfaces.py",
      "line": 119,
      "match": "NotImplementedError"
    },
    {
      "context": "t:\n        \\\"\\\"\\\"Return a scalar reward (higher is better).\\\"\\\"\\\"\n        raise NotImplementedError\n\n    def batch_evaluate(self, pairs: list[tuple[str, str]], *, metadatas: Optio",
      "file": "tools/apply_interfaces.py",
      "line": 135,
      "match": "NotImplementedError"
    },
    {
      "context": "ate model parameters from data and return training metrics.\\\"\\\"\\\"\n        raise NotImplementedError\n# END: CODEX_IFACE_REWARD\n\"\"\"\n\nS_RL = \"# BEGIN: CODEX_IFACE_RL\"\nRL = f\"\"\"{S_RL}",
      "file": "tools/apply_interfaces.py",
      "line": 148,
      "match": "NotImplementedError"
    },
    {
      "context": "-> Any:\n        \\\"\\\"\\\"Choose an action for the given state.\\\"\\\"\\\"\n        raise NotImplementedError\n\n    @abstractmethod\n    def update(self, trajectory: Mapping[str, Any]) -> dic",
      "file": "tools/apply_interfaces.py",
      "line": 164,
      "match": "NotImplementedError"
    },
    {
      "context": "te agent from a trajectory and return metrics (e.g., loss).\\\"\\\"\\\"\n        raise NotImplementedError\n\n    @abstractmethod\n    def save(self, path: str) -> None:\n        \\\"\\\"\\\"Persi",
      "file": "tools/apply_interfaces.py",
      "line": 169,
      "match": "NotImplementedError"
    },
    {
      "context": "elf, path: str) -> None:\n        \\\"\\\"\\\"Persist agent state.\\\"\\\"\\\"\n        raise NotImplementedError\n\n    @abstractmethod\n    def load(self, path: str) -> None:\n        \\\"\\\"\\\"Resto",
      "file": "tools/apply_interfaces.py",
      "line": 174,
      "match": "NotImplementedError"
    },
    {
      "context": "elf, path: str) -> None:\n        \\\"\\\"\\\"Restore agent state.\\\"\\\"\\\"\n        raise NotImplementedError\n# END: CODEX_IFACE_RL\n\"\"\"\n\nS_INIT = \"# BEGIN: CODEX_IFACE_INIT\"\nINIT = f\"\"\"{S_I",
      "file": "tools/apply_interfaces.py",
      "line": 179,
      "match": "NotImplementedError"
    },
    {
      "context": ".__class__.__name__, e)\n{indent2}if isinstance(e, (ImportError, AttributeError, NotImplementedError)):\n{indent2}    pytest.skip(f\"Required session logging hook not available: {{e!",
      "file": "tools/codex_patch_session_logging.py",
      "line": 182,
      "match": "NotImplementedError"
    },
    {
      "context": "skip semantics for missing hooks \"\n                \"(ImportError/AttributeError/NotImplementedError) and otherwise fail.\"\n            )\n            add_change(TARGET_REL, \"Patch e",
      "file": "tools/codex_patch_session_logging.py",
      "line": 281,
      "match": "NotImplementedError"
    },
    {
      "context": "xception:\n        pass\n\n\ndef find_todos():\n    out = sh([\"bash\", \"-lc\", 'rg -n \"TODO|NotImplementedError\" || true'], \"Phase 2: scan TODOs\")\n    if out.stdout.strip(",
      "file": "tools/codex_exec.py",
      "line": 90,
      "match": "\\bTODO\\b"
    },
    {
      "context": "se 2: scan TODOs\")\n    if out.stdout.strip():\n        append_changelog(\"- scan: TODO/NotImplemented present; see ripgrep output in local logs\")\n\n\ndef write_security",
      "file": "tools/codex_exec.py",
      "line": 92,
      "match": "\\bTODO\\b"
    },
    {
      "context": "ion:\n        pass\n\n\ndef find_todos():\n    out = sh([\"bash\", \"-lc\", 'rg -n \"TODO|NotImplementedError\" || true'], \"Phase 2: scan TODOs\")\n    if out.stdout.strip():\n        append_ch",
      "file": "tools/codex_exec.py",
      "line": 90,
      "match": "NotImplementedError"
    },
    {
      "context": "_ROOT / \"tools\", REPO_ROOT / \"scripts\"]\n    keywords = (\"NotImplementedError\", \"TODO\", \"FIXME\", \"pass  # stub\")\n    findings: List[Dict[str, object]] = []\n    for b",
      "file": "tools/status/status_update_executor.py",
      "line": 122,
      "match": "\\bTODO\\b"
    },
    {
      "context": "\"tools\", REPO_ROOT / \"scripts\"]\n    keywords = (\"NotImplementedError\", \"TODO\", \"FIXME\", \"pass  # stub\")\n    findings: List[Dict[str, object]] = []\n    for base in ta",
      "file": "tools/status/status_update_executor.py",
      "line": 122,
      "match": "\\bFIXME\\b"
    },
    {
      "context": "REPO_ROOT / \"src\", REPO_ROOT / \"tools\", REPO_ROOT / \"scripts\"]\n    keywords = (\"NotImplementedError\", \"TODO\", \"FIXME\", \"pass  # stub\")\n    findings: List[Dict[str, object]] = []\n ",
      "file": "tools/status/status_update_executor.py",
      "line": 122,
      "match": "NotImplementedError"
    },
    {
      "context": "aceholder\",\n    \"stub\",\n    \"disabled\",\n    \"coming soon\",\n)\n\nTODO_KEYWORDS = (\"TODO\", \"FIXME\", \"XXX\")\n\nDOC_TOPICS = (\n    \"tokenizer CLI\",\n    \"ExternalWebSearch\",",
      "file": "tools/status/generate_status_update.py",
      "line": 60,
      "match": "\\bTODO\\b"
    },
    {
      "context": "otal ({summary_counts['stubs_by_kind']}) |\"\n    )\n    lines.append(\n        f\"| TODO/FIXME/XXX occurrences | {summary_counts['todo_total']} across {summary_counts['",
      "file": "tools/status/generate_status_update.py",
      "line": 900,
      "match": "\\bTODO\\b"
    },
    {
      "context": "xtend(render_table_rows(stub_rows))\n    lines.append(\"\")\n\n    lines.append(\"### TODO/FIXME/XXX Counts\")\n    lines.append(\"| File | TODO | FIXME | XXX | Total |\")\n  ",
      "file": "tools/status/generate_status_update.py",
      "line": 924,
      "match": "\\bTODO\\b"
    },
    {
      "context": "d(\"\")\n\n    lines.append(\"### TODO/FIXME/XXX Counts\")\n    lines.append(\"| File | TODO | FIXME | XXX | Total |\")\n    lines.append(\"| --- | --- | --- | --- | --- |\")\n ",
      "file": "tools/status/generate_status_update.py",
      "line": 925,
      "match": "\\bTODO\\b"
    },
    {
      "context": "r\",\n    \"stub\",\n    \"disabled\",\n    \"coming soon\",\n)\n\nTODO_KEYWORDS = (\"TODO\", \"FIXME\", \"XXX\")\n\nDOC_TOPICS = (\n    \"tokenizer CLI\",\n    \"ExternalWebSearch\",\n    \"Hyd",
      "file": "tools/status/generate_status_update.py",
      "line": 60,
      "match": "\\bFIXME\\b"
    },
    {
      "context": "({summary_counts['stubs_by_kind']}) |\"\n    )\n    lines.append(\n        f\"| TODO/FIXME/XXX occurrences | {summary_counts['todo_total']} across {summary_counts['todo_f",
      "file": "tools/status/generate_status_update.py",
      "line": 900,
      "match": "\\bFIXME\\b"
    },
    {
      "context": "(render_table_rows(stub_rows))\n    lines.append(\"\")\n\n    lines.append(\"### TODO/FIXME/XXX Counts\")\n    lines.append(\"| File | TODO | FIXME | XXX | Total |\")\n    line",
      "file": "tools/status/generate_status_update.py",
      "line": 924,
      "match": "\\bFIXME\\b"
    },
    {
      "context": "    lines.append(\"### TODO/FIXME/XXX Counts\")\n    lines.append(\"| File | TODO | FIXME | XXX | Total |\")\n    lines.append(\"| --- | --- | --- | --- | --- |\")\n    todo_",
      "file": "tools/status/generate_status_update.py",
      "line": 925,
      "match": "\\bFIXME\\b"
    },
    {
      "context": ")\n                lowered_name = name.lower()\n                if name.endswith(\"NotImplementedError\") or lowered_name == \"notimplementederror\":\n                    self.record(\"No",
      "file": "tools/status/generate_status_update.py",
      "line": 288,
      "match": "NotImplementedError"
    },
    {
      "context": "or\") or lowered_name == \"notimplementederror\":\n                    self.record(\"NotImplementedError\", node, detail=message)\n                    return\n                if classify_",
      "file": "tools/status/generate_status_update.py",
      "line": 289,
      "match": "NotImplementedError"
    },
    {
      "context": " = name.lower() if name else \"\"\n            if (\n                name.endswith(\"NotImplementedError\")\n                or \"notimplemented\" in lowered_name\n                or classi",
      "file": "tools/status/generate_status_update.py",
      "line": 431,
      "match": "NotImplementedError"
    },
    {
      "context": "stub\",\n    \"disabled\",\n    \"coming soon\",\n)\n\nTODO_KEYWORDS = (\"TODO\", \"FIXME\", \"XXX\")\n\nDOC_TOPICS = (\n    \"tokenizer CLI\",\n    \"ExternalWebSearch\",\n    \"Hydra swee",
      "file": "tools/status/generate_status_update.py",
      "line": 60,
      "match": "\\bXXX\\b"
    },
    {
      "context": "ary_counts['stubs_by_kind']}) |\"\n    )\n    lines.append(\n        f\"| TODO/FIXME/XXX occurrences | {summary_counts['todo_total']} across {summary_counts['todo_files",
      "file": "tools/status/generate_status_update.py",
      "line": 900,
      "match": "\\bXXX\\b"
    },
    {
      "context": "r_table_rows(stub_rows))\n    lines.append(\"\")\n\n    lines.append(\"### TODO/FIXME/XXX Counts\")\n    lines.append(\"| File | TODO | FIXME | XXX | Total |\")\n    lines.ap",
      "file": "tools/status/generate_status_update.py",
      "line": 924,
      "match": "\\bXXX\\b"
    },
    {
      "context": "s.append(\"### TODO/FIXME/XXX Counts\")\n    lines.append(\"| File | TODO | FIXME | XXX | Total |\")\n    lines.append(\"| --- | --- | --- | --- | --- |\")\n    todo_rows =",
      "file": "tools/status/generate_status_update.py",
      "line": 925,
      "match": "\\bXXX\\b"
    },
    {
      "context": "t, wrap into a JSON record with message\n- AUDIT_PROMPT.md:24: - Identify stubs (TODO, NotImplementedError, pass, placeholders) and unimplemented areas.\n- CHANGELOG_",
      "file": ".codex/change_log-large.md",
      "line": 100,
      "match": "\\bTODO\\b"
    },
    {
      "context": "ell_type\":\"markdown\",\"metadata\":{},\"source\":[\"# GPU Training Example (Stub)\\n\",\"TODO: Fill with end-to-end training d\n- codex_ast_upgrade.py:119: pass\n- codex_ast_u",
      "file": ".codex/change_log-large.md",
      "line": 108,
      "match": "\\bTODO\\b"
    },
    {
      "context": "_ast_upgrade.py:382: pass\n- CODEBASE_AUDIT_2025-08-26_203612.md:26: - Keywords: TODO, FIXME, TBD, NotImplementedError, pass (empty bodies), raise NotImplementedErro",
      "file": ".codex/change_log-large.md",
      "line": 117,
      "match": "\\bTODO\\b"
    },
    {
      "context": "ex_digest/tokenizer.py:69: pass\n- docs/gaps_report.md:3: - codex_script.py:534: TODO\n- docs/gaps_report.md:4: - codex_ast_upgrade.py:248: NotImplementedError\n- docs",
      "file": ".codex/change_log-large.md",
      "line": 125,
      "match": "\\bTODO\\b"
    },
    {
      "context": " NotImplementedError\n- docs/gaps_report.md:5: - .codex/codex_repo_scout.py:238: TODO\n- docs/gaps_report.md:6: - .codex/codex_repo_scout.py:266: TODO\n- docs/gaps_rep",
      "file": ".codex/change_log-large.md",
      "line": 127,
      "match": "\\bTODO\\b"
    },
    {
      "context": "o_scout.py:238: TODO\n- docs/gaps_report.md:6: - .codex/codex_repo_scout.py:266: TODO\n- docs/gaps_report.md:7: - .codex/codex_repo_scout.py:335: TODO\n- docs/gaps_rep",
      "file": ".codex/change_log-large.md",
      "line": 128,
      "match": "\\bTODO\\b"
    },
    {
      "context": "o_scout.py:266: TODO\n- docs/gaps_report.md:7: - .codex/codex_repo_scout.py:335: TODO\n- docs/gaps_report.md:8: - .codex/codex_repo_scout.py:268: NotImplementedError\n",
      "file": ".codex/change_log-large.md",
      "line": 129,
      "match": "\\bTODO\\b"
    },
    {
      "context": ": NotImplementedError\n- docs/gaps_report.md:10: - .codex/run_repo_scout.py:236: TODO\n- docs/gaps_report.md:11: - .codex/run_repo_scout.py:253: TODO\n- docs/gaps_repo",
      "file": ".codex/change_log-large.md",
      "line": 132,
      "match": "\\bTODO\\b"
    },
    {
      "context": "po_scout.py:236: TODO\n- docs/gaps_report.md:11: - .codex/run_repo_scout.py:253: TODO\n- docs/gaps_report.md:12: - .codex/run_repo_scout.py:258: TODO\n- docs/gaps_repo",
      "file": ".codex/change_log-large.md",
      "line": 133,
      "match": "\\bTODO\\b"
    },
    {
      "context": "po_scout.py:253: TODO\n- docs/gaps_report.md:12: - .codex/run_repo_scout.py:258: TODO\n- docs/gaps_report.md:13: - .codex/run_repo_scout.py:263: TODO\n- docs/gaps_repo",
      "file": ".codex/change_log-large.md",
      "line": 134,
      "match": "\\bTODO\\b"
    },
    {
      "context": "po_scout.py:258: TODO\n- docs/gaps_report.md:13: - .codex/run_repo_scout.py:263: TODO\n- docs/gaps_report.md:14: - .codex/run_repo_scout.py:263: TODO\n- docs/gaps_repo",
      "file": ".codex/change_log-large.md",
      "line": 135,
      "match": "\\bTODO\\b"
    },
    {
      "context": "po_scout.py:263: TODO\n- docs/gaps_report.md:14: - .codex/run_repo_scout.py:263: TODO\n- docs/gaps_report.md:15: - .codex/run_repo_scout.py:264: TODO\n- docs/gaps_repo",
      "file": ".codex/change_log-large.md",
      "line": 136,
      "match": "\\bTODO\\b"
    },
    {
      "context": "po_scout.py:263: TODO\n- docs/gaps_report.md:15: - .codex/run_repo_scout.py:264: TODO\n- docs/gaps_report.md:16: - .codex/run_repo_scout.py:248: NotImplementedError\n-",
      "file": ".codex/change_log-large.md",
      "line": 137,
      "match": "\\bTODO\\b"
    },
    {
      "context": ": NotImplementedError\n- docs/gaps_report.md:18: - scripts/run_codex_tasks.py:9: TODO\n- docs/gaps_report.md:19: - scripts/run_codex_tasks.py:61: TODO\n- docs/gaps_rep",
      "file": ".codex/change_log-large.md",
      "line": 140,
      "match": "\\bTODO\\b"
    },
    {
      "context": "dex_tasks.py:9: TODO\n- docs/gaps_report.md:19: - scripts/run_codex_tasks.py:61: TODO\n- docs/gaps_report.md:20: - scripts/run_codex_tasks.py:62: TODO\n- docs/gaps_rep",
      "file": ".codex/change_log-large.md",
      "line": 141,
      "match": "\\bTODO\\b"
    },
    {
      "context": "ex_tasks.py:61: TODO\n- docs/gaps_report.md:20: - scripts/run_codex_tasks.py:62: TODO\n- docs/gaps_report.md:21: - scripts/run_codex_tasks.py:62: TODO\n- docs/gaps_rep",
      "file": ".codex/change_log-large.md",
      "line": 142,
      "match": "\\bTODO\\b"
    },
    {
      "context": "ex_tasks.py:62: TODO\n- docs/gaps_report.md:21: - scripts/run_codex_tasks.py:62: TODO\n- docs/gaps_report.md:22: - scripts/run_codex_tasks.py:101: TODO\n- docs/gaps_re",
      "file": ".codex/change_log-large.md",
      "line": 143,
      "match": "\\bTODO\\b"
    },
    {
      "context": "x_tasks.py:62: TODO\n- docs/gaps_report.md:22: - scripts/run_codex_tasks.py:101: TODO\n- docs/gaps_report.md:23: - scripts/run_codex_tasks.py:61: NotImplementedError\n",
      "file": ".codex/change_log-large.md",
      "line": 144,
      "match": "\\bTODO\\b"
    },
    {
      "context": "ementedError\n- docs/gaps_report.md:25: - scripts/run_codex_tasks.py:62: pass  # TODO\n- docs/gaps_report.md:26: - tests/test_offline_repo_auditor.py:9: TODO\n- docs/g",
      "file": ".codex/change_log-large.md",
      "line": 147,
      "match": "\\bTODO\\b"
    },
    {
      "context": " pass  # TODO\n- docs/gaps_report.md:26: - tests/test_offline_repo_auditor.py:9: TODO\n- docs/gaps_report.md:27: - tests/test_offline_repo_auditor.py:31: TODO\n- docs/",
      "file": ".codex/change_log-large.md",
      "line": 148,
      "match": "\\bTODO\\b"
    },
    {
      "context": "r.py:9: TODO\n- docs/gaps_report.md:27: - tests/test_offline_repo_auditor.py:31: TODO\n- docs/gaps_report.md:28: - tests/test_offline_repo_auditor.py:16: NotImplement",
      "file": ".codex/change_log-large.md",
      "line": 149,
      "match": "\\bTODO\\b"
    },
    {
      "context": "mplementedError\n- docs/gaps_report.md:30: - tests/test_interfaces_compat.py:28: TODO\n- docs/gaps_report.md:31: - tests/test_interfaces_compat.py:42: TODO\n- docs/gap",
      "file": ".codex/change_log-large.md",
      "line": 152,
      "match": "\\bTODO\\b"
    },
    {
      "context": "pat.py:28: TODO\n- docs/gaps_report.md:31: - tests/test_interfaces_compat.py:42: TODO\n- docs/gaps_report.md:32: - tests/test_interfaces_compat.py:52: TODO\n- docs/gap",
      "file": ".codex/change_log-large.md",
      "line": 153,
      "match": "\\bTODO\\b"
    },
    {
      "context": "pat.py:42: TODO\n- docs/gaps_report.md:32: - tests/test_interfaces_compat.py:52: TODO\n- docs/gaps_report.md:33: - tests/test_interfaces_compat.py:106: TODO\n- docs/ga",
      "file": ".codex/change_log-large.md",
      "line": 154,
      "match": "\\bTODO\\b"
    },
    {
      "context": "at.py:52: TODO\n- docs/gaps_report.md:33: - tests/test_interfaces_compat.py:106: TODO\n- docs/gaps_report.md:34: - tests/test_interfaces_compat.py:107: TODO\n- docs/ga",
      "file": ".codex/change_log-large.md",
      "line": 155,
      "match": "\\bTODO\\b"
    },
    {
      "context": "t.py:106: TODO\n- docs/gaps_report.md:34: - tests/test_interfaces_compat.py:107: TODO\n- docs/gaps_report.md:35: - tools/apply_ci_precommit.py:14: TODO\n- docs/gaps_re",
      "file": ".codex/change_log-large.md",
      "line": 156,
      "match": "\\bTODO\\b"
    },
    {
      "context": "compat.py:107: TODO\n- docs/gaps_report.md:35: - tools/apply_ci_precommit.py:14: TODO\n- docs/gaps_report.md:36: - tools/offline_repo_auditor.py:8: TODO\n- docs/gaps_r",
      "file": ".codex/change_log-large.md",
      "line": 157,
      "match": "\\bTODO\\b"
    },
    {
      "context": "commit.py:14: TODO\n- docs/gaps_report.md:36: - tools/offline_repo_auditor.py:8: TODO\n- docs/gaps_report.md:37: - tools/offline_repo_auditor.py:71: TODO\n- docs/gaps_",
      "file": ".codex/change_log-large.md",
      "line": 158,
      "match": "\\bTODO\\b"
    },
    {
      "context": "uditor.py:8: TODO\n- docs/gaps_report.md:37: - tools/offline_repo_auditor.py:71: TODO\n- docs/gaps_report.md:38: - tools/offline_repo_auditor.py:8: NotImplementedErro",
      "file": ".codex/change_log-large.md",
      "line": 159,
      "match": "\\bTODO\\b"
    },
    {
      "context": "ImplementedError\n- docs/gaps_report.md:41: - tools/apply_hydra_scaffold.py:155: TODO\n- docs/gaps_report.md:42: - tools/codex_exec.py:90: TODO\n- docs/gaps_report.md:",
      "file": ".codex/change_log-large.md",
      "line": 163,
      "match": "\\bTODO\\b"
    },
    {
      "context": "hydra_scaffold.py:155: TODO\n- docs/gaps_report.md:42: - tools/codex_exec.py:90: TODO\n- docs/gaps_report.md:43: - tools/codex_exec.py:90: TODO\n- docs/gaps_report.md:",
      "file": ".codex/change_log-large.md",
      "line": 164,
      "match": "\\bTODO\\b"
    },
    {
      "context": "ools/codex_exec.py:90: TODO\n- docs/gaps_report.md:43: - tools/codex_exec.py:90: TODO\n- docs/gaps_report.md:44: - tools/codex_exec.py:92: TODO\n- docs/gaps_report.md:",
      "file": ".codex/change_log-large.md",
      "line": 165,
      "match": "\\bTODO\\b"
    },
    {
      "context": "ools/codex_exec.py:90: TODO\n- docs/gaps_report.md:44: - tools/codex_exec.py:92: TODO\n- docs/gaps_report.md:45: - tools/codex_exec.py:90: NotImplementedError\n- docs/",
      "file": ".codex/change_log-large.md",
      "line": 166,
      "match": "\\bTODO\\b"
    },
    {
      "context": " NotImplementedError\n- docs/gaps_report.md:46: - tools/apply_interfaces.py:214: TODO\n- docs/gaps_report.md:47: - tools/apply_interfaces.py:227: TODO\n- docs/gaps_rep",
      "file": ".codex/change_log-large.md",
      "line": 168,
      "match": "\\bTODO\\b"
    },
    {
      "context": "erfaces.py:214: TODO\n- docs/gaps_report.md:47: - tools/apply_interfaces.py:227: TODO\n- docs/gaps_report.md:48: - tools/apply_interfaces.py:236: TODO\n- docs/gaps_rep",
      "file": ".codex/change_log-large.md",
      "line": 169,
      "match": "\\bTODO\\b"
    },
    {
      "context": "erfaces.py:227: TODO\n- docs/gaps_report.md:48: - tools/apply_interfaces.py:236: TODO\n- docs/gaps_report.md:49: - tools/apply_interfaces.py:287: TODO\n- docs/gaps_rep",
      "file": ".codex/change_log-large.md",
      "line": 170,
      "match": "\\bTODO\\b"
    },
    {
      "context": "erfaces.py:236: TODO\n- docs/gaps_report.md:49: - tools/apply_interfaces.py:287: TODO\n- docs/gaps_report.md:50: - tools/apply_interfaces.py:288: TODO\n- docs/gaps_rep",
      "file": ".codex/change_log-large.md",
      "line": 171,
      "match": "\\bTODO\\b"
    },
    {
      "context": "erfaces.py:287: TODO\n- docs/gaps_report.md:50: - tools/apply_interfaces.py:288: TODO\n- docs/gaps_report.md:51: - tools/apply_interfaces.py:296: TODO\n- docs/gaps_rep",
      "file": ".codex/change_log-large.md",
      "line": 172,
      "match": "\\bTODO\\b"
    },
    {
      "context": "erfaces.py:288: TODO\n- docs/gaps_report.md:51: - tools/apply_interfaces.py:296: TODO\n- docs/gaps_report.md:52: - tools/apply_interfaces.py:95: NotImplementedError\n-",
      "file": ".codex/change_log-large.md",
      "line": 173,
      "match": "\\bTODO\\b"
    },
    {
      "context": "otImplementedError\n- docs/gaps_report.md:63: - tools/apply_stack_polish.py:553: TODO\n- docs/gaps_report.md:64: - tools/codex_patch_session_logging.py:182: NotImplem",
      "file": ".codex/change_log-large.md",
      "line": 185,
      "match": "\\bTODO\\b"
    },
    {
      "context": "plementedError\n- docs/gaps_report.md:66: - tools/codex_workflow_executor.py:52: TODO\n- docs/gaps_report.md:67: - tools/codex_workflow_executor.py:54: TODO\n- docs/ga",
      "file": ".codex/change_log-large.md",
      "line": 188,
      "match": "\\bTODO\\b"
    },
    {
      "context": "or.py:52: TODO\n- docs/gaps_report.md:67: - tools/codex_workflow_executor.py:54: TODO\n- docs/gaps_report.md:68: - tools/codex_workflow_executor.py:54: TODO\n- docs/ga",
      "file": ".codex/change_log-large.md",
      "line": 189,
      "match": "\\bTODO\\b"
    },
    {
      "context": "or.py:54: TODO\n- docs/gaps_report.md:68: - tools/codex_workflow_executor.py:54: TODO\n- docs/gaps_report.md:69: - tools/codex_workflow_executor.py:65: TODO\n- docs/ga",
      "file": ".codex/change_log-large.md",
      "line": 190,
      "match": "\\bTODO\\b"
    },
    {
      "context": "or.py:54: TODO\n- docs/gaps_report.md:69: - tools/codex_workflow_executor.py:65: TODO\n- docs/gaps_report.md:70: - tools/codex_workflow_executor.py:70: TODO\n- docs/ga",
      "file": ".codex/change_log-large.md",
      "line": 191,
      "match": "\\bTODO\\b"
    },
    {
      "context": "or.py:65: TODO\n- docs/gaps_report.md:70: - tools/codex_workflow_executor.py:70: TODO\n- docs/gaps_report.md:71: - src/codex_ml/pipeline.py:33: NotImplementedError\n- ",
      "file": ".codex/change_log-large.md",
      "line": 192,
      "match": "\\bTODO\\b"
    },
    {
      "context": "y TODOs\n- docs/deep_research_prompts.md:3: This document enumerates all current TODO comments in the repository and provides suggested prompts for ChatGPT-5 Deep R\n",
      "file": ".codex/change_log-large.md",
      "line": 208,
      "match": "\\bTODO\\b"
    },
    {
      "context": "s suggested prompts for ChatGPT-5 Deep R\n- docs/deep_research_prompts.md:8: - **TODO:** Implement real step handlers; currently the pipeline simply reports success.",
      "file": ".codex/change_log-large.md",
      "line": 209,
      "match": "\\bTODO\\b"
    },
    {
      "context": "ly the pipeline simply reports success.\n- docs/deep_research_prompts.md:14: - **TODO:** Fill the GPU training example with an end-to-end demo.\n- docs/deep_research_",
      "file": ".codex/change_log-large.md",
      "line": 210,
      "match": "\\bTODO\\b"
    },
    {
      "context": "aining example with an end-to-end demo.\n- docs/deep_research_prompts.md:20: - **TODO:** Supply constructor kwargs where needed when instantiating Tokenizer, RewardM",
      "file": ".codex/change_log-large.md",
      "line": 211,
      "match": "\\bTODO\\b"
    },
    {
      "context": "ntations require constructor arguments.\n- docs/deep_research_prompts.md:34: - **TODO:** Replace placeholder `yourpkg.tokenizers.hf:HFTokenizer` with actual module a",
      "file": ".codex/change_log-large.md",
      "line": 215,
      "match": "\\bTODO\\b"
    },
    {
      "context": "er` with actual module and class names.\n- docs/deep_research_prompts.md:40: - **TODO:** README badges still reference a TODO repository slug.\n- docs/deep_research_p",
      "file": ".codex/change_log-large.md",
      "line": 216,
      "match": "\\bTODO\\b"
    },
    {
      "context": "- docs/deep_research_prompts.md:40: - **TODO:** README badges still reference a TODO repository slug.\n- docs/deep_research_prompts.md:46: - **TODO:** Wire `peft.get",
      "file": ".codex/change_log-large.md",
      "line": 216,
      "match": "\\bTODO\\b"
    },
    {
      "context": "still reference a TODO repository slug.\n- docs/deep_research_prompts.md:46: - **TODO:** Wire `peft.get_peft_model(model, LoraConfig(**cfg))` into `apply_lora`.\n- do",
      "file": ".codex/change_log-large.md",
      "line": 217,
      "match": "\\bTODO\\b"
    },
    {
      "context": " tools/monitoring_integrate.py:363: pass\n- tools/apply_hydra_scaffold.py:153: # TODO: Implement real step handlers; here we simulate success\n- tools/apply_hydra_sca",
      "file": ".codex/change_log-large.md",
      "line": 246,
      "match": "\\bTODO\\b"
    },
    {
      "context": "dex_exec.py:86: pass\n- tools/codex_exec.py:90: out = sh([\"bash\", \"-lc\", 'rg -n \"TODO|NotImplementedError\" || true'], \"Phase 2: scan TODOs\")\n- tools/codex_exec.py:92",
      "file": ".codex/change_log-large.md",
      "line": 256,
      "match": "\\bTODO\\b"
    },
    {
      "context": "e'], \"Phase 2: scan TODOs\")\n- tools/codex_exec.py:92: append_changelog(\"- scan: TODO/NotImplemented present; see ripgrep output in local logs\")\n- tools/apply_patch_",
      "file": ".codex/change_log-large.md",
      "line": 257,
      "match": "\\bTODO\\b"
    },
    {
      "context": "or\n- tools/apply_interfaces.py:329: path: yourpkg.tokenizers.hf:HFTokenizer   # TODO: replace with actual module:class\n- tools/codex_seq_runner.py:135: pattern = r\"",
      "file": ".codex/change_log-large.md",
      "line": 279,
      "match": "\\bTODO\\b"
    },
    {
      "context": "codex_workflow_executor.py:70: log_change(\"README: removed placeholder badges / TODO lines\")\n- tools/codex_workflow_executor.py:111: log_change(\"Local gates passed ",
      "file": ".codex/change_log-large.md",
      "line": 286,
      "match": "\\bTODO\\b"
    },
    {
      "context": "re-commit, pytest)\")\n- tools/apply_ci_precommit.py:14: - README.md (badges with TODO repo slug)\n- tools/codex_apply_modeling_monitoring_api.py:174: pass\n- tools/cod",
      "file": ".codex/change_log-large.md",
      "line": 288,
      "match": "\\bTODO\\b"
    },
    {
      "context": "ell_type\":\"markdown\",\"metadata\":{},\"source\":[\"# GPU Training Example (Stub)\\n\",\"TODO: Fill with end-to-end training d\n- tools/codex_patch_session_logging.py:5: `exc",
      "file": ".codex/change_log-large.md",
      "line": 316,
      "match": "\\bTODO\\b"
    },
    {
      "context": " with logging + skip/fail.\"\n- tools/offline_repo_auditor.py:8: - Detects stubs (TODO/FIXME/TBD, NotImplementedError, pass placeholders)\n- tools/offline_repo_auditor",
      "file": ".codex/change_log-large.md",
      "line": 329,
      "match": "\\bTODO\\b"
    },
    {
      "context": "s/test_engine_hf_trainer.py:191: pass\n- tests/test_offline_repo_auditor.py:9: # TODO: implement training loop\n- tests/test_offline_repo_auditor.py:13: pass\n- tests/",
      "file": ".codex/change_log-large.md",
      "line": 342,
      "match": "\\bTODO\\b"
    },
    {
      "context": "tImplementedError(\"later\")\n- tests/test_offline_repo_auditor.py:31: assert any(\"TODO\" in f.line for f in summary.stubs)\n- tests/test_chat_session_exit.py:20: pass\n-",
      "file": ".codex/change_log-large.md",
      "line": 345,
      "match": "\\bTODO\\b"
    },
    {
      "context": "edError, and pass statements.\"\"\"\n- scripts/run_codex_tasks.py:61: patterns = [r\"TODO\", r\"NotImplementedError\", r\"pass  # TODO\"]\n- scripts/run_codex_tasks.py:100: \"-",
      "file": ".codex/change_log-large.md",
      "line": 552,
      "match": "\\bTODO\\b"
    },
    {
      "context": "s/run_codex_tasks.py:61: patterns = [r\"TODO\", r\"NotImplementedError\", r\"pass  # TODO\"]\n- scripts/run_codex_tasks.py:100: \"- Generated gaps report for TODOs and stub",
      "file": ".codex/change_log-large.md",
      "line": 552,
      "match": "\\bTODO\\b"
    },
    {
      "context": "py:3249: pass\n- .venv/lib/python3.12/site-packages/typing_extensions.py:3319: # TODO: Use inspect.VALUE here, and make the annotations lazily evaluated\n- .venv/lib/",
      "file": ".codex/change_log-large.md",
      "line": 590,
      "match": "\\bTODO\\b"
    },
    {
      "context": "ite-packages/pkg_resources/__init__.py:118: _ResourceStream: TypeAlias = Any  # TODO / Incomplete: A readable file-like object\n- .venv/lib/python3.12/site-packages/",
      "file": ".codex/change_log-large.md",
      "line": 607,
      "match": "\\bTODO\\b"
    },
    {
      "context": "lving so\n- .venv/lib/python3.12/site-packages/pkg_resources/__init__.py:3334: # TODO: remove this except clause when python/cpython#103632 is fixed.\n- .venv/lib/pyt",
      "file": ".codex/change_log-large.md",
      "line": 625,
      "match": "\\bTODO\\b"
    },
    {
      "context": "ndboxing\n- .venv/lib/python3.12/site-packages/pkg_resources/__init__.py:3635: # TODO: Add a deadline?\n- .venv/lib/python3.12/site-packages/pkg_resources/tests/test_",
      "file": ".codex/change_log-large.md",
      "line": 632,
      "match": "\\bTODO\\b"
    },
    {
      "context": "lementedError(msg)\n- .venv/lib/python3.12/site-packages/distlib/version.py:267: TODO: fill this out\n- .venv/lib/python3.12/site-packages/distlib/version.py:482: pas",
      "file": ".codex/change_log-large.md",
      "line": 687,
      "match": "\\bTODO\\b"
    },
    {
      "context": "ion.py:482: pass\n- .venv/lib/python3.12/site-packages/distlib/version.py:516: # TODO: unintended side-effect on, e.g., \"2003.05.09\"\n- .venv/lib/python3.12/site-pack",
      "file": ".codex/change_log-large.md",
      "line": 689,
      "match": "\\bTODO\\b"
    },
    {
      "context": "wheel.py:778: pass\n- .venv/lib/python3.12/site-packages/distlib/wheel.py:844: # TODO version verification\n- .venv/lib/python3.12/site-packages/distlib/wheel.py:891:",
      "file": ".codex/change_log-large.md",
      "line": 693,
      "match": "\\bTODO\\b"
    },
    {
      "context": ".py:746: pass\n- .venv/lib/python3.12/site-packages/distlib/locators.py:760: XXX TODO Note: this cache is never actually cleared. It's assumed that\n- .venv/lib/pytho",
      "file": ".codex/change_log-large.md",
      "line": 708,
      "match": "\\bTODO\\b"
    },
    {
      "context": " this locator')\n- .venv/lib/python3.12/site-packages/distlib/locators.py:922: # TODO SHA256 digest\n- .venv/lib/python3.12/site-packages/distlib/locators.py:1050: ex",
      "file": ".codex/change_log-large.md",
      "line": 710,
      "match": "\\bTODO\\b"
    },
    {
      "context": " Return a Container\n- .venv/lib/python3.12/site-packages/distlib/util.py:401: # TODO check k, v for valid values\n- .venv/lib/python3.12/site-packages/distlib/util.p",
      "file": ".codex/change_log-large.md",
      "line": 728,
      "match": "\\bTODO\\b"
    },
    {
      "context": "index.password)\n- .venv/lib/python3.12/site-packages/distlib/metadata.py:239: # TODO document the mapping API and UNKNOWN default key\n- .venv/lib/python3.12/site-pa",
      "file": ".codex/change_log-large.md",
      "line": 745,
      "match": "\\bTODO\\b"
    },
    {
      "context": "ta.py:407: pass\n- .venv/lib/python3.12/site-packages/distlib/metadata.py:560: # TODO could add iter* variants\n- .venv/lib/python3.12/site-packages/distlib/metadata.",
      "file": ".codex/change_log-large.md",
      "line": 747,
      "match": "\\bTODO\\b"
    },
    {
      "context": "mplementedError\n- .venv/lib/python3.12/site-packages/distlib/metadata.py:984: # TODO: any other fields wanted\n- .venv/lib/python3.12/site-packages/omegaconf/_utils.",
      "file": ".codex/change_log-large.md",
      "line": 751,
      "match": "\\bTODO\\b"
    },
    {
      "context": "es/tqdm/cli.py:24: pass\n- .venv/lib/python3.12/site-packages/tqdm/cli.py:117: # TODO: add custom support for some of the following?\n- .venv/lib/python3.12/site-pack",
      "file": ".codex/change_log-large.md",
      "line": 836,
      "match": "\\bTODO\\b"
    },
    {
      "context": "or some of the following?\n- .venv/lib/python3.12/site-packages/tqdm/cli.py:125: TODO: find out why this is needed.\n- .venv/lib/python3.12/site-packages/tqdm/cli.py:",
      "file": ".codex/change_log-large.md",
      "line": 837,
      "match": "\\bTODO\\b"
    },
    {
      "context": "initers on next iteration\n- .venv/lib/python3.12/site-packages/tqdm/tk.py:31: # TODO: @classmethod: write()?\n- .venv/lib/python3.12/site-packages/tqdm/tk.py:128: pa",
      "file": ".codex/change_log-large.md",
      "line": 843,
      "match": "\\bTODO\\b"
    },
    {
      "context": "es/tqdm/tk.py:128: pass\n- .venv/lib/python3.12/site-packages/tqdm/rich.py:74: # TODO: @classmethod: write()?\n- .venv/lib/python3.12/site-packages/tqdm/rich.py:124: ",
      "file": ".codex/change_log-large.md",
      "line": 845,
      "match": "\\bTODO\\b"
    },
    {
      "context": "s/tqdm/rich.py:124: pass\n- .venv/lib/python3.12/site-packages/tqdm/gui.py:26: # TODO: @classmethod: write() on GUI?\n- .venv/lib/python3.12/site-packages/tqdm/gui.py",
      "file": ".codex/change_log-large.md",
      "line": 847,
      "match": "\\bTODO\\b"
    },
    {
      "context": "/tqdm/std.py:903: pass\n- .venv/lib/python3.12/site-packages/tqdm/std.py:1442: # TODO: private method\n- .venv/lib/python3.12/site-packages/tqdm/__init__.py:3: from .",
      "file": ".codex/change_log-large.md",
      "line": 859,
      "match": "\\bTODO\\b"
    },
    {
      "context": ".venv/lib/python3.12/site-packages/tqdm/__init__.py:3: from .cli import main  # TODO: remove in v5.0.0\n- .venv/lib/python3.12/site-packages/tqdm/__init__.py:4: from",
      "file": ".codex/change_log-large.md",
      "line": 860,
      "match": "\\bTODO\\b"
    },
    {
      "context": "thon3.12/site-packages/tqdm/__init__.py:4: from .gui import tqdm as tqdm_gui  # TODO: remove in v5.0.0\n- .venv/lib/python3.12/site-packages/tqdm/__init__.py:5: from",
      "file": ".codex/change_log-large.md",
      "line": 861,
      "match": "\\bTODO\\b"
    },
    {
      "context": "hon3.12/site-packages/tqdm/__init__.py:5: from .gui import trange as tgrange  # TODO: remove in v5.0.0\n- .venv/lib/python3.12/site-packages/tqdm/notebook.py:34: pas",
      "file": ".codex/change_log-large.md",
      "line": 862,
      "match": "\\bTODO\\b"
    },
    {
      "context": "m/notebook.py:287: pass\n- .venv/lib/python3.12/site-packages/tqdm/utils.py:9: # TODO consider using wcswidth third-party package for 0-width characters\n- .venv/lib/",
      "file": ".codex/change_log-large.md",
      "line": 867,
      "match": "\\bTODO\\b"
    },
    {
      "context": "s_filenames: bool\n- .venv/lib/python3.12/site-packages/pre_commit/hook.py:50: # TODO: have cfgv do this (?)\n- .venv/lib/python3.12/site-packages/pre_commit/store.py",
      "file": ".codex/change_log-large.md",
      "line": 889,
      "match": "\\bTODO\\b"
    },
    {
      "context": "fgv do this (?)\n- .venv/lib/python3.12/site-packages/pre_commit/store.py:234: # TODO: eventually remove this and only create in _create\n- .venv/lib/python3.12/site-",
      "file": ".codex/change_log-large.md",
      "line": 890,
      "match": "\\bTODO\\b"
    },
    {
      "context": "py:73: pass\n- .venv/lib/python3.12/site-packages/pre_commit/repository.py:97: # TODO: remove v1 state writing, no longer needed after pre-commit 3.0\n- .venv/lib/pyt",
      "file": ".codex/change_log-large.md",
      "line": 896,
      "match": "\\bTODO\\b"
    },
    {
      "context": "se it\n- .venv/lib/python3.12/site-packages/pre_commit/languages/golang.py:93: # TODO: 3.9+ .removeprefix('go')\n- .venv/lib/python3.12/site-packages/pre_commit/langu",
      "file": ".codex/change_log-large.md",
      "line": 910,
      "match": "\\bTODO\\b"
    },
    {
      "context": "('go')\n- .venv/lib/python3.12/site-packages/pre_commit/languages/julia.py:71: # TODO: Support language_version with juliaup similar to rust via\n- .venv/lib/python3.",
      "file": ".codex/change_log-large.md",
      "line": 911,
      "match": "\\bTODO\\b"
    },
    {
      "context": "()\n- .venv/lib/python3.12/site-packages/hydra/plugins/completion_plugin.py:3: # TODO: Test with /miniconda3/envs/hydra36/bin/python , seems to be running python for",
      "file": ".codex/change_log-large.md",
      "line": 919,
      "match": "\\bTODO\\b"
    },
    {
      "context": "n.\n- .venv/lib/python3.12/site-packages/hydra/plugins/completion_plugin.py:4: # TODO: Add tests for completion with +prefix (should suggest config groups that are n",
      "file": ".codex/change_log-large.md",
      "line": 920,
      "match": "\\bTODO\\b"
    },
    {
      "context": "d)\n- .venv/lib/python3.12/site-packages/hydra/plugins/completion_plugin.py:5: # TODO: Test completion when defaults has a missing mandatory item\n- .venv/lib/python3",
      "file": ".codex/change_log-large.md",
      "line": 921,
      "match": "\\bTODO\\b"
    },
    {
      "context": "ib/python3.12/site-packages/hydra/_internal/instantiate/_instantiate2.py:197: # TODO: print full key\n- .venv/lib/python3.12/site-packages/hydra/conf/__init__.py:173",
      "file": ".codex/change_log-large.md",
      "line": 939,
      "match": "\\bTODO\\b"
    },
    {
      "context": "int full key\n- .venv/lib/python3.12/site-packages/hydra/conf/__init__.py:173: # TODO: good use case for Union support in OmegaConf\n- .venv/lib/python3.12/site-packa",
      "file": ".codex/change_log-large.md",
      "line": 940,
      "match": "\\bTODO\\b"
    },
    {
      "context": "lizedAtn] = result\n- .venv/lib/python3.12/site-packages/antlr4/Parser.py:466: # TODO: useful in parser?\n- .venv/lib/python3.12/site-packages/antlr4/Lexer.py:27: pas",
      "file": ".codex/change_log-large.md",
      "line": 1031,
      "match": "\\bTODO\\b"
    },
    {
      "context": "4/Lexer.py:27: pass\n- .venv/lib/python3.12/site-packages/antlr4/Lexer.py:328: # TODO: Do we lose character or line position information?\n- .venv/lib/python3.12/site",
      "file": ".codex/change_log-large.md",
      "line": 1033,
      "match": "\\bTODO\\b"
    },
    {
      "context": " mean $\n- .venv/lib/python3.12/site-packages/antlr4/PredictionContext.py:515: # TODO: track whether this is possible above during merge sort for speed\n- .venv/lib/p",
      "file": ".codex/change_log-large.md",
      "line": 1040,
      "match": "\\bTODO\\b"
    },
    {
      "context": "r.py:25: pass\n- .venv/lib/python3.12/site-packages/antlr4/error/Errors.py:93: # TODO symbol = Utils.escapeWhitespace(symbol, false);\n- .venv/lib/python3.12/site-pac",
      "file": ".codex/change_log-large.md",
      "line": 1051,
      "match": "\\bTODO\\b"
    },
    {
      "context": "y:707: pass\n- .venv/lib/python3.12/site-packages/antlr4/atn/Transition.py:69: # TODO: make all transitions sets? no, should remove set edges\n- .venv/lib/python3.12/",
      "file": ".codex/change_log-large.md",
      "line": 1061,
      "match": "\\bTODO\\b"
    },
    {
      "context": "ages/antlr4/atn/ATNConfig.py:58: # accurate depth since I don't ever decrement. TODO: make it a boolean then\n- .venv/lib/python3.12/site-packages/antlr4/atn/ATNConf",
      "file": ".codex/change_log-large.md",
      "line": 1062,
      "match": "\\bTODO\\b"
    },
    {
      "context": "matchState))\n- .venv/lib/python3.12/site-packages/antlr4/atn/ATNState.py:141: # TODO System.err.format(Locale.getDefault(), \"ATN state %d has both epsilon and non-e",
      "file": ".codex/change_log-large.md",
      "line": 1090,
      "match": "\\bTODO\\b"
    },
    {
      "context": "ansitions\n- .venv/lib/python3.12/site-packages/antlr4/atn/ATNConfigSet.py:53: # TODO: these fields make me pretty uncomfortable but nice to pack up info together, s",
      "file": ".codex/change_log-large.md",
      "line": 1093,
      "match": "\\bTODO\\b"
    },
    {
      "context": "mputation\n- .venv/lib/python3.12/site-packages/antlr4/atn/ATNConfigSet.py:54: # TODO: can we track conflicts as they are added to save scanning configs later?\n- .ve",
      "file": ".codex/change_log-large.md",
      "line": 1094,
      "match": "\\bTODO\\b"
    },
    {
      "context": "s\n- .venv/lib/python3.12/site-packages/antlr4/atn/ParserATNSimulator.py:1084: # TODO: If we are doing predicates, there is no point in pursuing\n- .venv/lib/python3.",
      "file": ".codex/change_log-large.md",
      "line": 1100,
      "match": "\\bTODO\\b"
    },
    {
      "context": "es/antlr4/atn/ParserATNSimulator.py:1172: configs.dipsIntoOuterContext = True # TODO: can remove? only care when we add to set per middle of this method\n- .venv/lib",
      "file": ".codex/change_log-large.md",
      "line": 1101,
      "match": "\\bTODO\\b"
    },
    {
      "context": "on:\n- .venv/lib/python3.12/site-packages/antlr4/atn/LexerATNSimulator.py:403: # TODO: if the entry rule is invoked recursively, some\n- .venv/lib/python3.12/site-pac",
      "file": ".codex/change_log-large.md",
      "line": 1109,
      "match": "\\bTODO\\b"
    },
    {
      "context": "py:224: @pass_context\n- .venv/lib/python3.12/site-packages/jinja2/ext.py:251: # TODO: the i18n extension is currently reevaluating values in a few\n- .venv/lib/pytho",
      "file": ".codex/change_log-large.md",
      "line": 1219,
      "match": "\\bTODO\\b"
    },
    {
      "context": "- .venv/lib/python3.12/site-packages/transformers/configuration_utils.py:349: # TODO: remove later, deprecated arguments for TF models\n- .venv/lib/python3.12/site-p",
      "file": ".codex/change_log-large.md",
      "line": 1327,
      "match": "\\bTODO\\b"
    },
    {
      "context": "- .venv/lib/python3.12/site-packages/transformers/configuration_utils.py:460: # TODO (joao): this should be an exception if the user has modified the loaded config.",
      "file": ".codex/change_log-large.md",
      "line": 1329,
      "match": "\\bTODO\\b"
    },
    {
      "context": " arg\n- .venv/lib/python3.12/site-packages/transformers/modeling_layers.py:72: # TODO cyril: this one without `S` can be removed after deprection cycle\n- .venv/lib/p",
      "file": ".codex/change_log-large.md",
      "line": 1338,
      "match": "\\bTODO\\b"
    },
    {
      "context": " or when\n- .venv/lib/python3.12/site-packages/transformers/audio_utils.py:47: # TODO: @eustlb, we actually don't need librosa but soxr is installed with librosa\n- .",
      "file": ".codex/change_log-large.md",
      "line": 1482,
      "match": "\\bTODO\\b"
    },
    {
      "context": "librosa\n- .venv/lib/python3.12/site-packages/transformers/audio_utils.py:165: # TODO: @eustlb, we actually don't need librosa but soxr is installed with librosa\n- .",
      "file": ".codex/change_log-large.md",
      "line": 1483,
      "match": "\\bTODO\\b"
    },
    {
      "context": "ssed to\n- .venv/lib/python3.12/site-packages/transformers/audio_utils.py:610: # TODO This method does not support batching yet as we are mainly focused on inference",
      "file": ".codex/change_log-large.md",
      "line": 1485,
      "match": "\\bTODO\\b"
    },
    {
      "context": "re the DFT.\n- .venv/lib/python3.12/site-packages/transformers/trainer.py:268: # TODO: @AjayP13, @younesbelkada replace this check with version check at the next `ac",
      "file": ".codex/change_log-large.md",
      "line": 1487,
      "match": "\\bTODO\\b"
    },
    {
      "context": " the user.\n- .venv/lib/python3.12/site-packages/transformers/trainer.py:1622: # TODO Change dtypes back to M=FP32, Var = BF16, Kahan = False once they can be cast t",
      "file": ".codex/change_log-large.md",
      "line": 1501,
      "match": "\\bTODO\\b"
    },
    {
      "context": "ass method\n- .venv/lib/python3.12/site-packages/transformers/trainer.py:3001: # TODO: in the future support only specific min PEFT versions\n- .venv/lib/python3.12/s",
      "file": ".codex/change_log-large.md",
      "line": 1508,
      "match": "\\bTODO\\b"
    },
    {
      "context": "T versions\n- .venv/lib/python3.12/site-packages/transformers/trainer.py:3091: # TODO: in the future support only specific min PEFT versions\n- .venv/lib/python3.12/s",
      "file": ".codex/change_log-large.md",
      "line": 1509,
      "match": "\\bTODO\\b"
    },
    {
      "context": "function.\"\n- .venv/lib/python3.12/site-packages/transformers/trainer.py:3919: # TODO: check this only once or always, with speed being the cost\n- .venv/lib/python3.",
      "file": ".codex/change_log-large.md",
      "line": 1513,
      "match": "\\bTODO\\b"
    },
    {
      "context": "g the cost\n- .venv/lib/python3.12/site-packages/transformers/trainer.py:3961: # TODO Matt: This syntax is deprecated and the preferred version is\n- .venv/lib/python",
      "file": ".codex/change_log-large.md",
      "line": 1514,
      "match": "\\bTODO\\b"
    },
    {
      "context": "ot passed,\n- .venv/lib/python3.12/site-packages/transformers/trainer.py:4101: # TODO: this needs to be fixed and made cleaner later.\n- .venv/lib/python3.12/site-pac",
      "file": ".codex/change_log-large.md",
      "line": 1516,
      "match": "\\bTODO\\b"
    },
    {
      "context": "o datasets\n- .venv/lib/python3.12/site-packages/transformers/trainer.py:4896: # TODO: this needs to be fixed and made cleaner later.\n- .venv/lib/python3.12/site-pac",
      "file": ".codex/change_log-large.md",
      "line": 1520,
      "match": "\\bTODO\\b"
    },
    {
      "context": "the\n- .venv/lib/python3.12/site-packages/transformers/trainer_seq2seq.py:330: # TODO: remove this hack when the legacy code that initializes generation_config from ",
      "file": ".codex/change_log-large.md",
      "line": 1536,
      "match": "\\bTODO\\b"
    },
    {
      "context": "e}\")\n- .venv/lib/python3.12/site-packages/transformers/testing_utils.py:1371: # TODO: Remove once eetq releases a fix and this release is used in CI\n- .venv/lib/pyt",
      "file": ".codex/change_log-large.md",
      "line": 1550,
      "match": "\\bTODO\\b"
    },
    {
      "context": "n CI\n- .venv/lib/python3.12/site-packages/transformers/testing_utils.py:1708: # TODO (if possible): Avoid exceptional cases\n- .venv/lib/python3.12/site-packages/tra",
      "file": ".codex/change_log-large.md",
      "line": 1551,
      "match": "\\bTODO\\b"
    },
    {
      "context": "nv/lib/python3.12/site-packages/transformers/modeling_attn_mask_utils.py:275: # TODO: When tracing with TorchDynamo with fullgraph=True, the model is recompiled dep",
      "file": ".codex/change_log-large.md",
      "line": 1571,
      "match": "\\bTODO\\b"
    },
    {
      "context": "nv/lib/python3.12/site-packages/transformers/modeling_attn_mask_utils.py:303: # TODO: maybe revisit this with https://github.com/pytorch/pytorch/pull/114823 in PyTo",
      "file": ".codex/change_log-large.md",
      "line": 1573,
      "match": "\\bTODO\\b"
    },
    {
      "context": "nv/lib/python3.12/site-packages/transformers/modeling_attn_mask_utils.py:381: # TODO: For dynamo, rather use a check on fullgraph=True once this is possible (https:",
      "file": ".codex/change_log-large.md",
      "line": 1577,
      "match": "\\bTODO\\b"
    },
    {
      "context": "\n- .venv/lib/python3.12/site-packages/transformers/modeling_tf_utils.py:1480: # TODO Matt: This is a workaround for older versions of datasets that are missing the ",
      "file": ".codex/change_log-large.md",
      "line": 1597,
      "match": "\\bTODO\\b"
    },
    {
      "context": "\n- .venv/lib/python3.12/site-packages/transformers/modeling_tf_utils.py:2007: # TODO (joao): flagged for replacement (by `_v2_resized_token_embeddings`) due to embe",
      "file": ".codex/change_log-large.md",
      "line": 1608,
      "match": "\\bTODO\\b"
    },
    {
      "context": "\n- .venv/lib/python3.12/site-packages/transformers/modeling_tf_utils.py:2047: # TODO (joao): flagged for detection due to embeddings refactor\n- .venv/lib/python3.12",
      "file": ".codex/change_log-large.md",
      "line": 1609,
      "match": "\\bTODO\\b"
    },
    {
      "context": "\n- .venv/lib/python3.12/site-packages/transformers/modeling_tf_utils.py:2078: # TODO (joao): flagged for replacement (by `_v2_resize_token_embeddings`) due to embed",
      "file": ".codex/change_log-large.md",
      "line": 1610,
      "match": "\\bTODO\\b"
    },
    {
      "context": "\n- .venv/lib/python3.12/site-packages/transformers/modeling_tf_utils.py:2115: # TODO (joao): this one probably needs a v2 version with other models\n- .venv/lib/pyth",
      "file": ".codex/change_log-large.md",
      "line": 1611,
      "match": "\\bTODO\\b"
    },
    {
      "context": "\n- .venv/lib/python3.12/site-packages/transformers/modeling_tf_utils.py:2138: # TODO (joao): flagged for replacement (by `_v2_get_resized_lm_head_bias`) due to embe",
      "file": ".codex/change_log-large.md",
      "line": 1612,
      "match": "\\bTODO\\b"
    },
    {
      "context": "\n- .venv/lib/python3.12/site-packages/transformers/modeling_tf_utils.py:2265: # TODO (joao): flagged for replacement (by `_v2_get_resized_embeddings`) due to embedd",
      "file": ".codex/change_log-large.md",
      "line": 1613,
      "match": "\\bTODO\\b"
    },
    {
      "context": "\n- .venv/lib/python3.12/site-packages/transformers/modeling_tf_utils.py:2932: # TODO Matt: This is a temporary workaround to allow weight renaming, but requires a m",
      "file": ".codex/change_log-large.md",
      "line": 1624,
      "match": "\\bTODO\\b"
    },
    {
      "context": "\n- .venv/lib/python3.12/site-packages/transformers/modeling_tf_utils.py:3309: # TODO (joao): flagged for detection due to embeddings refactor\n- .venv/lib/python3.12",
      "file": ".codex/change_log-large.md",
      "line": 1628,
      "match": "\\bTODO\\b"
    },
    {
      "context": "tems\n- .venv/lib/python3.12/site-packages/transformers/modeling_utils.py:853: # TODO naming is stupid it loads it as well\n- .venv/lib/python3.12/site-packages/trans",
      "file": ".codex/change_log-large.md",
      "line": 1650,
      "match": "\\bTODO\\b"
    },
    {
      "context": "odeling_utils.py:2529: # check `supports_flash_attn_2` for BC with custom code. TODO: remove after a few releases\n- .venv/lib/python3.12/site-packages/transformers/",
      "file": ".codex/change_log-large.md",
      "line": 1667,
      "match": "\\bTODO\\b"
    },
    {
      "context": "odeling_utils.py:2824: # check `supports_flash_attn_2` for BC with custom code. TODO: remove after a few releases\n- .venv/lib/python3.12/site-packages/transformers/",
      "file": ".codex/change_log-large.md",
      "line": 1670,
      "match": "\\bTODO\\b"
    },
    {
      "context": "od.\n- .venv/lib/python3.12/site-packages/transformers/modeling_utils.py:4137: # TODO: fix safe_serialization for tied weights\n- .venv/lib/python3.12/site-packages/t",
      "file": ".codex/change_log-large.md",
      "line": 1680,
      "match": "\\bTODO\\b"
    },
    {
      "context": ".\")\n- .venv/lib/python3.12/site-packages/transformers/modeling_utils.py:4804: # TODO: we can relax this check when we support taking tp_plan from a json file, for e",
      "file": ".codex/change_log-large.md",
      "line": 1691,
      "match": "\\bTODO\\b"
    },
    {
      "context": "le.\n- .venv/lib/python3.12/site-packages/transformers/modeling_utils.py:4902: # TODO Cyril: raise an error instead of the warning in v4.53 (and change the test to c",
      "file": ".codex/change_log-large.md",
      "line": 1692,
      "match": "\\bTODO\\b"
    },
    {
      "context": "ass\n- .venv/lib/python3.12/site-packages/transformers/modeling_utils.py:5424: # TODO: we should only be calling hf_quantizer.skip_placement or something like that\n-",
      "file": ".codex/change_log-large.md",
      "line": 1706,
      "match": "\\bTODO\\b"
    },
    {
      "context": "/transformers/modeling_utils.py:5975: if \"llama4\" in self.config.model_type:  # TODO try to enable for FULL COMPILE HYBRID CACHE SUPPORT\n- .venv/lib/python3.12/site",
      "file": ".codex/change_log-large.md",
      "line": 1708,
      "match": "\\bTODO\\b"
    },
    {
      "context": " .venv/lib/python3.12/site-packages/transformers/image_processing_base.py:46: # TODO: Move BatchFeature to be imported by both image_processing_utils and image_proc",
      "file": ".codex/change_log-large.md",
      "line": 1790,
      "match": "\\bTODO\\b"
    },
    {
      "context": " .venv/lib/python3.12/site-packages/transformers/image_processing_base.py:63: # TODO: (Amy) - factor out the common parts of this and the feature extractor\n- .venv/",
      "file": ".codex/change_log-large.md",
      "line": 1791,
      "match": "\\bTODO\\b"
    },
    {
      "context": "h\n- .venv/lib/python3.12/site-packages/transformers/processing_utils.py:1373: # TODO: @yoni, change logic in v4.52 (when use_fast set to True by default)\n- .venv/li",
      "file": ".codex/change_log-large.md",
      "line": 1818,
      "match": "\\bTODO\\b"
    },
    {
      "context": "of size\n- .venv/lib/python3.12/site-packages/transformers/image_utils.py:953: # TODO raise a warning here instead of simply logging?\n- .venv/lib/python3.12/site-pac",
      "file": ".codex/change_log-large.md",
      "line": 1826,
      "match": "\\bTODO\\b"
    },
    {
      "context": "or is\n- .venv/lib/python3.12/site-packages/transformers/masking_utils.py:798: # TODO: cyril -> probably revisit and remove this, but a lot of tests rely on it\n- .ve",
      "file": ".codex/change_log-large.md",
      "line": 1830,
      "match": "\\bTODO\\b"
    },
    {
      "context": "on it\n- .venv/lib/python3.12/site-packages/transformers/masking_utils.py:895: # TODO: cyril -> probably revisit and remove this, but a lot of tests rely on it\n- .ve",
      "file": ".codex/change_log-large.md",
      "line": 1831,
      "match": "\\bTODO\\b"
    },
    {
      "context": "n it\n- .venv/lib/python3.12/site-packages/transformers/masking_utils.py:1015: # TODO: cyril -> probably revisit and remove this, but a lot of tests rely on it\n- .ve",
      "file": ".codex/change_log-large.md",
      "line": 1832,
      "match": "\\bTODO\\b"
    },
    {
      "context": "value\n- .venv/lib/python3.12/site-packages/transformers/training_args.py:213: # TODO: `TrainingArguments` users rely on it being fully mutable. In the future see if",
      "file": ".codex/change_log-large.md",
      "line": 1858,
      "match": "\\bTODO\\b"
    },
    {
      "context": "args.py:2150: # those deprecated arguments are removed from TrainingArguments. (TODO: v5)\n- .venv/lib/python3.12/site-packages/transformers/training_args.py:2223: \"",
      "file": ".codex/change_log-large.md",
      "line": 1916,
      "match": "\\bTODO\\b"
    },
    {
      "context": "ed.\"\"\"\n- .venv/lib/python3.12/site-packages/transformers/cache_utils.py:1362: # TODO(gante, sanchit-gandhi): move following functionality into `.generate`\n- .venv/l",
      "file": ".codex/change_log-large.md",
      "line": 1953,
      "match": "\\bTODO\\b"
    },
    {
      "context": "erate`\n- .venv/lib/python3.12/site-packages/transformers/cache_utils.py:1511: # TODO (joao, manuel): Remove this class in v4.59.0\n- .venv/lib/python3.12/site-packag",
      "file": ".codex/change_log-large.md",
      "line": 1954,
      "match": "\\bTODO\\b"
    },
    {
      "context": "b`] method.\n- .venv/lib/python3.12/site-packages/transformers/tf_utils.py:70: # TODO: When the issue linked above gets sorted, add a check on TF version here and us",
      "file": ".codex/change_log-large.md",
      "line": 2002,
      "match": "\\bTODO\\b"
    },
    {
      "context": "ss\n- .venv/lib/python3.12/site-packages/transformers/image_transforms.py:780: # TODO (Amy): Accept 1/3/4 channel numpy array as input and return np.array as default",
      "file": ".codex/change_log-large.md",
      "line": 2009,
      "match": "\\bTODO\\b"
    },
    {
      "context": "ils.py:246: # summary-only version for readability - traversing the tree again #TODO optimize?\n- .venv/lib/python3.12/site-packages/transformers/model_debugging_uti",
      "file": ".codex/change_log-large.md",
      "line": 2045,
      "match": "\\bTODO\\b"
    },
    {
      "context": "env/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:865: # TODO clean this up at some point (probably by switching to fast tokenizers)\n- .venv/",
      "file": ".codex/change_log-large.md",
      "line": 2049,
      "match": "\\bTODO\\b"
    },
    {
      "context": "pe_utils.py:71: self.register_buffer(\"inv_freq\", inv_freq, persistent=False)  # TODO joao: may break with compilation\n- .venv/lib/python3.12/site-packages/transform",
      "file": ".codex/change_log-large.md",
      "line": 2154,
      "match": "\\bTODO\\b"
    },
    {
      "context": "- .venv/lib/python3.12/site-packages/transformers/modeling_rope_utils.py:170: # TODO (joao): use the new `original_max_position_embeddings` from rope_scaling\n- .ven",
      "file": ".codex/change_log-large.md",
      "line": 2155,
      "match": "\\bTODO\\b"
    },
    {
      "context": "- .venv/lib/python3.12/site-packages/transformers/modeling_rope_utils.py:301: # TODO (joao): use the new `original_max_position_embeddings` from rope_scaling\n- .ven",
      "file": ".codex/change_log-large.md",
      "line": 2156,
      "match": "\\bTODO\\b"
    },
    {
      "context": "- .venv/lib/python3.12/site-packages/transformers/modeling_rope_utils.py:445: # TODO (joao): update logic for the inclusion of `original_max_position_embeddings`\n- ",
      "file": ".codex/change_log-large.md",
      "line": 2157,
      "match": "\\bTODO\\b"
    },
    {
      "context": "- .venv/lib/python3.12/site-packages/transformers/modeling_rope_utils.py:525: # TODO (joao): update logic for the inclusion of `original_max_position_embeddings`\n- ",
      "file": ".codex/change_log-large.md",
      "line": 2158,
      "match": "\\bTODO\\b"
    },
    {
      "context": "b/python3.12/site-packages/transformers/modeling_flash_attention_utils.py:34: # TODO Deprecate when all models have the attention interface\n- .venv/lib/python3.12/s",
      "file": ".codex/change_log-large.md",
      "line": 2163,
      "match": "\\bTODO\\b"
    },
    {
      "context": "b/python3.12/site-packages/transformers/modeling_flash_attention_utils.py:46: # TODO Deprecate when all models have the attention interface\n- .venv/lib/python3.12/s",
      "file": ".codex/change_log-large.md",
      "line": 2164,
      "match": "\\bTODO\\b"
    },
    {
      "context": "/python3.12/site-packages/transformers/modeling_flash_attention_utils.py:611: # TODO for now this is required to work with\n- .venv/lib/python3.12/site-packages/tran",
      "file": ".codex/change_log-large.md",
      "line": 2172,
      "match": "\\bTODO\\b"
    },
    {
      "context": "/python3.12/site-packages/transformers/modeling_flash_attention_utils.py:642: # TODO for now this is required to work with\n- .venv/lib/python3.12/site-packages/tran",
      "file": ".codex/change_log-large.md",
      "line": 2173,
      "match": "\\bTODO\\b"
    },
    {
      "context": "lib/python3.12/site-packages/transformers/image_processing_utils_fast.py:291: # TODO: remove this once the bug is fixed (detected with torch==2.7.0+git1fee196, torc",
      "file": ".codex/change_log-large.md",
      "line": 2174,
      "match": "\\bTODO\\b"
    },
    {
      "context": "\n- .venv/lib/python3.12/site-packages/transformers/tokenization_utils.py:545: # TODO this is fairly slow to improve!\n- .venv/lib/python3.12/site-packages/transforme",
      "file": ".codex/change_log-large.md",
      "line": 2178,
      "match": "\\bTODO\\b"
    },
    {
      "context": "- .venv/lib/python3.12/site-packages/transformers/tokenization_utils.py:1105: # TODO @ArthurZ in version 5, special tokens should be handled in convert_tokens_to_st",
      "file": ".codex/change_log-large.md",
      "line": 2184,
      "match": "\\bTODO\\b"
    },
    {
      "context": "formers/generation/configuration_utils.py:440: # Deprecated (moved to the Hub). TODO joao, manuel: remove in v4.62.0\n- .venv/lib/python3.12/site-packages/transforme",
      "file": ".codex/change_log-large.md",
      "line": 2192,
      "match": "\\bTODO\\b"
    },
    {
      "context": "/python3.12/site-packages/transformers/generation/configuration_utils.py:491: # TODO joao: find out a way of not depending on external fields (e.g. `assistant_model",
      "file": ".codex/change_log-large.md",
      "line": 2193,
      "match": "\\bTODO\\b"
    },
    {
      "context": "/python3.12/site-packages/transformers/generation/configuration_utils.py:532: # TODO joao, manuel: remove this in v4.62.0\n- .venv/lib/python3.12/site-packages/trans",
      "file": ".codex/change_log-large.md",
      "line": 2194,
      "match": "\\bTODO\\b"
    },
    {
      "context": "ib/python3.12/site-packages/transformers/generation/tf_logits_process.py:370: # TODO (Joao): this function might trigger XLA retracing as `cur_len` increases. Fix i",
      "file": ".codex/change_log-large.md",
      "line": 2225,
      "match": "\\bTODO\\b"
    },
    {
      "context": "ib/python3.12/site-packages/transformers/generation/tf_logits_process.py:427: # TODO (joao): enable XLA on this logits processor. See discussion and attempts in\n- .",
      "file": ".codex/change_log-large.md",
      "line": 2226,
      "match": "\\bTODO\\b"
    },
    {
      "context": "fi\n- .venv/lib/python3.12/site-packages/transformers/generation/utils.py:322: # TODO (joao): remove the equivalent classes and typing shortcuts below in v5\n- .venv/",
      "file": ".codex/change_log-large.md",
      "line": 2238,
      "match": "\\bTODO\\b"
    },
    {
      "context": "transformers/generation/utils.py:680: # 8. Remove unexpected `generate` inputs (TODO @joao: fix trainer and examples)\n- .venv/lib/python3.12/site-packages/transform",
      "file": ".codex/change_log-large.md",
      "line": 2245,
      "match": "\\bTODO\\b"
    },
    {
      "context": "ss\n- .venv/lib/python3.12/site-packages/transformers/generation/utils.py:964: # TODO (joao): remove output/input mismatch when these old models (xlnet, reformer) ar",
      "file": ".codex/change_log-large.md",
      "line": 2256,
      "match": "\\bTODO\\b"
    },
    {
      "context": "\"\n- .venv/lib/python3.12/site-packages/transformers/generation/utils.py:1248: # TODO (joao): find a strategy to specify the order of the processors\n- .venv/lib/pyth",
      "file": ".codex/change_log-large.md",
      "line": 2259,
      "match": "\\bTODO\\b"
    },
    {
      "context": "f\n- .venv/lib/python3.12/site-packages/transformers/generation/utils.py:1562: # TODO: A better way to handle this.\n- .venv/lib/python3.12/site-packages/transformers",
      "file": ".codex/change_log-large.md",
      "line": 2263,
      "match": "\\bTODO\\b"
    },
    {
      "context": "h\n- .venv/lib/python3.12/site-packages/transformers/generation/utils.py:1698: # TODO (joao): per-model generation config classes.\n- .venv/lib/python3.12/site-packag",
      "file": ".codex/change_log-large.md",
      "line": 2265,
      "match": "\\bTODO\\b"
    },
    {
      "context": ")\n- .venv/lib/python3.12/site-packages/transformers/generation/utils.py:1938: # TODO(joao): support static caches in assisted generation. assisted generation needs ",
      "file": ".codex/change_log-large.md",
      "line": 2268,
      "match": "\\bTODO\\b"
    },
    {
      "context": "d\n- .venv/lib/python3.12/site-packages/transformers/generation/utils.py:2331: # TODO (joao): generalize this check with other types of inputs\n- .venv/lib/python3.12",
      "file": ".codex/change_log-large.md",
      "line": 2280,
      "match": "\\bTODO\\b"
    },
    {
      "context": "g\n- .venv/lib/python3.12/site-packages/transformers/generation/utils.py:2495: # TODO joao, manuel: remove this in v4.62.0\n- .venv/lib/python3.12/site-packages/trans",
      "file": ".codex/change_log-large.md",
      "line": 2283,
      "match": "\\bTODO\\b"
    },
    {
      "context": "0\n- .venv/lib/python3.12/site-packages/transformers/generation/utils.py:2515: # TODO joao, manuel: remove this in v4.62.0\n- .venv/lib/python3.12/site-packages/trans",
      "file": ".codex/change_log-large.md",
      "line": 2284,
      "match": "\\bTODO\\b"
    },
    {
      "context": "g\n- .venv/lib/python3.12/site-packages/transformers/generation/utils.py:2911: # TODO (joao): this OP throws \"skipping cudagraphs due to ['incompatible ops']\", find ",
      "file": ".codex/change_log-large.md",
      "line": 2288,
      "match": "\\bTODO\\b"
    },
    {
      "context": "n\n- .venv/lib/python3.12/site-packages/transformers/generation/utils.py:3088: # TODO (joao): This function should take an optional beam scorer function, to manipula",
      "file": ".codex/change_log-large.md",
      "line": 2289,
      "match": "\\bTODO\\b"
    },
    {
      "context": "r\n- .venv/lib/python3.12/site-packages/transformers/generation/utils.py:3256: # TODO (joao): standardize special cases\n- .venv/lib/python3.12/site-packages/transfor",
      "file": ".codex/change_log-large.md",
      "line": 2290,
      "match": "\\bTODO\\b"
    },
    {
      "context": " .venv/lib/python3.12/site-packages/transformers/generation/tf_utils.py:1629: # TODO (Joao): fix cache format or find programmatic way to detect cache index\n- .venv",
      "file": ".codex/change_log-large.md",
      "line": 2375,
      "match": "\\bTODO\\b"
    },
    {
      "context": " .venv/lib/python3.12/site-packages/transformers/generation/tf_utils.py:1913: # TODO (Joao): fix cache format or find programmatic way to detect cache index\n- .venv",
      "file": ".codex/change_log-large.md",
      "line": 2377,
      "match": "\\bTODO\\b"
    },
    {
      "context": " .venv/lib/python3.12/site-packages/transformers/generation/tf_utils.py:2256: # TODO (Joao): fix cache format or find programmatic way to detect cache index\n- .venv",
      "file": ".codex/change_log-large.md",
      "line": 2379,
      "match": "\\bTODO\\b"
    },
    {
      "context": " .venv/lib/python3.12/site-packages/transformers/generation/tf_utils.py:2791: # TODO (Joao): fix cache format or find programmatic way to detect cache index\n- .venv",
      "file": ".codex/change_log-large.md",
      "line": 2380,
      "match": "\\bTODO\\b"
    },
    {
      "context": "nv/lib/python3.12/site-packages/transformers/generation/logits_process.py:29: # TODO (joao): We shouldn't need this, but there would be a circular import\n- .venv/li",
      "file": ".codex/change_log-large.md",
      "line": 2405,
      "match": "\\bTODO\\b"
    },
    {
      "context": "/lib/python3.12/site-packages/transformers/generation/logits_process.py:2016: # TODO(Patrick): Make sure that official models have max_initial_timestamp_index set t",
      "file": ".codex/change_log-large.md",
      "line": 2409,
      "match": "\\bTODO\\b"
    },
    {
      "context": "eration/continuous_batching/continuous_api.py:70: slice_inputs: bool = True,  # TODO: remove this once parity is ensured\n- .venv/lib/python3.12/site-packages/transf",
      "file": ".codex/change_log-large.md",
      "line": 2418,
      "match": "\\bTODO\\b"
    },
    {
      "context": "s_batching/continuous_api.py:604: batch_processor.output_probs.copy_(logits)  # TODO\n- .venv/lib/python3.12/site-packages/transformers/generation/continuous_batchin",
      "file": ".codex/change_log-large.md",
      "line": 2422,
      "match": "\\bTODO\\b"
    },
    {
      "context": ":614: # Pass continuous batching context to logits processor if it supports it. TODO we should find a way to make this a littl\n- .venv/lib/python3.12/site-packages/",
      "file": ".codex/change_log-large.md",
      "line": 2423,
      "match": "\\bTODO\\b"
    },
    {
      "context": "ation/continuous_batching/cache.py:68: # self.num_key_value_heads //= tp_size # TODO: why is this commented out?\n- .venv/lib/python3.12/site-packages/transformers/g",
      "file": ".codex/change_log-large.md",
      "line": 2426,
      "match": "\\bTODO\\b"
    },
    {
      "context": ".12/site-packages/transformers/generation/continuous_batching/classes.py:158: # TODO: this logic seems one token off, check it out\n- .venv/lib/python3.12/site-packa",
      "file": ".codex/change_log-large.md",
      "line": 2427,
      "match": "\\bTODO\\b"
    },
    {
      "context": "/python3.12/site-packages/transformers/loss/loss_for_object_detection.py:197: # TODO use valid to mask invalid areas due to padding in loss\n- .venv/lib/python3.12/s",
      "file": ".codex/change_log-large.md",
      "line": 2430,
      "match": "\\bTODO\\b"
    },
    {
      "context": "nv/lib/python3.12/site-packages/transformers/quantizers/quantizer_hqq.py:202: # TODO: This is a compatibility hack. HQQ-quantized linear layers do not have a `weigh",
      "file": ".codex/change_log-large.md",
      "line": 2453,
      "match": "\\bTODO\\b"
    },
    {
      "context": "b/python3.12/site-packages/transformers/quantizers/quantizer_bnb_8bit.py:287: # TODO: consider bringing replace_with_bnb_linear() code from ..integrations/bitsandby",
      "file": ".codex/change_log-large.md",
      "line": 2462,
      "match": "\\bTODO\\b"
    },
    {
      "context": "nv/lib/python3.12/site-packages/transformers/quantizers/quantizer_eetq.py:62: # TODO: Update message once eetq releases a fix\n- .venv/lib/python3.12/site-packages/t",
      "file": ".codex/change_log-large.md",
      "line": 2467,
      "match": "\\bTODO\\b"
    },
    {
      "context": "ges/transformers/quantizers/quantizer_awq.py:136: model._awq_is_fused = True  # TODO: consider storing this flag in model.config instead\n- .venv/lib/python3.12/site",
      "file": ".codex/change_log-large.md",
      "line": 2469,
      "match": "\\bTODO\\b"
    },
    {
      "context": "b/python3.12/site-packages/transformers/quantizers/quantizer_bnb_4bit.py:324: # TODO: consider bringing replace_with_bnb_linear() code from ..integrations/bitsandby",
      "file": ".codex/change_log-large.md",
      "line": 2476,
      "match": "\\bTODO\\b"
    },
    {
      "context": "ansformers/utils/attention_visualizer.py:199: if \"token_type_ids\" in inputs:  # TODO inspect signature of update causal mask\n- .venv/lib/python3.12/site-packages/tr",
      "file": ".codex/change_log-large.md",
      "line": 2500,
      "match": "\\bTODO\\b"
    },
    {
      "context": "al_context\n- .venv/lib/python3.12/site-packages/transformers/utils/fx.py:201: # TODO: add support for them as it should be quite easy to do so (small blocking issue",
      "file": ".codex/change_log-large.md",
      "line": 2508,
      "match": "\\bTODO\\b"
    },
    {
      "context": "g issues).\n- .venv/lib/python3.12/site-packages/transformers/utils/fx.py:393: # TODO: infer shape without performing the computation, this might be quite hard.\n- .v",
      "file": ".codex/change_log-large.md",
      "line": 2509,
      "match": "\\bTODO\\b"
    },
    {
      "context": "uite hard.\n- .venv/lib/python3.12/site-packages/transformers/utils/fx.py:586: # TODO: infer shape without performing the computation.\n- .venv/lib/python3.12/site-pa",
      "file": ".codex/change_log-large.md",
      "line": 2510,
      "match": "\\bTODO\\b"
    },
    {
      "context": "ith them.\n- .venv/lib/python3.12/site-packages/transformers/utils/fx.py:1340: # TODO: solves GraphModule creation.\n- .venv/lib/python3.12/site-packages/transformers",
      "file": ".codex/change_log-large.md",
      "line": 2519,
      "match": "\\bTODO\\b"
    },
    {
      "context": "/lib/python3.12/site-packages/transformers/utils/quantization_config.py:2039: # TODO: Remove this check once configuration version is handled natively by Quark.\n- .",
      "file": ".codex/change_log-large.md",
      "line": 2540,
      "match": "\\bTODO\\b"
    },
    {
      "context": "ransformers/utils/auto_docstring.py:1407: # elif param_type == \"\" and False:  # TODO: Enforce typing for all parameters\n- .venv/lib/python3.12/site-packages/transfo",
      "file": ".codex/change_log-large.md",
      "line": 2561,
      "match": "\\bTODO\\b"
    },
    {
      "context": ".venv/lib/python3.12/site-packages/transformers/utils/auto_docstring.py:1880: # TODO (Yoni): Add support for Attributes section in docs\n- .venv/lib/python3.12/site-",
      "file": ".codex/change_log-large.md",
      "line": 2562,
      "match": "\\bTODO\\b"
    },
    {
      "context": ".\n- .venv/lib/python3.12/site-packages/transformers/utils/import_utils.py:44: # TODO: This doesn't work for all packages (`bs4`, `faiss`, etc.) Talk to Sylvain to s",
      "file": ".codex/change_log-large.md",
      "line": 2567,
      "match": "\\bTODO\\b"
    },
    {
      "context": ".\n- .venv/lib/python3.12/site-packages/transformers/utils/import_utils.py:51: # TODO: Once python 3.9 support is dropped, `importlib.metadata.packages_distributions",
      "file": ".codex/change_log-large.md",
      "line": 2568,
      "match": "\\bTODO\\b"
    },
    {
      "context": "`\n- .venv/lib/python3.12/site-packages/transformers/utils/import_utils.py:74: # TODO: remove once `importlib.metadata.packages_distributions()` is supported.\n- .ven",
      "file": ".codex/change_log-large.md",
      "line": 2569,
      "match": "\\bTODO\\b"
    },
    {
      "context": "\n- .venv/lib/python3.12/site-packages/transformers/utils/import_utils.py:464: # TODO check if some bugs cause push backs on the exact version\n- .venv/lib/python3.12",
      "file": ".codex/change_log-large.md",
      "line": 2571,
      "match": "\\bTODO\\b"
    },
    {
      "context": "\n- .venv/lib/python3.12/site-packages/transformers/utils/import_utils.py:685: # TODO: more precise exception matching, if possible.\n- .venv/lib/python3.12/site-pack",
      "file": ".codex/change_log-large.md",
      "line": 2572,
      "match": "\\bTODO\\b"
    },
    {
      "context": "\n- .venv/lib/python3.12/site-packages/transformers/utils/import_utils.py:709: # TODO: more precise exception matching, if possible.\n- .venv/lib/python3.12/site-pack",
      "file": ".codex/change_log-large.md",
      "line": 2573,
      "match": "\\bTODO\\b"
    },
    {
      "context": "- .venv/lib/python3.12/site-packages/transformers/utils/import_utils.py:1186: # TODO: Bump the requirement to 2.1.0 once released in https://github.com/ROCmSoftware",
      "file": ".codex/change_log-large.md",
      "line": 2574,
      "match": "\\bTODO\\b"
    },
    {
      "context": "- .venv/lib/python3.12/site-packages/transformers/utils/import_utils.py:1207: # TODO: Check for a minimum version when FA3 is stable\n- .venv/lib/python3.12/site-pac",
      "file": ".codex/change_log-large.md",
      "line": 2575,
      "match": "\\bTODO\\b"
    },
    {
      "context": "2286: pass\n- .venv/lib/python3.12/site-packages/transformers/utils/hub.py:99: # TODO: clean this for v5?\n- .venv/lib/python3.12/site-packages/transformers/utils/hub",
      "file": ".codex/change_log-large.md",
      "line": 2577,
      "match": "\\bTODO\\b"
    },
    {
      "context": ": pass\n- .venv/lib/python3.12/site-packages/transformers/utils/hub.py:914: )  # TODO: This is only used for testing and should be removed once save_jinja_files beco",
      "file": ".codex/change_log-large.md",
      "line": 2586,
      "match": "\\bTODO\\b"
    },
    {
      "context": "/python3.12/site-packages/transformers/models/rt_detr/modeling_rt_detr.py:46: # TODO: Replace all occurrences of the checkpoint with the final one\n- .venv/lib/pytho",
      "file": ".codex/change_log-large.md",
      "line": 2609,
      "match": "\\bTODO\\b"
    },
    {
      "context": "3.12/site-packages/transformers/models/owlvit/image_processing_owlvit.py:464: # TODO: (amy) add support for other frameworks\n- .venv/lib/python3.12/site-packages/tr",
      "file": ".codex/change_log-large.md",
      "line": 2679,
      "match": "\\bTODO\\b"
    },
    {
      "context": "/site-packages/transformers/models/owlvit/image_processing_owlvit_fast.py:71: # TODO: (amy) add support for other frameworks\n- .venv/lib/python3.12/site-packages/tr",
      "file": ".codex/change_log-large.md",
      "line": 2683,
      "match": "\\bTODO\\b"
    },
    {
      "context": ".12/site-packages/transformers/models/gpt_bigcode/modeling_gpt_bigcode.py:53: # TODO: Could have better fused kernels depending on scaling, dropout and head mask.\n-",
      "file": ".codex/change_log-large.md",
      "line": 2688,
      "match": "\\bTODO\\b"
    },
    {
      "context": "e to grayscale format using the NTSC formula. Only support numpy and PIL Image. TODO support torch\n- .venv/lib/python3.12/site-packages/transformers/models/superglu",
      "file": ".codex/change_log-large.md",
      "line": 2699,
      "match": "\\bTODO\\b"
    },
    {
      "context": "e to grayscale format using the NTSC formula. Only support numpy and PIL Image. TODO support torch\n- .venv/lib/python3.12/site-packages/transformers/models/lightglu",
      "file": ".codex/change_log-large.md",
      "line": 2720,
      "match": "\\bTODO\\b"
    },
    {
      "context": "/python3.12/site-packages/transformers/models/t5gemma/modular_t5gemma.py:483: # TODO: support intialization for encoders and decoders separately(?)\n- .venv/lib/pyth",
      "file": ".codex/change_log-large.md",
      "line": 2726,
      "match": "\\bTODO\\b"
    },
    {
      "context": "python3.12/site-packages/transformers/models/t5gemma/modeling_t5gemma.py:602: # TODO: support intialization for encoders and decoders separately(?)\n- .venv/lib/pyth",
      "file": ".codex/change_log-large.md",
      "line": 2732,
      "match": "\\bTODO\\b"
    },
    {
      "context": "b/python3.12/site-packages/transformers/models/funnel/modeling_funnel.py:884: # TODO: deal with head_mask\n- .venv/lib/python3.12/site-packages/transformers/models/f",
      "file": ".codex/change_log-large.md",
      "line": 2769,
      "match": "\\bTODO\\b"
    },
    {
      "context": "b/python3.12/site-packages/transformers/models/funnel/modeling_funnel.py:951: # TODO: deal with head_mask\n- .venv/lib/python3.12/site-packages/transformers/models/f",
      "file": ".codex/change_log-large.md",
      "line": 2770,
      "match": "\\bTODO\\b"
    },
    {
      "context": "hon3.12/site-packages/transformers/models/chameleon/modeling_chameleon.py:69: # TODO(joao): add me back asap :)\n- .venv/lib/python3.12/site-packages/transformers/mo",
      "file": ".codex/change_log-large.md",
      "line": 2772,
      "match": "\\bTODO\\b"
    },
    {
      "context": "ameleon.py:128: self.register_buffer(\"inv_freq\", inv_freq, persistent=False)  # TODO joao: this may break with compilation\n- .venv/lib/python3.12/site-packages/tran",
      "file": ".codex/change_log-large.md",
      "line": 2773,
      "match": "\\bTODO\\b"
    },
    {
      "context": "on3.12/site-packages/transformers/models/chameleon/modeling_chameleon.py:287: # TODO(joao): add me back asap :)\n- .venv/lib/python3.12/site-packages/transformers/mo",
      "file": ".codex/change_log-large.md",
      "line": 2775,
      "match": "\\bTODO\\b"
    },
    {
      "context": "lib/python3.12/site-packages/transformers/models/xlm/tokenization_xlm.py:415: # TODO: make sure we are using `FacebookAI/xlm-mlm-enro-1024`, since XLM-100 doesn't h",
      "file": ".codex/change_log-large.md",
      "line": 2783,
      "match": "\\bTODO\\b"
    },
    {
      "context": "/lib/python3.12/site-packages/transformers/models/led/modeling_tf_led.py:466: # TODO: This code is most likely not very efficient and should be improved\n- .venv/lib",
      "file": ".codex/change_log-large.md",
      "line": 2828,
      "match": "\\bTODO\\b"
    },
    {
      "context": "lib/python3.12/site-packages/transformers/models/led/modeling_tf_led.py:2470: # TODO (Joao): investigate why LED has numerical issues in XLA generate\n- .venv/lib/py",
      "file": ".codex/change_log-large.md",
      "line": 2854,
      "match": "\\bTODO\\b"
    },
    {
      "context": "env/lib/python3.12/site-packages/transformers/models/led/modeling_led.py:265: # TODO: remove the redundant computation\n- .venv/lib/python3.12/site-packages/transfor",
      "file": ".codex/change_log-large.md",
      "line": 2855,
      "match": "\\bTODO\\b"
    },
    {
      "context": "env/lib/python3.12/site-packages/transformers/models/led/modeling_led.py:380: # TODO replace this with\n- .venv/lib/python3.12/site-packages/transformers/models/led/",
      "file": ".codex/change_log-large.md",
      "line": 2856,
      "match": "\\bTODO\\b"
    },
    {
      "context": "blip/modeling_instructblip.py:1146: _keep_in_fp32_modules = [\"query_tokens\"]  # TODO @ArthurZucker I don't know why this is required for FP8\n- .venv/lib/python3.12/",
      "file": ".codex/change_log-large.md",
      "line": 2878,
      "match": "\\bTODO\\b"
    },
    {
      "context": "blip/modeling_instructblip.py:1351: _keep_in_fp32_modules = [\"query_tokens\"]  # TODO @ArthurZucker I don't know why this is required for FP8\n- .venv/lib/python3.12/",
      "file": ".codex/change_log-large.md",
      "line": 2881,
      "match": "\\bTODO\\b"
    },
    {
      "context": "kages/transformers/models/encoder_decoder/modeling_tf_encoder_decoder.py:625: # TODO (joao): the `TFBaseModelOutput` wrapper should not be needed after the generate",
      "file": ".codex/change_log-large.md",
      "line": 2895,
      "match": "\\bTODO\\b"
    },
    {
      "context": "3.12/site-packages/transformers/models/granitemoe/modeling_granitemoe.py:676: # TODO (joao): remove this exception in v4.56 -- it exists for users that try to pass ",
      "file": ".codex/change_log-large.md",
      "line": 2939,
      "match": "\\bTODO\\b"
    },
    {
      "context": "nv/lib/python3.12/site-packages/transformers/models/mt5/modeling_mt5.py:1823: # TODO(thom): Add z_loss https://github.com/tensorflow/mesh/blob/fa19d69eafc9a482aff0b",
      "file": ".codex/change_log-large.md",
      "line": 2948,
      "match": "\\bTODO\\b"
    },
    {
      "context": "on3.12/site-packages/transformers/models/persimmon/modeling_persimmon.py:475: # TODO (joao): remove this exception in v4.56 -- it exists for users that try to pass ",
      "file": ".codex/change_log-large.md",
      "line": 2982,
      "match": "\\bTODO\\b"
    },
    {
      "context": "ython3.12/site-packages/transformers/models/mbart/configuration_mbart.py:189: # TODO: figure this case out.\n- .venv/lib/python3.12/site-packages/transformers/models",
      "file": ".codex/change_log-large.md",
      "line": 3066,
      "match": "\\bTODO\\b"
    },
    {
      "context": "ython3.12/site-packages/transformers/models/mbart/configuration_mbart.py:288: # TODO: test this.\n- .venv/lib/python3.12/site-packages/transformers/models/mbart/toke",
      "file": ".codex/change_log-large.md",
      "line": 3067,
      "match": "\\bTODO\\b"
    },
    {
      "context": "lib/python3.12/site-packages/transformers/models/mbart/modeling_mbart.py:209: # TODO: we need a refactor so that the different attention modules can get their speci",
      "file": ".codex/change_log-large.md",
      "line": 3070,
      "match": "\\bTODO\\b"
    },
    {
      "context": "b/python3.12/site-packages/transformers/models/jetmoe/modeling_jetmoe.py:583: # TODO: Improve this warning with e.g. `model.config.attn_implementation = \"manual\"` o",
      "file": ".codex/change_log-large.md",
      "line": 3083,
      "match": "\\bTODO\\b"
    },
    {
      "context": "b/python3.12/site-packages/transformers/models/jetmoe/modeling_jetmoe.py:656: # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n- .venv/lib/",
      "file": ".codex/change_log-large.md",
      "line": 3084,
      "match": "\\bTODO\\b"
    },
    {
      "context": "b/python3.12/site-packages/transformers/models/jetmoe/modeling_jetmoe.py:712: # TODO: These transpose are quite inefficient but Flash Attention requires the layout ",
      "file": ".codex/change_log-large.md",
      "line": 3086,
      "match": "\\bTODO\\b"
    },
    {
      "context": "b/python3.12/site-packages/transformers/models/jetmoe/modeling_jetmoe.py:924: # TODO (joao): remove this exception in v4.56 -- it exists for users that try to pass ",
      "file": ".codex/change_log-large.md",
      "line": 3087,
      "match": "\\bTODO\\b"
    },
    {
      "context": "thon3.12/site-packages/transformers/models/d_fine/configuration_d_fine.py:30: # TODO: Attribute map assignment logic should be fixed in modular\n- .venv/lib/python3.",
      "file": ".codex/change_log-large.md",
      "line": 3117,
      "match": "\\bTODO\\b"
    },
    {
      "context": "lib/python3.12/site-packages/transformers/models/d_fine/modular_d_fine.py:49: # TODO: Attribute map assignment logic should be fixed in modular\n- .venv/lib/python3.",
      "file": ".codex/change_log-large.md",
      "line": 3127,
      "match": "\\bTODO\\b"
    },
    {
      "context": "ython3.12/site-packages/transformers/models/idefics/modeling_idefics.py:1051: # TODO (joao): remove this exception in v4.56 -- it exists for users that try to pass ",
      "file": ".codex/change_log-large.md",
      "line": 3148,
      "match": "\\bTODO\\b"
    },
    {
      "context": "ython3.12/site-packages/transformers/models/idefics/modeling_idefics.py:1154: # TODO(ls): Add cross attention values to respective lists\n- .venv/lib/python3.12/site",
      "file": ".codex/change_log-large.md",
      "line": 3150,
      "match": "\\bTODO\\b"
    },
    {
      "context": "on3.12/site-packages/transformers/models/idefics/modeling_tf_idefics.py:1416: # TODO(ls): Add cross attention values to respective lists\n- .venv/lib/python3.12/site",
      "file": ".codex/change_log-large.md",
      "line": 3163,
      "match": "\\bTODO\\b"
    },
    {
      "context": "lib/python3.12/site-packages/transformers/models/jamba/modeling_jamba.py:377: # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n- .venv/lib/",
      "file": ".codex/change_log-large.md",
      "line": 3228,
      "match": "\\bTODO\\b"
    },
    {
      "context": "lib/python3.12/site-packages/transformers/models/jamba/modeling_jamba.py:488: # TODO: Improve this warning with e.g. `model.config.attn_implementation = \"manual\"` o",
      "file": ".codex/change_log-large.md",
      "line": 3230,
      "match": "\\bTODO\\b"
    },
    {
      "context": "on3.12/site-packages/transformers/models/kosmos2_5/modeling_kosmos2_5.py:392: # TODO: check with krip\n- .venv/lib/python3.12/site-packages/transformers/models/kosmo",
      "file": ".codex/change_log-large.md",
      "line": 3245,
      "match": "\\bTODO\\b"
    },
    {
      "context": "on3.12/site-packages/transformers/models/kosmos2_5/modeling_kosmos2_5.py:937: # TODO (ydshieh): Remove this (to match Llama's code)\n- .venv/lib/python3.12/site-pack",
      "file": ".codex/change_log-large.md",
      "line": 3246,
      "match": "\\bTODO\\b"
    },
    {
      "context": "n3.12/site-packages/transformers/models/kosmos2_5/modeling_kosmos2_5.py:1007: # TODO (ydshieh): Remove this (to match Llama's code)\n- .venv/lib/python3.12/site-pack",
      "file": ".codex/change_log-large.md",
      "line": 3247,
      "match": "\\bTODO\\b"
    },
    {
      "context": "packages/transformers/models/kosmos2_5/image_processing_kosmos2_5_fast.py:99: # TODO: revert once the issue is fixed: https://huggingface.slack.com/archives/C02TXKQ",
      "file": ".codex/change_log-large.md",
      "line": 3250,
      "match": "\\bTODO\\b"
    },
    {
      "context": "ackages/transformers/models/kosmos2_5/image_processing_kosmos2_5_fast.py:150: # TODO: correct this return type, and the docstring\n- .venv/lib/python3.12/site-packag",
      "file": ".codex/change_log-large.md",
      "line": 3251,
      "match": "\\bTODO\\b"
    },
    {
      "context": "ackages/transformers/models/kosmos2_5/image_processing_kosmos2_5_fast.py:250: # TODO: if it's possible to do in batch mode\n- .venv/lib/python3.12/site-packages/tran",
      "file": ".codex/change_log-large.md",
      "line": 3252,
      "match": "\\bTODO\\b"
    },
    {
      "context": "ackages/transformers/models/kosmos2_5/image_processing_kosmos2_5_fast.py:254: # TODO: we need this to be in batch from\n- .venv/lib/python3.12/site-packages/transfor",
      "file": ".codex/change_log-large.md",
      "line": 3253,
      "match": "\\bTODO\\b"
    },
    {
      "context": "te-packages/transformers/models/bridgetower/configuration_bridgetower.py:279: # TODO: remove this once the Hub files are updated.\n- .venv/lib/python3.12/site-packag",
      "file": ".codex/change_log-large.md",
      "line": 3290,
      "match": "\\bTODO\\b"
    },
    {
      "context": "/lib/python3.12/site-packages/transformers/models/mamba/modeling_mamba.py:93: # TODO (joao): add layer_device_map arg and update code in `generate` accordingly\n- .v",
      "file": ".codex/change_log-large.md",
      "line": 3294,
      "match": "\\bTODO\\b"
    },
    {
      "context": "3.12/site-packages/transformers/models/deepseek_v3/modeling_deepseek_v3.py:281: TODO let's just use the original freqcis computation to not have the view\n- .venv/li",
      "file": ".codex/change_log-large.md",
      "line": 3301,
      "match": "\\bTODO\\b"
    },
    {
      "context": "on3.12/site-packages/transformers/models/deepseek_v3/modular_deepseek_v3.py:44: TODO let's just use the original freqcis computation to not have the view\n- .venv/li",
      "file": ".codex/change_log-large.md",
      "line": 3313,
      "match": "\\bTODO\\b"
    },
    {
      "context": "/models/deepseek_v3/configuration_deepseek_v3.py:135: base_model_tp_plan = {  # TODO: only replicate attention layers when > first_k_dense_replace\n- .venv/lib/pytho",
      "file": ".codex/change_log-large.md",
      "line": 3325,
      "match": "\\bTODO\\b"
    },
    {
      "context": "env/lib/python3.12/site-packages/transformers/models/vit/modeling_vit.py:477: # TODO: maybe have a cleaner way to cast the input (from `ImageProcessor` side?)\n- .ve",
      "file": ".codex/change_log-large.md",
      "line": 3333,
      "match": "\\bTODO\\b"
    },
    {
      "context": "-packages/transformers/models/grounding_dino/modeling_grounding_dino.py:1494: # TODO: valid_ratios could be useless here. check https://github.com/fundamentalvision",
      "file": ".codex/change_log-large.md",
      "line": 3358,
      "match": "\\bTODO\\b"
    },
    {
      "context": "packages/transformers/models/grounding_dino/processing_grounding_dino.py:294: # TODO: @pavel, set labels to None since v4.51.0 or find a way to extract ids\n- .venv/",
      "file": ".codex/change_log-large.md",
      "line": 3366,
      "match": "\\bTODO\\b"
    },
    {
      "context": "python3.12/site-packages/transformers/models/gpt_neo/modeling_gpt_neo.py:274: # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n- .venv/lib/",
      "file": ".codex/change_log-large.md",
      "line": 3379,
      "match": "\\bTODO\\b"
    },
    {
      "context": "rmers/models/gpt_neo/modeling_gpt_neo.py:480: _can_compile_fullgraph = False  # TODO: needs a hybrid cache\n- .venv/lib/python3.12/site-packages/transformers/models/",
      "file": ".codex/change_log-large.md",
      "line": 3381,
      "match": "\\bTODO\\b"
    },
    {
      "context": "python3.12/site-packages/transformers/models/gpt_neo/modeling_gpt_neo.py:574: # TODO (joao): remove this exception in v4.56 -- it exists for users that try to pass ",
      "file": ".codex/change_log-large.md",
      "line": 3383,
      "match": "\\bTODO\\b"
    },
    {
      "context": "ite-packages/transformers/models/oneformer/image_processing_oneformer.py:648: # TODO: (Amy)\n- .venv/lib/python3.12/site-packages/transformers/models/oneformer/image",
      "file": ".codex/change_log-large.md",
      "line": 3437,
      "match": "\\bTODO\\b"
    },
    {
      "context": "ite-packages/transformers/models/mobilevit/image_processing_mobilevit.py:490: # TODO: add support for other frameworks\n- .venv/lib/python3.12/site-packages/transfor",
      "file": ".codex/change_log-large.md",
      "line": 3442,
      "match": "\\bTODO\\b"
    },
    {
      "context": "onal_detr/image_processing_conditional_detr.py:1530: # POSTPROCESSING METHODS - TODO: add support for other frameworks\n- .venv/lib/python3.12/site-packages/transfor",
      "file": ".codex/change_log-large.md",
      "line": 3499,
      "match": "\\bTODO\\b"
    },
    {
      "context": "ckages/transformers/models/conditional_detr/modeling_conditional_detr.py:411: # TODO find a better way of exposing other arguments\n- .venv/lib/python3.12/site-packa",
      "file": ".codex/change_log-large.md",
      "line": 3510,
      "match": "\\bTODO\\b"
    },
    {
      "context": "nv/lib/python3.12/site-packages/transformers/models/fsmt/modeling_fsmt.py:99: # TODO:\n- .venv/lib/python3.12/site-packages/transformers/models/fsmt/modeling_fsmt.py",
      "file": ".codex/change_log-large.md",
      "line": 3599,
      "match": "\\bTODO\\b"
    },
    {
      "context": "/lib/python3.12/site-packages/transformers/models/fsmt/modeling_fsmt.py:1150: # TODO(SS): do we need to ignore pad tokens in labels?\n- .venv/lib/python3.12/site-pac",
      "file": ".codex/change_log-large.md",
      "line": 3604,
      "match": "\\bTODO\\b"
    },
    {
      "context": "python3.12/site-packages/transformers/models/roberta/modeling_roberta.py:292: # TODO: Improve this warning with e.g. `model.config._attn_implementation = \"manual\"` ",
      "file": ".codex/change_log-large.md",
      "line": 3647,
      "match": "\\bTODO\\b"
    },
    {
      "context": "thon3.12/site-packages/transformers/models/nllb_moe/modeling_nllb_moe.py:555: # TODO: we need a refactor so that the different attention modules can get their speci",
      "file": ".codex/change_log-large.md",
      "line": 3663,
      "match": "\\bTODO\\b"
    },
    {
      "context": "thon3.12/site-packages/transformers/models/nllb_moe/modeling_nllb_moe.py:849: # TODO: If anyone is up to it to make sure tests pass etc\n- .venv/lib/python3.12/site-",
      "file": ".codex/change_log-large.md",
      "line": 3664,
      "match": "\\bTODO\\b"
    },
    {
      "context": ".12/site-packages/transformers/models/cohere/tokenization_cohere_fast.py:150: # TODO @ArthurZucker this can only work one way for now, to update later-on. Tests sho",
      "file": ".codex/change_log-large.md",
      "line": 3691,
      "match": "\\bTODO\\b"
    },
    {
      "context": ".12/site-packages/transformers/models/cohere/tokenization_cohere_fast.py:499: # TODO ArthurZ let's rely on the template processor instead, refactor all fast tokeniz",
      "file": ".codex/change_log-large.md",
      "line": 3700,
      "match": "\\bTODO\\b"
    },
    {
      "context": "/site-packages/transformers/models/deberta_v2/tokenization_deberta_v2.py:327: # TODO add a deprecation cycle as this can have different behaviour from our API\n- .ve",
      "file": ".codex/change_log-large.md",
      "line": 3724,
      "match": "\\bTODO\\b"
    },
    {
      "context": "ib/python3.12/site-packages/transformers/models/longt5/modeling_longt5.py:61: # TODO: Update before the merge\n- .venv/lib/python3.12/site-packages/transformers/mode",
      "file": ".codex/change_log-large.md",
      "line": 3736,
      "match": "\\bTODO\\b"
    },
    {
      "context": "ormers/models/longt5/modeling_longt5.py:1258: _can_compile_fullgraph = False  # TODO: @raushan more involved due to local/global attn\n- .venv/lib/python3.12/site-pa",
      "file": ".codex/change_log-large.md",
      "line": 3740,
      "match": "\\bTODO\\b"
    },
    {
      "context": "/python3.12/site-packages/transformers/models/longt5/modeling_longt5.py:2086: # TODO(thom): Add z_loss https://github.com/tensorflow/mesh/blob/fa19d69eafc9a482aff0b",
      "file": ".codex/change_log-large.md",
      "line": 3745,
      "match": "\\bTODO\\b"
    },
    {
      "context": "b/python3.12/site-packages/transformers/models/hubert/modeling_hubert.py:310: # TODO: we need a refactor so that the different attention modules can get their speci",
      "file": ".codex/change_log-large.md",
      "line": 3757,
      "match": "\\bTODO\\b"
    },
    {
      "context": "ython3.12/site-packages/transformers/models/hubert/modeling_tf_hubert.py:431: # TODO Matt: Assigning to attributes in call() is deeply sinful in TensorFlow, as it s",
      "file": ".codex/change_log-large.md",
      "line": 3766,
      "match": "\\bTODO\\b"
    },
    {
      "context": ".12/site-packages/transformers/models/unispeech/modular_unispeech.py:377: >>> # TODO: Add full pretraining example\n- .venv/lib/python3.12/site-packages/transformers",
      "file": ".codex/change_log-large.md",
      "line": 3776,
      "match": "\\bTODO\\b"
    },
    {
      "context": "hon3.12/site-packages/transformers/models/unispeech/modular_unispeech.py:414: # TODO(PVP) - add negative sampling & loss computation\n- .venv/lib/python3.12/site-pac",
      "file": ".codex/change_log-large.md",
      "line": 3777,
      "match": "\\bTODO\\b"
    },
    {
      "context": "on3.12/site-packages/transformers/models/unispeech/modeling_unispeech.py:342: # TODO: we need a refactor so that the different attention modules can get their speci",
      "file": ".codex/change_log-large.md",
      "line": 3780,
      "match": "\\bTODO\\b"
    },
    {
      "context": "2/site-packages/transformers/models/unispeech/modeling_unispeech.py:1179: >>> # TODO: Add full pretraining example\n- .venv/lib/python3.12/site-packages/transformers",
      "file": ".codex/change_log-large.md",
      "line": 3781,
      "match": "\\bTODO\\b"
    },
    {
      "context": "n3.12/site-packages/transformers/models/unispeech/modeling_unispeech.py:1216: # TODO(PVP) - add negative sampling & loss computation\n- .venv/lib/python3.12/site-pac",
      "file": ".codex/change_log-large.md",
      "line": 3782,
      "match": "\\bTODO\\b"
    },
    {
      "context": "rmers/models/time_series_transformer/modeling_time_series_transformer.py:365: # TODO: we need a refactor so that the different attention modules can get their speci",
      "file": ".codex/change_log-large.md",
      "line": 3797,
      "match": "\\bTODO\\b"
    },
    {
      "context": "rmers/models/time_series_transformer/modeling_time_series_transformer.py:636: # TODO: tests would need a rewrite to check for correct implementation\n- .venv/lib/pyt",
      "file": ".codex/change_log-large.md",
      "line": 3798,
      "match": "\\bTODO\\b"
    },
    {
      "context": "ransformers/models/phi4_multimodal/feature_extraction_phi4_multimodal.py:205: # TODO; @eustlb, move this to audio_utils in a general spectogram_batch function that ",
      "file": ".codex/change_log-large.md",
      "line": 3884,
      "match": "\\bTODO\\b"
    },
    {
      "context": "thon3.12/site-packages/transformers/models/stablelm/modeling_stablelm.py:325: # TODO: Improve this warning with e.g. `model.config.attn_implementation = \"manual\"` o",
      "file": ".codex/change_log-large.md",
      "line": 3898,
      "match": "\\bTODO\\b"
    },
    {
      "context": "thon3.12/site-packages/transformers/models/stablelm/modeling_stablelm.py:430: # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n- .venv/lib/",
      "file": ".codex/change_log-large.md",
      "line": 3904,
      "match": "\\bTODO\\b"
    },
    {
      "context": "thon3.12/site-packages/transformers/models/stablelm/modeling_stablelm.py:495: # TODO: These transpose are quite inefficient but Flash Attention requires the layout ",
      "file": ".codex/change_log-large.md",
      "line": 3909,
      "match": "\\bTODO\\b"
    },
    {
      "context": "thon3.12/site-packages/transformers/models/stablelm/modeling_stablelm.py:703: # TODO (joao): remove this exception in v4.56 -- it exists for users that try to pass ",
      "file": ".codex/change_log-large.md",
      "line": 3910,
      "match": "\\bTODO\\b"
    },
    {
      "context": "env/lib/python3.12/site-packages/transformers/models/git/modeling_git.py:408: # TODO (joao): remove this exception in v4.56 -- it exists for users that try to pass ",
      "file": ".codex/change_log-large.md",
      "line": 3928,
      "match": "\\bTODO\\b"
    },
    {
      "context": "thon3.12/site-packages/transformers/models/big_bird/modeling_big_bird.py:826: # TODO(PVP): need to verify if below code is correct\n- .venv/lib/python3.12/site-packa",
      "file": ".codex/change_log-large.md",
      "line": 3957,
      "match": "\\bTODO\\b"
    },
    {
      "context": "b/python3.12/site-packages/transformers/models/gemma3/modeling_gemma3.py:466: # TODO: raushan fix this after RoPE refactor. For now we hack it by reassigning thetas",
      "file": ".codex/change_log-large.md",
      "line": 3986,
      "match": "\\bTODO\\b"
    },
    {
      "context": "ib/python3.12/site-packages/transformers/models/gemma3/modular_gemma3.py:549: # TODO: raushan fix this after RoPE refactor. For now we hack it by reassigning thetas",
      "file": ".codex/change_log-large.md",
      "line": 3993,
      "match": "\\bTODO\\b"
    },
    {
      "context": "lib/python3.12/site-packages/transformers/models/olmoe/modeling_olmoe.py:370: # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n- .venv/lib/",
      "file": ".codex/change_log-large.md",
      "line": 4035,
      "match": "\\bTODO\\b"
    },
    {
      "context": "lib/python3.12/site-packages/transformers/models/olmoe/modeling_olmoe.py:415: # TODO: These transpose are quite inefficient but Flash Attention requires the layout ",
      "file": ".codex/change_log-large.md",
      "line": 4036,
      "match": "\\bTODO\\b"
    },
    {
      "context": "lib/python3.12/site-packages/transformers/models/olmoe/modeling_olmoe.py:495: # TODO: Improve this warning with e.g. `model.config.attn_implementation = \"manual\"` o",
      "file": ".codex/change_log-large.md",
      "line": 4038,
      "match": "\\bTODO\\b"
    },
    {
      "context": "lib/python3.12/site-packages/transformers/models/olmoe/modeling_olmoe.py:791: # TODO (joao): remove this exception in v4.56 -- it exists for users that try to pass ",
      "file": ".codex/change_log-large.md",
      "line": 4039,
      "match": "\\bTODO\\b"
    },
    {
      "context": "packages/transformers/models/deformable_detr/modeling_deformable_detr.py:474: # TODO find a better way of exposing other arguments\n- .venv/lib/python3.12/site-packa",
      "file": ".codex/change_log-large.md",
      "line": 4043,
      "match": "\\bTODO\\b"
    },
    {
      "context": "ackages/transformers/models/deformable_detr/modeling_deformable_detr.py:1021: # TODO: valid_ratios could be useless here. check https://github.com/fundamentalvision",
      "file": ".codex/change_log-large.md",
      "line": 4044,
      "match": "\\bTODO\\b"
    },
    {
      "context": "mable_detr/image_processing_deformable_detr.py:1528: # POSTPROCESSING METHODS - TODO: add support for other frameworks\n- .venv/lib/python3.12/site-packages/transfor",
      "file": ".codex/change_log-large.md",
      "line": 4056,
      "match": "\\bTODO\\b"
    },
    {
      "context": "ckages/transformers/models/granitemoeshared/modeling_granitemoeshared.py:652: # TODO (joao): remove this exception in v4.56 -- it exists for users that try to pass ",
      "file": ".codex/change_log-large.md",
      "line": 4090,
      "match": "\\bTODO\\b"
    },
    {
      "context": "ite-packages/transformers/models/unispeech_sat/modeling_unispeech_sat.py:347: # TODO: we need a refactor so that the different attention modules can get their speci",
      "file": ".codex/change_log-large.md",
      "line": 4092,
      "match": "\\bTODO\\b"
    },
    {
      "context": "ackages/transformers/models/unispeech_sat/modeling_unispeech_sat.py:1191: >>> # TODO: Add full pretraining example\n- .venv/lib/python3.12/site-packages/transformers",
      "file": ".codex/change_log-large.md",
      "line": 4093,
      "match": "\\bTODO\\b"
    },
    {
      "context": "te-packages/transformers/models/unispeech_sat/modeling_unispeech_sat.py:1208: # TODO(PVP) - add pretraining logic and add to tests\n- .venv/lib/python3.12/site-packa",
      "file": ".codex/change_log-large.md",
      "line": 4094,
      "match": "\\bTODO\\b"
    },
    {
      "context": "-packages/transformers/models/unispeech_sat/modular_unispeech_sat.py:396: >>> # TODO: Add full pretraining example\n- .venv/lib/python3.12/site-packages/transformers",
      "file": ".codex/change_log-large.md",
      "line": 4104,
      "match": "\\bTODO\\b"
    },
    {
      "context": "site-packages/transformers/models/unispeech_sat/modular_unispeech_sat.py:413: # TODO(PVP) - add pretraining logic and add to tests\n- .venv/lib/python3.12/site-packa",
      "file": ".codex/change_log-large.md",
      "line": 4105,
      "match": "\\bTODO\\b"
    },
    {
      "context": "hon3.12/site-packages/transformers/models/pegasus/tokenization_pegasus.py:34: # TODO ArthurZ refactor this to only use the added_tokens_encoder\n- .venv/lib/python3.",
      "file": ".codex/change_log-large.md",
      "line": 4120,
      "match": "\\bTODO\\b"
    },
    {
      "context": "python3.12/site-packages/transformers/models/pegasus/modeling_pegasus.py:199: # TODO: we need a refactor so that the different attention modules can get their speci",
      "file": ".codex/change_log-large.md",
      "line": 4130,
      "match": "\\bTODO\\b"
    },
    {
      "context": "/rag/modeling_rag.py:1609: # because RAG expands input for doc-size internally. TODO: raushan, remove me when all models support\n- .venv/lib/python3.12/site-package",
      "file": ".codex/change_log-large.md",
      "line": 4186,
      "match": "\\bTODO\\b"
    },
    {
      "context": "nv/lib/python3.12/site-packages/transformers/models/rag/retrieval_rag.py:510: # TODO(Patrick): if we train more RAG models, I want to put the input first to take ad",
      "file": ".codex/change_log-large.md",
      "line": 4258,
      "match": "\\bTODO\\b"
    },
    {
      "context": "nv/lib/python3.12/site-packages/transformers/models/rag/retrieval_rag.py:511: # TODO(piktus): better handling of truncation\n- .venv/lib/python3.12/site-packages/tra",
      "file": ".codex/change_log-large.md",
      "line": 4259,
      "match": "\\bTODO\\b"
    },
    {
      "context": "thon3.12/site-packages/transformers/models/patchtst/modeling_patchtst.py:110: # TODO: we need a refactor so that the different attention modules can get their speci",
      "file": ".codex/change_log-large.md",
      "line": 4260,
      "match": "\\bTODO\\b"
    },
    {
      "context": "b/python3.12/site-packages/transformers/models/biogpt/modeling_biogpt.py:178: # TODO: we need a refactor so that the different attention modules can get their speci",
      "file": ".codex/change_log-large.md",
      "line": 4266,
      "match": "\\bTODO\\b"
    },
    {
      "context": "_moe/configuration_hunyuan_v1_moe.py:162: # self._rope_scaling_validation()   # TODO: Need validation?\n- .venv/lib/python3.12/site-packages/transformers/models/huny",
      "file": ".codex/change_log-large.md",
      "line": 4275,
      "match": "\\bTODO\\b"
    },
    {
      "context": "ite-packages/transformers/models/segformer/image_processing_segformer.py:443: # TODO: add support for other frameworks\n- .venv/lib/python3.12/site-packages/transfor",
      "file": ".codex/change_log-large.md",
      "line": 4314,
      "match": "\\bTODO\\b"
    },
    {
      "context": "ackages/transformers/models/segformer/image_processing_segformer_fast.py:220: # TODO: add support for other frameworks\n- .venv/lib/python3.12/site-packages/transfor",
      "file": ".codex/change_log-large.md",
      "line": 4318,
      "match": "\\bTODO\\b"
    },
    {
      "context": "packages/transformers/models/musicgen_melody/modeling_musicgen_melody.py:241: # TODO: we need a refactor so that the different attention modules can get their speci",
      "file": ".codex/change_log-large.md",
      "line": 4331,
      "match": "\\bTODO\\b"
    },
    {
      "context": "ckages/transformers/models/mobilenet_v2/image_processing_mobilenet_v2.py:495: # TODO: add support for other frameworks\n- .venv/lib/python3.12/site-packages/transfor",
      "file": ".codex/change_log-large.md",
      "line": 4460,
      "match": "\\bTODO\\b"
    },
    {
      "context": "s/transformers/models/mobilenet_v2/image_processing_mobilenet_v2_fast.py:227: # TODO: add support for other frameworks\n- .venv/lib/python3.12/site-packages/transfor",
      "file": ".codex/change_log-large.md",
      "line": 4462,
      "match": "\\bTODO\\b"
    },
    {
      "context": "mers/models/vision_encoder_decoder/modeling_tf_vision_encoder_decoder.py:665: # TODO (joao): the `TFBaseModelOutput` wrapper should not be needed after the generate",
      "file": ".codex/change_log-large.md",
      "line": 4487,
      "match": "\\bTODO\\b"
    },
    {
      "context": "thon3.12/site-packages/transformers/models/nemotron/modeling_nemotron.py:286: # TODO cyril: modular\n- .venv/lib/python3.12/site-packages/transformers/models/nemotro",
      "file": ".codex/change_log-large.md",
      "line": 4513,
      "match": "\\bTODO\\b"
    },
    {
      "context": "thon3.12/site-packages/transformers/models/nemotron/modeling_nemotron.py:297: # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n- .venv/lib/",
      "file": ".codex/change_log-large.md",
      "line": 4515,
      "match": "\\bTODO\\b"
    },
    {
      "context": "thon3.12/site-packages/transformers/models/nemotron/modeling_nemotron.py:344: # TODO: These transpose are quite inefficient but Flash Attention requires the layout ",
      "file": ".codex/change_log-large.md",
      "line": 4516,
      "match": "\\bTODO\\b"
    },
    {
      "context": "thon3.12/site-packages/transformers/models/nemotron/modeling_nemotron.py:407: # TODO cyril: modular\n- .venv/lib/python3.12/site-packages/transformers/models/nemotro",
      "file": ".codex/change_log-large.md",
      "line": 4517,
      "match": "\\bTODO\\b"
    },
    {
      "context": "thon3.12/site-packages/transformers/models/nemotron/modeling_nemotron.py:429: # TODO: Improve this warning with e.g. `model.config.attn_implementation = \"manual\"` o",
      "file": ".codex/change_log-large.md",
      "line": 4519,
      "match": "\\bTODO\\b"
    },
    {
      "context": "thon3.12/site-packages/transformers/models/nemotron/modeling_nemotron.py:858: # TODO: re-enable check: Copied from transformers.models.llama.modeling_llama.LlamaFor",
      "file": ".codex/change_log-large.md",
      "line": 4520,
      "match": "\\bTODO\\b"
    },
    {
      "context": "thon3.12/site-packages/transformers/models/informer/modeling_informer.py:447: # TODO: we need a refactor so that the different attention modules can get their speci",
      "file": ".codex/change_log-large.md",
      "line": 4535,
      "match": "\\bTODO\\b"
    },
    {
      "context": "3.12/site-packages/transformers/models/hgnet_v2/configuration_hgnet_v2.py:27: # TODO: Modular conversion for resnet must be fixed as\n- .venv/lib/python3.12/site-pac",
      "file": ".codex/change_log-large.md",
      "line": 4541,
      "match": "\\bTODO\\b"
    },
    {
      "context": "python3.12/site-packages/transformers/models/hgnet_v2/modular_hgnet_v2.py:38: # TODO: Modular conversion for resnet must be fixed as\n- .venv/lib/python3.12/site-pac",
      "file": ".codex/change_log-large.md",
      "line": 4542,
      "match": "\\bTODO\\b"
    },
    {
      "context": "/site-packages/transformers/models/qwen2_5_omni/modular_qwen2_5_omni.py:4083: # TODO: raushan, defaults should be saved in generation config\n- .venv/lib/python3.12/",
      "file": ".codex/change_log-large.md",
      "line": 4551,
      "match": "\\bTODO\\b"
    },
    {
      "context": "site-packages/transformers/models/qwen2_5_omni/modeling_qwen2_5_omni.py:3782: # TODO: raushan, defaults should be saved in generation config\n- .venv/lib/python3.12/",
      "file": ".codex/change_log-large.md",
      "line": 4568,
      "match": "\\bTODO\\b"
    },
    {
      "context": "python3.12/site-packages/transformers/models/mluke/tokenization_mluke.py:346: # TODO check if the t5/llama PR also applies here\n- .venv/lib/python3.12/site-packages",
      "file": ".codex/change_log-large.md",
      "line": 4573,
      "match": "\\bTODO\\b"
    },
    {
      "context": "v/lib/python3.12/site-packages/transformers/models/rwkv/modeling_rwkv.py:270: # TODO: maybe jit, otherwise move inside forward\n- .venv/lib/python3.12/site-packages/",
      "file": ".codex/change_log-large.md",
      "line": 4617,
      "match": "\\bTODO\\b"
    },
    {
      "context": "ges/transformers/models/mm_grounding_dino/modeling_mm_grounding_dino.py:1160: # TODO: valid_ratios could be useless here. check https://github.com/fundamentalvision",
      "file": ".codex/change_log-large.md",
      "line": 4663,
      "match": "\\bTODO\\b"
    },
    {
      "context": "python3.12/site-packages/transformers/models/whisper/modeling_whisper.py:298: # TODO: we need a refactor so that the different attention modules can get their speci",
      "file": ".codex/change_log-large.md",
      "line": 4735,
      "match": "\\bTODO\\b"
    },
    {
      "context": "n3.12/site-packages/transformers/models/whisper/tokenization_whisper.py:1013: # TODO Handle when language is different from the previous\n- .venv/lib/python3.12/site",
      "file": ".codex/change_log-large.md",
      "line": 4746,
      "match": "\\bTODO\\b"
    },
    {
      "context": "on3.12/site-packages/transformers/models/whisper/modeling_tf_whisper.py:1665: # TODO: Implement `WhisperTimeStampLogitsProcessor`.\n- .venv/lib/python3.12/site-packa",
      "file": ".codex/change_log-large.md",
      "line": 4789,
      "match": "\\bTODO\\b"
    },
    {
      "context": "ite-packages/transformers/models/shieldgemma2/processing_shieldgemma2.py:153: # TODO(ryanmullins): Support images from PIL or URLs.\n- .venv/lib/python3.12/site-pack",
      "file": ".codex/change_log-large.md",
      "line": 4795,
      "match": "\\bTODO\\b"
    },
    {
      "context": "python3.12/site-packages/transformers/models/m2m_100/modeling_m2m_100.py:265: # TODO: we need a refactor so that the different attention modules can get their speci",
      "file": ".codex/change_log-large.md",
      "line": 4797,
      "match": "\\bTODO\\b"
    },
    {
      "context": "n3.12/site-packages/transformers/models/m2m_100/configuration_m2m_100.py:275: # TODO: test this.\n- .venv/lib/python3.12/site-packages/transformers/models/m2m_100/to",
      "file": ".codex/change_log-large.md",
      "line": 4804,
      "match": "\\bTODO\\b"
    },
    {
      "context": "n3.12/site-packages/transformers/models/llama/tokenization_llama_fast.py:237: # TODO ArthurZ let's rely on the template processor instead, refactor all fast tokeniz",
      "file": ".codex/change_log-large.md",
      "line": 4817,
      "match": "\\bTODO\\b"
    },
    {
      "context": "ython3.12/site-packages/transformers/models/gpt_neox/modular_gpt_neox.py:309: # TODO (joao): remove this exception in v4.56 -- it exists for users that try to pass ",
      "file": ".codex/change_log-large.md",
      "line": 4823,
      "match": "\\bTODO\\b"
    },
    {
      "context": "thon3.12/site-packages/transformers/models/gpt_neox/modeling_gpt_neox.py:431: # TODO (joao): remove this exception in v4.56 -- it exists for users that try to pass ",
      "file": ".codex/change_log-large.md",
      "line": 4831,
      "match": "\\bTODO\\b"
    },
    {
      "context": "b/python3.12/site-packages/transformers/models/mllama/processing_mllama.py:261: TODO: add aspect_ratio_ids and aspect_ratio_mask and cross_attention_mask\n- .venv/li",
      "file": ".codex/change_log-large.md",
      "line": 4838,
      "match": "\\bTODO\\b"
    },
    {
      "context": ".12/site-packages/transformers/models/qwen2_vl/configuration_qwen2_vl.py:250: # TODO: @raushan update config in the hub\n- .venv/lib/python3.12/site-packages/transfo",
      "file": ".codex/change_log-large.md",
      "line": 4896,
      "match": "\\bTODO\\b"
    },
    {
      "context": "v/lib/python3.12/site-packages/transformers/models/bark/modeling_bark.py:191: # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n- .venv/lib/",
      "file": ".codex/change_log-large.md",
      "line": 4909,
      "match": "\\bTODO\\b"
    },
    {
      "context": "/lib/python3.12/site-packages/transformers/models/bark/modeling_bark.py:1515: # TODO (joao):workaround until nested generation config is compatible with PreTrained ",
      "file": ".codex/change_log-large.md",
      "line": 4922,
      "match": "\\bTODO\\b"
    },
    {
      "context": "/site-packages/transformers/models/bark/generation_configuration_bark.py:244: # TODO (joao): nested from_dict\n- .venv/lib/python3.12/site-packages/transformers/mode",
      "file": ".codex/change_log-large.md",
      "line": 4930,
      "match": "\\bTODO\\b"
    },
    {
      "context": "thon3.12/site-packages/transformers/models/auto/image_processing_auto.py:457: # TODO: @yoni, change in v4.48 (use_fast set to True by default)\n- .venv/lib/python3.1",
      "file": ".codex/change_log-large.md",
      "line": 5029,
      "match": "\\bTODO\\b"
    },
    {
      "context": "thon3.12/site-packages/transformers/models/auto/image_processing_auto.py:522: # TODO: @yoni, change logic in v4.52 (when use_fast set to True by default)\n- .venv/li",
      "file": ".codex/change_log-large.md",
      "line": 5030,
      "match": "\\bTODO\\b"
    },
    {
      "context": "b/python3.12/site-packages/transformers/models/phimoe/modeling_phimoe.py:450: # TODO: Improve this warning with e.g. `model.config.attn_implementation = \"manual\"` o",
      "file": ".codex/change_log-large.md",
      "line": 5056,
      "match": "\\bTODO\\b"
    },
    {
      "context": "b/python3.12/site-packages/transformers/models/phimoe/modeling_phimoe.py:977: # TODO (joao): remove this exception in v4.56 -- it exists for users that try to pass ",
      "file": ".codex/change_log-large.md",
      "line": 5061,
      "match": "\\bTODO\\b"
    },
    {
      "context": "ges/transformers/models/bigbird_pegasus/configuration_bigbird_pegasus.py:211: # TODO: figure this case out.\n- .venv/lib/python3.12/site-packages/transformers/models",
      "file": ".codex/change_log-large.md",
      "line": 5064,
      "match": "\\bTODO\\b"
    },
    {
      "context": "ges/transformers/models/bigbird_pegasus/configuration_bigbird_pegasus.py:310: # TODO: test this.\n- .venv/lib/python3.12/site-packages/transformers/models/bigbird_pe",
      "file": ".codex/change_log-large.md",
      "line": 5065,
      "match": "\\bTODO\\b"
    },
    {
      "context": "packages/transformers/models/bigbird_pegasus/modeling_bigbird_pegasus.py:639: # TODO(PVP): need to verify if below code is correct\n- .venv/lib/python3.12/site-packa",
      "file": ".codex/change_log-large.md",
      "line": 5067,
      "match": "\\bTODO\\b"
    },
    {
      "context": "ackages/transformers/models/bigbird_pegasus/modeling_bigbird_pegasus.py:1259: # TODO: we need a refactor so that the different attention modules can get their speci",
      "file": ".codex/change_log-large.md",
      "line": 5069,
      "match": "\\bTODO\\b"
    },
    {
      "context": "e to grayscale format using the NTSC formula. Only support numpy and PIL Image. TODO support torch\n- .venv/lib/python3.12/site-packages/transformers/models/superpoi",
      "file": ".codex/change_log-large.md",
      "line": 5093,
      "match": "\\bTODO\\b"
    },
    {
      "context": "2/site-packages/transformers/models/owlv2/image_processing_owlv2_fast.py:107: # TODO: (amy) add support for other frameworks\n- .venv/lib/python3.12/site-packages/tr",
      "file": ".codex/change_log-large.md",
      "line": 5104,
      "match": "\\bTODO\\b"
    },
    {
      "context": "lib/python3.12/site-packages/transformers/models/esm/configuration_esm.py:26: # TODO Update this\n- .venv/lib/python3.12/site-packages/transformers/models/esm/config",
      "file": ".codex/change_log-large.md",
      "line": 5143,
      "match": "\\bTODO\\b"
    },
    {
      "context": "ib/python3.12/site-packages/transformers/models/esm/modeling_esmfold.py:1979: # TODO Add information to the docstring about any methods that convert to PDB format, ",
      "file": ".codex/change_log-large.md",
      "line": 5148,
      "match": "\\bTODO\\b"
    },
    {
      "context": "/lib/python3.12/site-packages/transformers/models/esm/tokenization_esm.py:65: # TODO, all the tokens are added? But they are also part of the vocab... bit strange.\n",
      "file": ".codex/change_log-large.md",
      "line": 5151,
      "match": "\\bTODO\\b"
    },
    {
      "context": "ite-packages/transformers/models/esm/openfold_utils/residue_constants.py:364: # TODO: ^ interpret this\n- .venv/lib/python3.12/site-packages/transformers/models/esm/",
      "file": ".codex/change_log-large.md",
      "line": 5161,
      "match": "\\bTODO\\b"
    },
    {
      "context": "ite-packages/transformers/models/esm/openfold_utils/residue_constants.py:416: # TODO: this file should be downloaded in a setup script\n- .venv/lib/python3.12/site-p",
      "file": ".codex/change_log-large.md",
      "line": 5162,
      "match": "\\bTODO\\b"
    },
    {
      "context": "ages/transformers/models/table_transformer/modeling_table_transformer.py:361: # TODO find a better way of exposing other arguments\n- .venv/lib/python3.12/site-packa",
      "file": ".codex/change_log-large.md",
      "line": 5166,
      "match": "\\bTODO\\b"
    },
    {
      "context": "ckages/transformers/models/blenderbot_small/modeling_blenderbot_small.py:183: # TODO: we need a refactor so that the different attention modules can get their speci",
      "file": ".codex/change_log-large.md",
      "line": 5180,
      "match": "\\bTODO\\b"
    },
    {
      "context": "s/transformers/models/blenderbot_small/configuration_blenderbot_small.py:189: # TODO: figure this case out.\n- .venv/lib/python3.12/site-packages/transformers/models",
      "file": ".codex/change_log-large.md",
      "line": 5204,
      "match": "\\bTODO\\b"
    },
    {
      "context": "s/transformers/models/blenderbot_small/configuration_blenderbot_small.py:288: # TODO: test this.\n- .venv/lib/python3.12/site-packages/transformers/models/efficientn",
      "file": ".codex/change_log-large.md",
      "line": 5205,
      "match": "\\bTODO\\b"
    },
    {
      "context": "on3.12/site-packages/transformers/models/qwen2_moe/modeling_qwen2_moe.py:375: # TODO cyril: modular\n- .venv/lib/python3.12/site-packages/transformers/models/qwen2_m",
      "file": ".codex/change_log-large.md",
      "line": 5208,
      "match": "\\bTODO\\b"
    },
    {
      "context": "on3.12/site-packages/transformers/models/qwen2_moe/modeling_qwen2_moe.py:388: # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n- .venv/lib/",
      "file": ".codex/change_log-large.md",
      "line": 5210,
      "match": "\\bTODO\\b"
    },
    {
      "context": "on3.12/site-packages/transformers/models/qwen2_moe/modeling_qwen2_moe.py:492: # TODO cyril: modular\n- .venv/lib/python3.12/site-packages/transformers/models/qwen2_m",
      "file": ".codex/change_log-large.md",
      "line": 5211,
      "match": "\\bTODO\\b"
    },
    {
      "context": "on3.12/site-packages/transformers/models/qwen2_moe/modeling_qwen2_moe.py:514: # TODO: Improve this warning with e.g. `model.config.attn_implementation = \"manual\"` o",
      "file": ".codex/change_log-large.md",
      "line": 5213,
      "match": "\\bTODO\\b"
    },
    {
      "context": "on3.12/site-packages/transformers/models/qwen2_moe/modeling_qwen2_moe.py:826: # TODO (joao): remove this exception in v4.56 -- it exists for users that try to pass ",
      "file": ".codex/change_log-large.md",
      "line": 5214,
      "match": "\\bTODO\\b"
    },
    {
      "context": "lib/python3.12/site-packages/transformers/models/ijepa/modeling_ijepa.py:457: # TODO: maybe have a cleaner way to cast the input (from `ImageProcessor` side?)\n- .ve",
      "file": ".codex/change_log-large.md",
      "line": 5270,
      "match": "\\bTODO\\b"
    },
    {
      "context": "3.12/site-packages/transformers/models/groupvit/modeling_tf_groupvit.py:2127: # TODO: As is this currently fails with saved_model=True, because\n- .venv/lib/python3.",
      "file": ".codex/change_log-large.md",
      "line": 5273,
      "match": "\\bTODO\\b"
    },
    {
      "context": "3.12/site-packages/transformers/models/sam2_video/modeling_sam2_video.py:745: # TODO: This leads to ~1e-07 max diff and ~1e-09 avg diff for q_embed and k_embed from",
      "file": ".codex/change_log-large.md",
      "line": 5289,
      "match": "\\bTODO\\b"
    },
    {
      "context": "3.12/site-packages/transformers/models/sam2_video/modular_sam2_video.py:1083: # TODO: This leads to ~1e-07 max diff and ~1e-09 avg diff for q_embed and k_embed from",
      "file": ".codex/change_log-large.md",
      "line": 5310,
      "match": "\\bTODO\\b"
    },
    {
      "context": "12/site-packages/transformers/models/data2vec/modeling_data2vec_audio.py:250: # TODO: we need a refactor so that the different attention modules can get their speci",
      "file": ".codex/change_log-large.md",
      "line": 5352,
      "match": "\\bTODO\\b"
    },
    {
      "context": "v/lib/python3.12/site-packages/transformers/models/mimi/modeling_mimi.py:708: # TODO cyril: modular\n- .venv/lib/python3.12/site-packages/transformers/models/mimi/mo",
      "file": ".codex/change_log-large.md",
      "line": 5365,
      "match": "\\bTODO\\b"
    },
    {
      "context": "v/lib/python3.12/site-packages/transformers/models/mimi/modeling_mimi.py:719: # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n- .venv/lib/",
      "file": ".codex/change_log-large.md",
      "line": 5367,
      "match": "\\bTODO\\b"
    },
    {
      "context": "v/lib/python3.12/site-packages/transformers/models/mimi/modeling_mimi.py:764: # TODO: These transpose are quite inefficient but Flash Attention requires the layout ",
      "file": ".codex/change_log-large.md",
      "line": 5368,
      "match": "\\bTODO\\b"
    },
    {
      "context": "v/lib/python3.12/site-packages/transformers/models/mimi/modeling_mimi.py:826: # TODO cyril: modular\n- .venv/lib/python3.12/site-packages/transformers/models/mimi/mo",
      "file": ".codex/change_log-large.md",
      "line": 5369,
      "match": "\\bTODO\\b"
    },
    {
      "context": "v/lib/python3.12/site-packages/transformers/models/mimi/modeling_mimi.py:848: # TODO: Improve this warning with e.g. `model.config.attn_implementation = \"manual\"` o",
      "file": ".codex/change_log-large.md",
      "line": 5371,
      "match": "\\bTODO\\b"
    },
    {
      "context": "/lib/python3.12/site-packages/transformers/models/mimi/modeling_mimi.py:1101: # TODO (joao): remove this exception in v4.56 -- it exists for users that try to pass ",
      "file": ".codex/change_log-large.md",
      "line": 5373,
      "match": "\\bTODO\\b"
    },
    {
      "context": "/lib/python3.12/site-packages/transformers/models/mimi/modeling_mimi.py:1471: # TODO: @eustlb, let's make the encoder support padding_mask so that batched inputs ar",
      "file": ".codex/change_log-large.md",
      "line": 5374,
      "match": "\\bTODO\\b"
    },
    {
      "context": "/lib/python3.12/site-packages/transformers/models/mimi/modeling_mimi.py:1474: # TODO: @eustlb, convert the padding mask to attention mask.\n- .venv/lib/python3.12/si",
      "file": ".codex/change_log-large.md",
      "line": 5375,
      "match": "\\bTODO\\b"
    },
    {
      "context": "b/python3.12/site-packages/transformers/models/t5/tokenization_t5_fast.py:38: # TODO(PVP) - this should be removed in Transformers v5\n- .venv/lib/python3.12/site-pa",
      "file": ".codex/change_log-large.md",
      "line": 5399,
      "match": "\\bTODO\\b"
    },
    {
      "context": "venv/lib/python3.12/site-packages/transformers/models/t5/modeling_t5.py:1800: # TODO(thom): Add z_loss https://github.com/tensorflow/mesh/blob/fa19d69eafc9a482aff0b",
      "file": ".codex/change_log-large.md",
      "line": 5409,
      "match": "\\bTODO\\b"
    },
    {
      "context": "lib/python3.12/site-packages/transformers/models/flava/modeling_flava.py:604: # TODO: Check fp32 layer norm possibility\n- .venv/lib/python3.12/site-packages/transfo",
      "file": ".codex/change_log-large.md",
      "line": 5453,
      "match": "\\bTODO\\b"
    },
    {
      "context": "formers/models/yolos/image_processing_yolos.py:1439: # POSTPROCESSING METHODS - TODO: add support for other frameworks\n- .venv/lib/python3.12/site-packages/transfor",
      "file": ".codex/change_log-large.md",
      "line": 5472,
      "match": "\\bTODO\\b"
    },
    {
      "context": "packages/transformers/models/mask2former/image_processing_mask2former.py:303: # TODO: (Amy) Move to image_transforms\n- .venv/lib/python3.12/site-packages/transforme",
      "file": ".codex/change_log-large.md",
      "line": 5489,
      "match": "\\bTODO\\b"
    },
    {
      "context": "packages/transformers/models/mask2former/image_processing_mask2former.py:687: # TODO: (Amy)\n- .venv/lib/python3.12/site-packages/transformers/models/mask2former/ima",
      "file": ".codex/change_log-large.md",
      "line": 5493,
      "match": "\\bTODO\\b"
    },
    {
      "context": "on3.12/site-packages/transformers/models/gpt_sw3/tokenization_gpt_sw3.py:215: # TODO: Check if this is needed, as it ensures that decode(encode(doc)) != doc by addi",
      "file": ".codex/change_log-large.md",
      "line": 5529,
      "match": "\\bTODO\\b"
    },
    {
      "context": "hon3.12/site-packages/transformers/models/xlnet/modeling_tf_xlnet.py:525: qlen: TODO Lysandre didn't fill\n- .venv/lib/python3.12/site-packages/transformers/models/x",
      "file": ".codex/change_log-large.md",
      "line": 5543,
      "match": "\\bTODO\\b"
    },
    {
      "context": "hon3.12/site-packages/transformers/models/xlnet/modeling_tf_xlnet.py:526: mlen: TODO Lysandre didn't fill\n- .venv/lib/python3.12/site-packages/transformers/models/x",
      "file": ".codex/change_log-large.md",
      "line": 5544,
      "match": "\\bTODO\\b"
    },
    {
      "context": "se/configuration_hunyuan_v1_dense.py:148: # self._rope_scaling_validation()   # TODO: Need validation?\n- .venv/lib/python3.12/site-packages/transformers/models/layo",
      "file": ".codex/change_log-large.md",
      "line": 5608,
      "match": "\\bTODO\\b"
    },
    {
      "context": "env/lib/python3.12/site-packages/transformers/models/csm/modeling_csm.py:845: # TODO: @eustlb, this should be batched !!!\n- .venv/lib/python3.12/site-packages/trans",
      "file": ".codex/change_log-large.md",
      "line": 5627,
      "match": "\\bTODO\\b"
    },
    {
      "context": "v/lib/python3.12/site-packages/transformers/models/csm/processing_csm.py:169: # TODO: @eustlb, this should be in AudioProcessor\n- .venv/lib/python3.12/site-packages",
      "file": ".codex/change_log-large.md",
      "line": 5629,
      "match": "\\bTODO\\b"
    },
    {
      "context": "v/lib/python3.12/site-packages/transformers/models/csm/generation_csm.py:265: # TODO (joao): this OP throws \"skipping cudagraphs due to ['incompatible ops']\", find ",
      "file": ".codex/change_log-large.md",
      "line": 5632,
      "match": "\\bTODO\\b"
    },
    {
      "context": "v/lib/python3.12/site-packages/transformers/models/csm/generation_csm.py:472: # TODO: @eustlb, this should be batched !!!\n- .venv/lib/python3.12/site-packages/trans",
      "file": ".codex/change_log-large.md",
      "line": 5640,
      "match": "\\bTODO\\b"
    },
    {
      "context": "venv/lib/python3.12/site-packages/transformers/models/csm/modular_csm.py:523: # TODO: @eustlb, this should be batched !!!\n- .venv/lib/python3.12/site-packages/trans",
      "file": ".codex/change_log-large.md",
      "line": 5650,
      "match": "\\bTODO\\b"
    },
    {
      "context": "/site-packages/transformers/models/longformer/modeling_tf_longformer.py:1030: # TODO: This code is most likely not very efficient and should be improved\n- .venv/lib",
      "file": ".codex/change_log-large.md",
      "line": 5704,
      "match": "\\bTODO\\b"
    },
    {
      "context": "3.12/site-packages/transformers/models/longformer/modeling_longformer.py:621: # TODO: remove the redundant computation\n- .venv/lib/python3.12/site-packages/transfor",
      "file": ".codex/change_log-large.md",
      "line": 5729,
      "match": "\\bTODO\\b"
    },
    {
      "context": "3.12/site-packages/transformers/models/longformer/modeling_longformer.py:736: # TODO replace this with\n- .venv/lib/python3.12/site-packages/transformers/models/long",
      "file": ".codex/change_log-large.md",
      "line": 5730,
      "match": "\\bTODO\\b"
    },
    {
      "context": "ansformers/models/instructblipvideo/image_processing_instructblipvideo.py:45: # TODO (raushan): processor can be removed after v5 release. Kept for backwards compat",
      "file": ".codex/change_log-large.md",
      "line": 5747,
      "match": "\\bTODO\\b"
    },
    {
      "context": "modeling_instructblipvideo.py:1142: _keep_in_fp32_modules = [\"query_tokens\"]  # TODO @ArthurZucker I don't know why this is required for FP8\n- .venv/lib/python3.12/",
      "file": ".codex/change_log-large.md",
      "line": 5749,
      "match": "\\bTODO\\b"
    },
    {
      "context": "modeling_instructblipvideo.py:1363: _keep_in_fp32_modules = [\"query_tokens\"]  # TODO @ArthurZucker I don't know why this is required for FP8\n- .venv/lib/python3.12/",
      "file": ".codex/change_log-large.md",
      "line": 5752,
      "match": "\\bTODO\\b"
    },
    {
      "context": "b/python3.12/site-packages/transformers/models/clip/modeling_tf_clip.py:1446: # TODO: As is this currently fails with saved_model=True, because\n- .venv/lib/python3.",
      "file": ".codex/change_log-large.md",
      "line": 5761,
      "match": "\\bTODO\\b"
    },
    {
      "context": "okenization_byt5.py:97: additional_special_tokens=additional_special_tokens,  # TODO extra ids are not used :sweatywmile:\n- .venv/lib/python3.12/site-packages/trans",
      "file": ".codex/change_log-large.md",
      "line": 5766,
      "match": "\\bTODO\\b"
    },
    {
      "context": "ython3.12/site-packages/transformers/models/videomae/modeling_videomae.py:85: # TODO: make it with torch instead of numpy\n- .venv/lib/python3.12/site-packages/trans",
      "file": ".codex/change_log-large.md",
      "line": 5819,
      "match": "\\bTODO\\b"
    },
    {
      "context": "v/lib/python3.12/site-packages/transformers/models/dbrx/modeling_dbrx.py:324: # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n- .venv/lib/",
      "file": ".codex/change_log-large.md",
      "line": 5872,
      "match": "\\bTODO\\b"
    },
    {
      "context": "v/lib/python3.12/site-packages/transformers/models/dbrx/modeling_dbrx.py:379: # TODO: These transpose are quite inefficient but Flash Attention requires the layout\n",
      "file": ".codex/change_log-large.md",
      "line": 5873,
      "match": "\\bTODO\\b"
    },
    {
      "context": "v/lib/python3.12/site-packages/transformers/models/dbrx/modeling_dbrx.py:458: # TODO: Improve this warning with e.g. `model.config.attn_implementation = \"manual\"` o",
      "file": ".codex/change_log-large.md",
      "line": 5875,
      "match": "\\bTODO\\b"
    },
    {
      "context": "v/lib/python3.12/site-packages/transformers/models/dbrx/modeling_dbrx.py:913: # TODO (joao): remove this exception in v4.56 -- it exists for users that try to pass ",
      "file": ".codex/change_log-large.md",
      "line": 5876,
      "match": "\\bTODO\\b"
    },
    {
      "context": "on3.12/site-packages/transformers/models/pegasus_x/modeling_pegasus_x.py:220: # TODO: we need a refactor so that the different attention modules can get their speci",
      "file": ".codex/change_log-large.md",
      "line": 5880,
      "match": "\\bTODO\\b"
    },
    {
      "context": "n3.12/site-packages/transformers/models/dpt/image_processing_dpt_fast.py:285: # TODO: add support for other frameworks\n- .venv/lib/python3.12/site-packages/transfor",
      "file": ".codex/change_log-large.md",
      "line": 5892,
      "match": "\\bTODO\\b"
    },
    {
      "context": "python3.12/site-packages/transformers/models/dpt/image_processing_dpt.py:610: # TODO: add support for other frameworks\n- .venv/lib/python3.12/site-packages/transfor",
      "file": ".codex/change_log-large.md",
      "line": 5898,
      "match": "\\bTODO\\b"
    },
    {
      "context": ".12/site-packages/transformers/models/nougat/tokenization_nougat_fast.py:537: # TODO Come up with footnote formatting inside a table\n- .venv/lib/python3.12/site-pac",
      "file": ".codex/change_log-large.md",
      "line": 5911,
      "match": "\\bTODO\\b"
    },
    {
      "context": "on3.12/site-packages/transformers/models/diffllama/modeling_diffllama.py:232: # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n- .venv/lib/",
      "file": ".codex/change_log-large.md",
      "line": 5920,
      "match": "\\bTODO\\b"
    },
    {
      "context": "on3.12/site-packages/transformers/models/diffllama/modeling_diffllama.py:284: # TODO: These transpose are quite inefficient but Flash Attention requires the layout ",
      "file": ".codex/change_log-large.md",
      "line": 5921,
      "match": "\\bTODO\\b"
    },
    {
      "context": "hon3.12/site-packages/transformers/models/diffllama/modular_diffllama.py:169: # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n- .venv/lib/",
      "file": ".codex/change_log-large.md",
      "line": 5929,
      "match": "\\bTODO\\b"
    },
    {
      "context": "hon3.12/site-packages/transformers/models/diffllama/modular_diffllama.py:221: # TODO: These transpose are quite inefficient but Flash Attention requires the layout ",
      "file": ".codex/change_log-large.md",
      "line": 5930,
      "match": "\\bTODO\\b"
    },
    {
      "context": "site-packages/transformers/models/qwen2_5_vl/configuration_qwen2_5_vl.py:261: # TODO: @raushan update config in the hub\n- .venv/lib/python3.12/site-packages/transfo",
      "file": ".codex/change_log-large.md",
      "line": 5957,
      "match": "\\bTODO\\b"
    },
    {
      "context": "3.12/site-packages/transformers/models/blenderbot/modeling_blenderbot.py:199: # TODO: we need a refactor so that the different attention modules can get their speci",
      "file": ".codex/change_log-large.md",
      "line": 6001,
      "match": "\\bTODO\\b"
    },
    {
      "context": "ansformers/models/kyutai_speech_to_text/modular_kyutai_speech_to_text.py:444: # TODO: @eustlb, this should be standardized\n- .venv/lib/python3.12/site-packages/tran",
      "file": ".codex/change_log-large.md",
      "line": 6017,
      "match": "\\bTODO\\b"
    },
    {
      "context": "ansformers/models/kyutai_speech_to_text/modular_kyutai_speech_to_text.py:472: # TODO: @eustlb, this should be standardized\n- .venv/lib/python3.12/site-packages/tran",
      "file": ".codex/change_log-large.md",
      "line": 6018,
      "match": "\\bTODO\\b"
    },
    {
      "context": "ansformers/models/kyutai_speech_to_text/modular_kyutai_speech_to_text.py:489: # TODO: @eustlb, we should have per-batch-idx values\n- .venv/lib/python3.12/site-packa",
      "file": ".codex/change_log-large.md",
      "line": 6019,
      "match": "\\bTODO\\b"
    },
    {
      "context": "nsformers/models/kyutai_speech_to_text/modeling_kyutai_speech_to_text.py:498: # TODO cyril: modular\n- .venv/lib/python3.12/site-packages/transformers/models/kyutai_",
      "file": ".codex/change_log-large.md",
      "line": 6024,
      "match": "\\bTODO\\b"
    },
    {
      "context": "nsformers/models/kyutai_speech_to_text/modeling_kyutai_speech_to_text.py:509: # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n- .venv/lib/",
      "file": ".codex/change_log-large.md",
      "line": 6026,
      "match": "\\bTODO\\b"
    },
    {
      "context": "nsformers/models/kyutai_speech_to_text/modeling_kyutai_speech_to_text.py:559: # TODO: These transpose are quite inefficient but Flash Attention requires the layout ",
      "file": ".codex/change_log-large.md",
      "line": 6027,
      "match": "\\bTODO\\b"
    },
    {
      "context": "nsformers/models/kyutai_speech_to_text/modeling_kyutai_speech_to_text.py:621: # TODO cyril: modular\n- .venv/lib/python3.12/site-packages/transformers/models/kyutai_",
      "file": ".codex/change_log-large.md",
      "line": 6028,
      "match": "\\bTODO\\b"
    },
    {
      "context": "nsformers/models/kyutai_speech_to_text/modeling_kyutai_speech_to_text.py:643: # TODO: Improve this warning with e.g. `model.config.attn_implementation = \"manual\"` o",
      "file": ".codex/change_log-large.md",
      "line": 6030,
      "match": "\\bTODO\\b"
    },
    {
      "context": "nsformers/models/kyutai_speech_to_text/modeling_kyutai_speech_to_text.py:871: # TODO (joao): remove this exception in v4.56 -- it exists for users that try to pass ",
      "file": ".codex/change_log-large.md",
      "line": 6031,
      "match": "\\bTODO\\b"
    },
    {
      "context": "sformers/models/kyutai_speech_to_text/modeling_kyutai_speech_to_text.py:1309: # TODO: @eustlb, this should be standardized\n- .venv/lib/python3.12/site-packages/tran",
      "file": ".codex/change_log-large.md",
      "line": 6033,
      "match": "\\bTODO\\b"
    },
    {
      "context": "sformers/models/kyutai_speech_to_text/modeling_kyutai_speech_to_text.py:1337: # TODO: @eustlb, this should be standardized\n- .venv/lib/python3.12/site-packages/tran",
      "file": ".codex/change_log-large.md",
      "line": 6034,
      "match": "\\bTODO\\b"
    },
    {
      "context": "sformers/models/kyutai_speech_to_text/modeling_kyutai_speech_to_text.py:1354: # TODO: @eustlb, we should have per-batch-idx values\n- .venv/lib/python3.12/site-packa",
      "file": ".codex/change_log-large.md",
      "line": 6035,
      "match": "\\bTODO\\b"
    },
    {
      "context": "ite-packages/transformers/models/xlm_roberta/tokenization_xlm_roberta.py:260: # TODO check if the t5/llama PR also applies here\n- .venv/lib/python3.12/site-packages",
      "file": ".codex/change_log-large.md",
      "line": 6042,
      "match": "\\bTODO\\b"
    },
    {
      "context": "12/site-packages/transformers/models/xlm_roberta/modeling_xlm_roberta.py:293: # TODO: Improve this warning with e.g. `model.config._attn_implementation = \"manual\"` ",
      "file": ".codex/change_log-large.md",
      "line": 6047,
      "match": "\\bTODO\\b"
    },
    {
      "context": "-packages/transformers/models/granite_speech/processing_granite_speech.py:68: # TODO (@alex-jw-brooks); we should add a util to get_num_audio_tokens\n- .venv/lib/pyt",
      "file": ".codex/change_log-large.md",
      "line": 6059,
      "match": "\\bTODO\\b"
    },
    {
      "context": "e-packages/transformers/models/granite_speech/modeling_granite_speech.py:166: # TODO (@avihu111) find a fast alternative to einsum\n- .venv/lib/python3.12/site-packa",
      "file": ".codex/change_log-large.md",
      "line": 6061,
      "match": "\\bTODO\\b"
    },
    {
      "context": "e-packages/transformers/models/granite_speech/modeling_granite_speech.py:383: # TODO (@alex-jw-brooks) add an example to this docstring once models are released\n- .",
      "file": ".codex/change_log-large.md",
      "line": 6062,
      "match": "\\bTODO\\b"
    },
    {
      "context": "s/transformers/models/granite_speech/feature_extraction_granite_speech.py:81: # TODO (@alex-jw-brooks): Currently input_features_mask is not\n- .venv/lib/python3.12/",
      "file": ".codex/change_log-large.md",
      "line": 6065,
      "match": "\\bTODO\\b"
    },
    {
      "context": "ages/transformers/models/gpt_neox_japanese/modeling_gpt_neox_japanese.py:453: # TODO (joao): remove this exception in v4.56 -- it exists for users that try to pass ",
      "file": ".codex/change_log-large.md",
      "line": 6080,
      "match": "\\bTODO\\b"
    },
    {
      "context": "e-packages/transformers/models/speech_to_text/modeling_speech_to_text.py:257: # TODO: we need a refactor so that the different attention modules can get their speci",
      "file": ".codex/change_log-large.md",
      "line": 6139,
      "match": "\\bTODO\\b"
    },
    {
      "context": "e-packages/transformers/models/speech_to_text/modeling_speech_to_text.py:395: # TODO: change copy when applying cache class\n- .venv/lib/python3.12/site-packages/tra",
      "file": ".codex/change_log-large.md",
      "line": 6140,
      "match": "\\bTODO\\b"
    },
    {
      "context": "e-packages/transformers/models/speech_to_text/modeling_speech_to_text.py:517: # TODO: tests would need a rewrite to check for correct implementation\n- .venv/lib/pyt",
      "file": ".codex/change_log-large.md",
      "line": 6141,
      "match": "\\bTODO\\b"
    },
    {
      "context": "ckages/transformers/models/speech_to_text/modeling_tf_speech_to_text.py:1417: # TODO (Joao): investigate why Speech2Text has numerical issues in XLA generate\n- .ven",
      "file": ".codex/change_log-large.md",
      "line": 6157,
      "match": "\\bTODO\\b"
    },
    {
      "context": "thon3.12/site-packages/transformers/models/musicgen/modeling_musicgen.py:233: # TODO: we need a refactor so that the different attention modules can get their speci",
      "file": ".codex/change_log-large.md",
      "line": 6162,
      "match": "\\bTODO\\b"
    },
    {
      "context": "hon3.12/site-packages/transformers/models/barthez/tokenization_barthez.py:35: # TODO this class is useless. This is the most standard sentencpiece model. Let's find",
      "file": ".codex/change_log-large.md",
      "line": 6196,
      "match": "\\bTODO\\b"
    },
    {
      "context": "hon3.12/site-packages/transformers/models/idefics2/modeling_idefics2.py:1061: # TODO (joao): remove this exception in v4.56 -- it exists for users that try to pass ",
      "file": ".codex/change_log-large.md",
      "line": 6205,
      "match": "\\bTODO\\b"
    },
    {
      "context": "transformers/models/dots1/configuration_dots1.py:109: base_model_tp_plan = {  # TODO: only replicate attention layers when > first_k_dense_replace\n- .venv/lib/pytho",
      "file": ".codex/change_log-large.md",
      "line": 6217,
      "match": "\\bTODO\\b"
    },
    {
      "context": "e to grayscale format using the NTSC formula. Only support numpy and PIL Image. TODO support torch\n- .venv/lib/python3.12/site-packages/transformers/models/efficien",
      "file": ".codex/change_log-large.md",
      "line": 6218,
      "match": "\\bTODO\\b"
    },
    {
      "context": "lib/python3.12/site-packages/transformers/models/sew_d/modeling_sew_d.py:572: # TODO: We should check if the opset_version being used to export\n- .venv/lib/python3.",
      "file": ".codex/change_log-large.md",
      "line": 6244,
      "match": "\\bTODO\\b"
    },
    {
      "context": "b/python3.12/site-packages/transformers/models/falcon/modeling_falcon.py:428: # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n- .venv/lib/",
      "file": ".codex/change_log-large.md",
      "line": 6249,
      "match": "\\bTODO\\b"
    },
    {
      "context": "b/python3.12/site-packages/transformers/models/falcon/modeling_falcon.py:467: # TODO: These transpose are quite inefficient but Flash Attention requires the layout ",
      "file": ".codex/change_log-large.md",
      "line": 6250,
      "match": "\\bTODO\\b"
    },
    {
      "context": "b/python3.12/site-packages/transformers/models/falcon/modeling_falcon.py:762: # TODO (joao): remove this exception in v4.56 -- it exists for users that try to pass ",
      "file": ".codex/change_log-large.md",
      "line": 6252,
      "match": "\\bTODO\\b"
    },
    {
      "context": "b/python3.12/site-packages/transformers/models/falcon/modeling_falcon.py:857: # TODO: As of torch==2.2.0, the `attention_mask` passed to the model in `generate` is ",
      "file": ".codex/change_log-large.md",
      "line": 6253,
      "match": "\\bTODO\\b"
    },
    {
      "context": "env/lib/python3.12/site-packages/transformers/models/sew/modeling_sew.py:303: # TODO: we need a refactor so that the different attention modules can get their speci",
      "file": ".codex/change_log-large.md",
      "line": 6272,
      "match": "\\bTODO\\b"
    },
    {
      "context": "ython3.12/site-packages/transformers/models/gemma3n/modeling_gemma3n.py:1531: # TODO (raushan): Fix this after RoPE refactor. For now we hack it by\n- .venv/lib/pyth",
      "file": ".codex/change_log-large.md",
      "line": 6286,
      "match": "\\bTODO\\b"
    },
    {
      "context": "/site-packages/transformers/models/gemma3n/feature_extraction_gemma3n.py:266: # TODO: The filtered mask is always exactly 3 elements longer than the mel_spectrogram",
      "file": ".codex/change_log-large.md",
      "line": 6288,
      "match": "\\bTODO\\b"
    },
    {
      "context": "python3.12/site-packages/transformers/models/gemma3n/modular_gemma3n.py:1982: # TODO (raushan): Fix this after RoPE refactor. For now we hack it by\n- .venv/lib/pyth",
      "file": ".codex/change_log-large.md",
      "line": 6295,
      "match": "\\bTODO\\b"
    },
    {
      "context": "b/python3.12/site-packages/transformers/models/llama4/modeling_llama4.py:742: # TODO there is a different RoPE for vision encoder, defined as below\n- .venv/lib/pyth",
      "file": ".codex/change_log-large.md",
      "line": 6302,
      "match": "\\bTODO\\b"
    },
    {
      "context": "te-packages/transformers/models/llama4/modeling_llama4.py:812: scaling=None,  # TODO Might be enforced here for TP compatibility as scaling is not just sqrt(head_di",
      "file": ".codex/change_log-large.md",
      "line": 6303,
      "match": "\\bTODO\\b"
    },
    {
      "context": "s/transformers/models/llama4/modeling_llama4.py:900: freqs_ci: torch.Tensor,  # TODO move this to an attribute instead of keeping it around\n- .venv/lib/python3.12/s",
      "file": ".codex/change_log-large.md",
      "line": 6304,
      "match": "\\bTODO\\b"
    },
    {
      "context": "figuration_llama4.py:59: vision_feature_layer (``, *optional*, defaults to -1): TODO\n- .venv/lib/python3.12/site-packages/transformers/models/llama4/configuration_l",
      "file": ".codex/change_log-large.md",
      "line": 6307,
      "match": "\\bTODO\\b"
    },
    {
      "context": "0: vision_feature_select_strategy (`int`, *optional*, defaults to `\"default\"`): TODO\n- .venv/lib/python3.12/site-packages/transformers/models/llama4/configuration_l",
      "file": ".codex/change_log-large.md",
      "line": 6308,
      "match": "\\bTODO\\b"
    },
    {
      "context": "uration_llama4.py:63: pixel_shuffle_ratio (`int`, *optional*, defaults to 0.5): TODO\n- .venv/lib/python3.12/site-packages/transformers/models/llama4/configuration_l",
      "file": ".codex/change_log-large.md",
      "line": 6309,
      "match": "\\bTODO\\b"
    },
    {
      "context": "ration_llama4.py:64: projector_input_dim (`int`, *optional*, defaults to 4096): TODO\n- .venv/lib/python3.12/site-packages/transformers/models/llama4/configuration_l",
      "file": ".codex/change_log-large.md",
      "line": 6310,
      "match": "\\bTODO\\b"
    },
    {
      "context": "ation_llama4.py:65: projector_output_dim (`int`, *optional*, defaults to 4096): TODO\n- .venv/lib/python3.12/site-packages/transformers/models/llama4/configuration_l",
      "file": ".codex/change_log-large.md",
      "line": 6311,
      "match": "\\bTODO\\b"
    },
    {
      "context": "ma4.py:66: multi_modal_projector_bias (`int`, *optional*, defaults to `False`): TODO\n- .venv/lib/python3.12/site-packages/transformers/models/llama4/configuration_l",
      "file": ".codex/change_log-large.md",
      "line": 6312,
      "match": "\\bTODO\\b"
    },
    {
      "context": "iguration_llama4.py:67: projector_dropout (`int`, *optional*, defaults to 0.0): TODO\n- .venv/lib/python3.12/site-packages/transformers/models/llama4/configuration_l",
      "file": ".codex/change_log-large.md",
      "line": 6313,
      "match": "\\bTODO\\b"
    },
    {
      "context": "iguration_llama4.py:68: attention_dropout (`int`, *optional*, defaults to 0.0): TODO\n- .venv/lib/python3.12/site-packages/transformers/models/llama4/configuration_l",
      "file": ".codex/change_log-large.md",
      "line": 6314,
      "match": "\\bTODO\\b"
    },
    {
      "context": "/configuration_llama4.py:69: rope_theta (`int`, *optional*, defaults to 10000): TODO\n- .venv/lib/python3.12/site-packages/transformers/models/llama4/configuration_l",
      "file": ".codex/change_log-large.md",
      "line": 6315,
      "match": "\\bTODO\\b"
    },
    {
      "context": "on_llama4.py:150: intermediate_size_mlp (`int`, *optional*, defaults to 16384): TODO\n- .venv/lib/python3.12/site-packages/transformers/models/llama4/configuration_l",
      "file": ".codex/change_log-large.md",
      "line": 6317,
      "match": "\\bTODO\\b"
    },
    {
      "context": "ma4/configuration_llama4.py:158: head_dim (`int`, *optional*, defaults to 128): TODO\n- .venv/lib/python3.12/site-packages/transformers/models/llama4/configuration_l",
      "file": ".codex/change_log-large.md",
      "line": 6318,
      "match": "\\bTODO\\b"
    },
    {
      "context": "guration_llama4.py:179: attention_dropout (`int`, *optional*, defaults to 0.0): TODO\n- .venv/lib/python3.12/site-packages/transformers/models/llama4/configuration_l",
      "file": ".codex/change_log-large.md",
      "line": 6319,
      "match": "\\bTODO\\b"
    },
    {
      "context": "guration_llama4.py:180: num_experts_per_tok (`int`, *optional*, defaults to 1): TODO\n- .venv/lib/python3.12/site-packages/transformers/models/llama4/configuration_l",
      "file": ".codex/change_log-large.md",
      "line": 6320,
      "match": "\\bTODO\\b"
    },
    {
      "context": "iguration_llama4.py:181: num_local_experts (`int`, *optional*, defaults to 16): TODO\n- .venv/lib/python3.12/site-packages/transformers/models/llama4/configuration_l",
      "file": ".codex/change_log-large.md",
      "line": 6321,
      "match": "\\bTODO\\b"
    },
    {
      "context": "mers/models/llama4/configuration_llama4.py:182: moe_layers (`int`, *optional*): TODO\n- .venv/lib/python3.12/site-packages/transformers/models/llama4/configuration_l",
      "file": ".codex/change_log-large.md",
      "line": 6322,
      "match": "\\bTODO\\b"
    },
    {
      "context": "on_llama4.py:183: interleave_moe_layer_step (`int`, *optional*, defaults to 1): TODO\n- .venv/lib/python3.12/site-packages/transformers/models/llama4/configuration_l",
      "file": ".codex/change_log-large.md",
      "line": 6323,
      "match": "\\bTODO\\b"
    },
    {
      "context": "nfiguration_llama4.py:184: use_qk_norm (`int`, *optional*, defaults to `True`): TODO\n- .venv/lib/python3.12/site-packages/transformers/models/llama4/configuration_l",
      "file": ".codex/change_log-large.md",
      "line": 6324,
      "match": "\\bTODO\\b"
    },
    {
      "context": "n_llama4.py:185: output_router_logits (`int`, *optional*, defaults to `False`): TODO\n- .venv/lib/python3.12/site-packages/transformers/models/llama4/configuration_l",
      "file": ".codex/change_log-large.md",
      "line": 6325,
      "match": "\\bTODO\\b"
    },
    {
      "context": "ion_llama4.py:186: router_aux_loss_coef (`int`, *optional*, defaults to 0.001): TODO\n- .venv/lib/python3.12/site-packages/transformers/models/llama4/configuration_l",
      "file": ".codex/change_log-large.md",
      "line": 6326,
      "match": "\\bTODO\\b"
    },
    {
      "context": "ration_llama4.py:187: router_jitter_noise (`int`, *optional*, defaults to 0.0): TODO\n- .venv/lib/python3.12/site-packages/transformers/models/llama4/configuration_l",
      "file": ".codex/change_log-large.md",
      "line": 6327,
      "match": "\\bTODO\\b"
    },
    {
      "context": "thon3.12/site-packages/transformers/models/llama4/configuration_llama4.py:225: <TODO>\n- .venv/lib/python3.12/site-packages/transformers/models/llama4/configuration_",
      "file": ".codex/change_log-large.md",
      "line": 6328,
      "match": "\\bTODO\\b"
    },
    {
      "context": "thon3.12/site-packages/transformers/models/llama4/configuration_llama4.py:226: <TODO>\n- .venv/lib/python3.12/site-packages/transformers/models/llama4/configuration_",
      "file": ".codex/change_log-large.md",
      "line": 6329,
      "match": "\\bTODO\\b"
    },
    {
      "context": "thon3.12/site-packages/transformers/models/llama4/configuration_llama4.py:235: <TODO>\n- .venv/lib/python3.12/site-packages/transformers/models/llama4/configuration_",
      "file": ".codex/change_log-large.md",
      "line": 6330,
      "match": "\\bTODO\\b"
    },
    {
      "context": "configuration_llama4.py:241: floor_scale (`int`, *optional*, defaults to 8192): TODO\n- .venv/lib/python3.12/site-packages/transformers/models/llama4/configuration_l",
      "file": ".codex/change_log-large.md",
      "line": 6331,
      "match": "\\bTODO\\b"
    },
    {
      "context": "4/configuration_llama4.py:242: attn_scale (`int`, *optional*, defaults to 0.1): TODO\n- .venv/lib/python3.12/site-packages/transformers/models/llama4/image_processin",
      "file": ".codex/change_log-large.md",
      "line": 6332,
      "match": "\\bTODO\\b"
    },
    {
      "context": "nt_gemma.py:323: # `sqrt` in order to prevent NaNs during training in bfloat16. TODO a bit annoying\n- .venv/lib/python3.12/site-packages/transformers/models/recurre",
      "file": ".codex/change_log-large.md",
      "line": 6340,
      "match": "\\bTODO\\b"
    },
    {
      "context": "packages/transformers/models/recurrent_gemma/modeling_recurrent_gemma.py:340: # TODO refactor\n- .venv/lib/python3.12/site-packages/transformers/models/recurrent_gem",
      "file": ".codex/change_log-large.md",
      "line": 6341,
      "match": "\\bTODO\\b"
    },
    {
      "context": "deling_recurrent_gemma.py:624: if use_cache and inputs_embeds.shape[1] != 1:  # TODO let's maybe only call in the `generate`?\n- .venv/lib/python3.12/site-packages/t",
      "file": ".codex/change_log-large.md",
      "line": 6343,
      "match": "\\bTODO\\b"
    },
    {
      "context": "packages/transformers/models/recurrent_gemma/modeling_recurrent_gemma.py:688: # TODO: re-enable check: Copied from transformers.models.llama.modeling_llama.LlamaFor",
      "file": ".codex/change_log-large.md",
      "line": 6344,
      "match": "\\bTODO\\b"
    },
    {
      "context": "s/models/recurrent_gemma/modeling_recurrent_gemma.py:758: # Soft-cap the logits TODO remove if always done.\n- .venv/lib/python3.12/site-packages/transformers/models",
      "file": ".codex/change_log-large.md",
      "line": 6345,
      "match": "\\bTODO\\b"
    },
    {
      "context": "nv/lib/python3.12/site-packages/transformers/models/bloom/modeling_bloom.py:62: TODO @thomasw21 this doesn't work as nicely due to the masking strategy, and so mask",
      "file": ".codex/change_log-large.md",
      "line": 6353,
      "match": "\\bTODO\\b"
    },
    {
      "context": "lib/python3.12/site-packages/transformers/models/bloom/modeling_bloom.py:548: # TODO (joao): remove this exception in v4.56 -- it exists for users that try to pass ",
      "file": ".codex/change_log-large.md",
      "line": 6358,
      "match": "\\bTODO\\b"
    },
    {
      "context": "n3.12/site-packages/transformers/models/bloom/tokenization_bloom_fast.py:113: # TODO @ArthurZucker this can only work one way for now, to update later-on. Tests sho",
      "file": ".codex/change_log-large.md",
      "line": 6376,
      "match": "\\bTODO\\b"
    },
    {
      "context": "ython3.12/site-packages/transformers/models/bloom/configuration_bloom.py:157: # TODO: how to do that better?\n- .venv/lib/python3.12/site-packages/transformers/model",
      "file": ".codex/change_log-large.md",
      "line": 6378,
      "match": "\\bTODO\\b"
    },
    {
      "context": "python3.12/site-packages/transformers/models/codegen/modeling_codegen.py:164: # TODO(enijkamp): factor out number of logical TPU-v4 cores or make forward pass agnos",
      "file": ".codex/change_log-large.md",
      "line": 6380,
      "match": "\\bTODO\\b"
    },
    {
      "context": "python3.12/site-packages/transformers/models/codegen/modeling_codegen.py:379: # TODO (joao): remove this exception in v4.56 -- it exists for users that try to pass ",
      "file": ".codex/change_log-large.md",
      "line": 6386,
      "match": "\\bTODO\\b"
    },
    {
      "context": "n3.12/site-packages/transformers/models/codegen/configuration_codegen.py:160: # TODO: how to do that better?\n- .venv/lib/python3.12/site-packages/transformers/model",
      "file": ".codex/change_log-large.md",
      "line": 6391,
      "match": "\\bTODO\\b"
    },
    {
      "context": "b/python3.12/site-packages/transformers/models/canine/modeling_canine.py:382: # TODO add support for MLM\n- .venv/lib/python3.12/site-packages/transformers/models/ca",
      "file": ".codex/change_log-large.md",
      "line": 6425,
      "match": "\\bTODO\\b"
    },
    {
      "context": "v/lib/python3.12/site-packages/transformers/models/bart/modeling_bart.py:200: # TODO: we need a refactor so that the different attention modules can get their speci",
      "file": ".codex/change_log-large.md",
      "line": 6439,
      "match": "\\bTODO\\b"
    },
    {
      "context": "/python3.12/site-packages/transformers/models/bart/configuration_bart.py:204: # TODO: figure this case out.\n- .venv/lib/python3.12/site-packages/transformers/models",
      "file": ".codex/change_log-large.md",
      "line": 6458,
      "match": "\\bTODO\\b"
    },
    {
      "context": "/python3.12/site-packages/transformers/models/bart/configuration_bart.py:303: # TODO: test this.\n- .venv/lib/python3.12/site-packages/transformers/models/bart/token",
      "file": ".codex/change_log-large.md",
      "line": 6459,
      "match": "\\bTODO\\b"
    },
    {
      "context": "/site-packages/transformers/models/patchtsmixer/modeling_patchtsmixer.py:313: # TODO: we need a refactor so that the different attention modules can get their speci",
      "file": ".codex/change_log-large.md",
      "line": 6495,
      "match": "\\bTODO\\b"
    },
    {
      "context": "/site-packages/transformers/models/falcon_mamba/modeling_falcon_mamba.py:100: # TODO (joao): add layer_device_map arg and update code in `generate` accordingly\n- .v",
      "file": ".codex/change_log-large.md",
      "line": 6513,
      "match": "\\bTODO\\b"
    },
    {
      "context": "e-packages/transformers/models/maskformer/image_processing_maskformer.py:309: # TODO: (Amy) Move to image_transforms\n- .venv/lib/python3.12/site-packages/transforme",
      "file": ".codex/change_log-large.md",
      "line": 6571,
      "match": "\\bTODO\\b"
    },
    {
      "context": "e-packages/transformers/models/maskformer/image_processing_maskformer.py:690: # TODO: (Amy)\n- .venv/lib/python3.12/site-packages/transformers/models/maskformer/imag",
      "file": ".codex/change_log-large.md",
      "line": 6575,
      "match": "\\bTODO\\b"
    },
    {
      "context": "/site-packages/transformers/models/zoedepth/image_processing_zoedepth.py:233: # TODO support align_corners=True in image_transforms.resize\n- .venv/lib/python3.12/si",
      "file": ".codex/change_log-large.md",
      "line": 6591,
      "match": "\\bTODO\\b"
    },
    {
      "context": "lib/python3.12/site-packages/transformers/models/dinat/modeling_dinat.py:171: # TODO: Support arbitrary patch sizes.\n- .venv/lib/python3.12/site-packages/transforme",
      "file": ".codex/change_log-large.md",
      "line": 6610,
      "match": "\\bTODO\\b"
    },
    {
      "context": "on3.12/site-packages/transformers/models/moonshine/modeling_moonshine.py:472: # TODO arthur, how do we separate when it cross / self coming from different layer?\n- ",
      "file": ".codex/change_log-large.md",
      "line": 6651,
      "match": "\\bTODO\\b"
    },
    {
      "context": "hon3.12/site-packages/transformers/models/moonshine/modular_moonshine.py:504: # TODO arthur, how do we separate when it cross / self coming from different layer?\n- ",
      "file": ".codex/change_log-large.md",
      "line": 6654,
      "match": "\\bTODO\\b"
    },
    {
      "context": "3.12/site-packages/transformers/models/distilbert/modeling_distilbert.py:245: # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n- .venv/lib/",
      "file": ".codex/change_log-large.md",
      "line": 6658,
      "match": "\\bTODO\\b"
    },
    {
      "context": "hon3.12/site-packages/transformers/models/marian/configuration_marian.py:189: # TODO: figure this case out.\n- .venv/lib/python3.12/site-packages/transformers/models",
      "file": ".codex/change_log-large.md",
      "line": 6682,
      "match": "\\bTODO\\b"
    },
    {
      "context": "hon3.12/site-packages/transformers/models/marian/configuration_marian.py:289: # TODO: test this.\n- .venv/lib/python3.12/site-packages/transformers/models/marian/mod",
      "file": ".codex/change_log-large.md",
      "line": 6683,
      "match": "\\bTODO\\b"
    },
    {
      "context": "b/python3.12/site-packages/transformers/models/marian/modeling_marian.py:200: # TODO: we need a refactor so that the different attention modules can get their speci",
      "file": ".codex/change_log-large.md",
      "line": 6685,
      "match": "\\bTODO\\b"
    },
    {
      "context": "e-packages/transformers/models/xlm_roberta_xl/modeling_xlm_roberta_xl.py:290: # TODO: Improve this warning with e.g. `model.config._attn_implementation = \"manual\"` ",
      "file": ".codex/change_log-large.md",
      "line": 6706,
      "match": "\\bTODO\\b"
    },
    {
      "context": "/lib/python3.12/site-packages/transformers/models/sam/modeling_tf_sam.py:692: # TODO Matt: What is going on here? Why is a non-trainable weight randomly initialized",
      "file": ".codex/change_log-large.md",
      "line": 6725,
      "match": "\\bTODO\\b"
    },
    {
      "context": ".12/site-packages/transformers/models/beit/image_processing_beit_fast.py:206: # TODO: add support for other frameworks\n- .venv/lib/python3.12/site-packages/transfor",
      "file": ".codex/change_log-large.md",
      "line": 6761,
      "match": "\\bTODO\\b"
    },
    {
      "context": "thon3.12/site-packages/transformers/models/beit/image_processing_beit.py:476: # TODO: add support for other frameworks\n- .venv/lib/python3.12/site-packages/transfor",
      "file": ".codex/change_log-large.md",
      "line": 6766,
      "match": "\\bTODO\\b"
    },
    {
      "context": "v/lib/python3.12/site-packages/transformers/models/dia/processing_dia.py:167: # TODO: dac with batching is currently broken, but non-batch is working\n- .venv/lib/py",
      "file": ".codex/change_log-large.md",
      "line": 6770,
      "match": "\\bTODO\\b"
    },
    {
      "context": "v/lib/python3.12/site-packages/transformers/models/dia/processing_dia.py:319: # TODO: see above, dac doesn't work in batches yet\n- .venv/lib/python3.12/site-package",
      "file": ".codex/change_log-large.md",
      "line": 6772,
      "match": "\\bTODO\\b"
    },
    {
      "context": "v/lib/python3.12/site-packages/transformers/models/dia/processing_dia.py:373: # TODO: @eustlb, this should be in AudioProcessor\n- .venv/lib/python3.12/site-packages",
      "file": ".codex/change_log-large.md",
      "line": 6773,
      "match": "\\bTODO\\b"
    },
    {
      "context": "b/python3.12/site-packages/transformers/models/plbart/modeling_plbart.py:384: # TODO: we need a refactor so that the different attention modules can get their speci",
      "file": ".codex/change_log-large.md",
      "line": 6815,
      "match": "\\bTODO\\b"
    },
    {
      "context": "n3.12/site-packages/transformers/models/wav2vec2/modeling_tf_wav2vec2.py:461: # TODO Matt: Assigning to attributes in call() is deeply sinful in TensorFlow, as it s",
      "file": ".codex/change_log-large.md",
      "line": 6858,
      "match": "\\bTODO\\b"
    },
    {
      "context": "thon3.12/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py:534: # TODO: we need a refactor so that the different attention modules can get their speci",
      "file": ".codex/change_log-large.md",
      "line": 6866,
      "match": "\\bTODO\\b"
    },
    {
      "context": "12/site-packages/transformers/models/perceiver/tokenization_perceiver.py:182: # TODO @ArthurZ refactor this as well....\n- .venv/lib/python3.12/site-packages/transfo",
      "file": ".codex/change_log-large.md",
      "line": 6976,
      "match": "\\bTODO\\b"
    },
    {
      "context": "v/lib/python3.12/site-packages/transformers/models/deit/modeling_deit.py:470: # TODO: maybe have a cleaner way to cast the input (from `ImageProcessor` side?)\n- .ve",
      "file": ".codex/change_log-large.md",
      "line": 6982,
      "match": "\\bTODO\\b"
    },
    {
      "context": "lib/python3.12/site-packages/transformers/models/fuyu/processing_fuyu.py:127: # TODO Remove this logic in a subsequent release since subsequences are not supported.",
      "file": ".codex/change_log-large.md",
      "line": 7019,
      "match": "\\bTODO\\b"
    },
    {
      "context": "ers/models/fuyu/processing_fuyu.py:360: self.max_position_embeddings = 16384  # TODO Can't derive this from model files: where to set it?\n- .venv/lib/python3.12/sit",
      "file": ".codex/change_log-large.md",
      "line": 7020,
      "match": "\\bTODO\\b"
    },
    {
      "context": "thon3.12/site-packages/transformers/models/fuyu/image_processing_fuyu.py:579: # TODO refer to https://github.com/ArthurZucker/transformers/blob/0f0a3fe5ca5697ee58fa",
      "file": ".codex/change_log-large.md",
      "line": 7028,
      "match": "\\bTODO\\b"
    },
    {
      "context": ": # don't pass kwargs because Persimmon-backbone doesn't accept FA2 kwargs yet, TODO: raushan\n- .venv/lib/python3.12/site-packages/transformers/models/decision_tran",
      "file": ".codex/change_log-large.md",
      "line": 7029,
      "match": "\\bTODO\\b"
    },
    {
      "context": "nv/lib/python3.12/site-packages/transformers/models/sam2/modular_sam2.py:278: # TODO: add connected components kernel for postprocessing\n- .venv/lib/python3.12/site",
      "file": ".codex/change_log-large.md",
      "line": 7050,
      "match": "\\bTODO\\b"
    },
    {
      "context": ".12/site-packages/transformers/models/sam2/image_processing_sam2_fast.py:684: # TODO: add connected components kernel for postprocessing\n- .venv/lib/python3.12/site",
      "file": ".codex/change_log-large.md",
      "line": 7063,
      "match": "\\bTODO\\b"
    },
    {
      "context": "n3.12/site-packages/transformers/models/voxtral/configuration_voxtral.py:109: # TODO: @eustlb, we do not use dropout and layerdrop, yet we need to hardcode them\n- .",
      "file": ".codex/change_log-large.md",
      "line": 7075,
      "match": "\\bTODO\\b"
    },
    {
      "context": "b/python3.12/site-packages/transformers/models/voxtral/modular_voxtral.py:55: # TODO: @eustlb, I would really prefer to use WhisperEncoder but it's messing with mod",
      "file": ".codex/change_log-large.md",
      "line": 7078,
      "match": "\\bTODO\\b"
    },
    {
      "context": "thon3.12/site-packages/transformers/models/voxtral/processing_voxtral.py:286: # TODO: @eustlb, this should be moved to mistral_common + testing\n- .venv/lib/python3.",
      "file": ".codex/change_log-large.md",
      "line": 7082,
      "match": "\\bTODO\\b"
    },
    {
      "context": "ackages/transformers/models/deprecated/vit_hybrid/modeling_vit_hybrid.py:621: # TODO: maybe have a cleaner way to cast the input (from `ImageProcessor` side?)\n- .ve",
      "file": ".codex/change_log-large.md",
      "line": 7108,
      "match": "\\bTODO\\b"
    },
    {
      "context": "n3.12/site-packages/transformers/models/deprecated/deta/modeling_deta.py:424: # TODO fix this\n- .venv/lib/python3.12/site-packages/transformers/models/deprecated/de",
      "file": ".codex/change_log-large.md",
      "line": 7127,
      "match": "\\bTODO\\b"
    },
    {
      "context": "n3.12/site-packages/transformers/models/deprecated/deta/modeling_deta.py:517: # TODO find a better way of exposing other arguments\n- .venv/lib/python3.12/site-packa",
      "file": ".codex/change_log-large.md",
      "line": 7129,
      "match": "\\bTODO\\b"
    },
    {
      "context": "3.12/site-packages/transformers/models/deprecated/deta/modeling_deta.py:1157: # TODO: valid_ratios could be useless here. check https://github.com/fundamentalvision",
      "file": ".codex/change_log-large.md",
      "line": 7133,
      "match": "\\bTODO\\b"
    },
    {
      "context": "sformers/models/deprecated/xlm_prophetnet/tokenization_xlm_prophetnet.py:158: # TODO ArthurZ fairseq_ids_to_tokens should be removed\n- .venv/lib/python3.12/site-pac",
      "file": ".codex/change_log-large.md",
      "line": 7195,
      "match": "\\bTODO\\b"
    },
    {
      "context": "hon3.12/site-packages/transformers/models/deprecated/nat/modeling_nat.py:216: # TODO: Support arbitrary patch sizes.\n- .venv/lib/python3.12/site-packages/transforme",
      "file": ".codex/change_log-large.md",
      "line": 7239,
      "match": "\\bTODO\\b"
    },
    {
      "context": "hon3.12/site-packages/transformers/models/deprecated/nat/modeling_nat.py:931: # TODO can we simplify this?\n- .venv/lib/python3.12/site-packages/transformers/models/",
      "file": ".codex/change_log-large.md",
      "line": 7240,
      "match": "\\bTODO\\b"
    },
    {
      "context": "ite-packages/transformers/models/deprecated/tapex/tokenization_tapex.py:1345: # TODO (Qian): is it possible to revert the original cell if it is in the final answer",
      "file": ".codex/change_log-large.md",
      "line": 7245,
      "match": "\\bTODO\\b"
    },
    {
      "context": "on3.12/site-packages/transformers/models/camembert/modeling_camembert.py:293: # TODO: Improve this warning with e.g. `model.config._attn_implementation = \"manual\"` ",
      "file": ".codex/change_log-large.md",
      "line": 7350,
      "match": "\\bTODO\\b"
    },
    {
      "context": "12/site-packages/transformers/models/camembert/tokenization_camembert.py:196: # TODO decode outputs do not match between fast and slow\n- .venv/lib/python3.12/site-p",
      "file": ".codex/change_log-large.md",
      "line": 7366,
      "match": "\\bTODO\\b"
    },
    {
      "context": "/python3.12/site-packages/transformers/models/pixtral/modeling_pixtral.py:85: # TODO maybe make it torch compatible later on. We can also just slice\n- .venv/lib/pyt",
      "file": ".codex/change_log-large.md",
      "line": 7371,
      "match": "\\bTODO\\b"
    },
    {
      "context": "v/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:342: # TODO: Improve this warning with e.g. `model.config._attn_implementation = \"manual\"` ",
      "file": ".codex/change_log-large.md",
      "line": 7382,
      "match": "\\bTODO\\b"
    },
    {
      "context": "/python3.12/site-packages/transformers/models/gpt2/configuration_gpt2.py:203: # TODO: how to do that better?\n- .venv/lib/python3.12/site-packages/transformers/model",
      "file": ".codex/change_log-large.md",
      "line": 7399,
      "match": "\\bTODO\\b"
    },
    {
      "context": "88: self.t2u_variance_predictor_embed_dim = t2u_variance_predictor_embed_dim  # TODO: add to docstrings\n- .venv/lib/python3.12/site-packages/transformers/models/sea",
      "file": ".codex/change_log-large.md",
      "line": 7439,
      "match": "\\bTODO\\b"
    },
    {
      "context": ": self.t2u_variance_predictor_hidden_dim = t2u_variance_predictor_hidden_dim  # TODO: add to docstrings\n- .venv/lib/python3.12/site-packages/transformers/models/sea",
      "file": ".codex/change_log-large.md",
      "line": 7440,
      "match": "\\bTODO\\b"
    },
    {
      "context": "self.t2u_variance_predictor_kernel_size = t2u_variance_predictor_kernel_size  # TODO: add to docstrings\n- .venv/lib/python3.12/site-packages/transformers/models/sea",
      "file": ".codex/change_log-large.md",
      "line": 7441,
      "match": "\\bTODO\\b"
    },
    {
      "context": "ss_m4t_v2.py:391: self.t2u_variance_pred_dropout = t2u_variance_pred_dropout  # TODO: add to docstrings\n- .venv/lib/python3.12/site-packages/transformers/models/sea",
      "file": ".codex/change_log-large.md",
      "line": 7442,
      "match": "\\bTODO\\b"
    },
    {
      "context": "lib/python3.12/site-packages/transformers/models/moshi/modeling_moshi.py:501: # TODO cyril: modular\n- .venv/lib/python3.12/site-packages/transformers/models/moshi/m",
      "file": ".codex/change_log-large.md",
      "line": 7503,
      "match": "\\bTODO\\b"
    },
    {
      "context": "lib/python3.12/site-packages/transformers/models/moshi/modeling_moshi.py:512: # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n- .venv/lib/",
      "file": ".codex/change_log-large.md",
      "line": 7505,
      "match": "\\bTODO\\b"
    },
    {
      "context": "lib/python3.12/site-packages/transformers/models/moshi/modeling_moshi.py:562: # TODO: These transpose are quite inefficient but Flash Attention requires the layout ",
      "file": ".codex/change_log-large.md",
      "line": 7506,
      "match": "\\bTODO\\b"
    },
    {
      "context": "lib/python3.12/site-packages/transformers/models/moshi/modeling_moshi.py:624: # TODO cyril: modular\n- .venv/lib/python3.12/site-packages/transformers/models/moshi/m",
      "file": ".codex/change_log-large.md",
      "line": 7507,
      "match": "\\bTODO\\b"
    },
    {
      "context": "lib/python3.12/site-packages/transformers/models/moshi/modeling_moshi.py:646: # TODO: Improve this warning with e.g. `model.config.attn_implementation = \"manual\"` o",
      "file": ".codex/change_log-large.md",
      "line": 7509,
      "match": "\\bTODO\\b"
    },
    {
      "context": "ib/python3.12/site-packages/transformers/models/moshi/modeling_moshi.py:1278: # TODO (joao): remove this exception in v4.56 -- it exists for users that try to pass ",
      "file": ".codex/change_log-large.md",
      "line": 7512,
      "match": "\\bTODO\\b"
    },
    {
      "context": "v/lib/python3.12/site-packages/transformers/models/detr/modeling_detr.py:403: # TODO find a better way of exposing other arguments\n- .venv/lib/python3.12/site-packa",
      "file": ".codex/change_log-large.md",
      "line": 7554,
      "match": "\\bTODO\\b"
    },
    {
      "context": "thon3.12/site-packages/transformers/models/detr/image_processing_detr.py:638: # TODO - (Amy) make compatible with other frameworks\n- .venv/lib/python3.12/site-packa",
      "file": ".codex/change_log-large.md",
      "line": 7578,
      "match": "\\bTODO\\b"
    },
    {
      "context": "thon3.12/site-packages/transformers/models/detr/image_processing_detr.py:661: # TODO - (Amy) make compatible with other frameworks\n- .venv/lib/python3.12/site-packa",
      "file": ".codex/change_log-large.md",
      "line": 7579,
      "match": "\\bTODO\\b"
    },
    {
      "context": "hon3.12/site-packages/transformers/models/detr/image_processing_detr.py:1042: # TODO (Amy) - update to use `rescale_factor` instead of `scale`\n- .venv/lib/python3.1",
      "file": ".codex/change_log-large.md",
      "line": 7580,
      "match": "\\bTODO\\b"
    },
    {
      "context": "nsformers/models/detr/image_processing_detr.py:1503: # POSTPROCESSING METHODS - TODO: add support for other frameworks\n- .venv/lib/python3.12/site-packages/transfor",
      "file": ".codex/change_log-large.md",
      "line": 7582,
      "match": "\\bTODO\\b"
    },
    {
      "context": "v/lib/python3.12/site-packages/transformers/models/gptj/modeling_gptj.py:270: # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n- .venv/lib/",
      "file": ".codex/change_log-large.md",
      "line": 7595,
      "match": "\\bTODO\\b"
    },
    {
      "context": "v/lib/python3.12/site-packages/transformers/models/gptj/modeling_gptj.py:659: # TODO (joao): remove this exception in v4.56 -- it exists for users that try to pass ",
      "file": ".codex/change_log-large.md",
      "line": 7601,
      "match": "\\bTODO\\b"
    },
    {
      "context": "/python3.12/site-packages/transformers/models/gptj/configuration_gptj.py:149: # TODO: how to do that better?\n- .venv/lib/python3.12/site-packages/transformers/model",
      "file": ".codex/change_log-large.md",
      "line": 7627,
      "match": "\\bTODO\\b"
    },
    {
      "context": "ers/models/vision_text_dual_encoder/modeling_vision_text_dual_encoder.py:388: # TODO: Should we use the pre-trained projection as well ?\n- .venv/lib/python3.12/site",
      "file": ".codex/change_log-large.md",
      "line": 7649,
      "match": "\\bTODO\\b"
    },
    {
      "context": "env/lib/python3.12/site-packages/transformers/integrations/executorch.py:778: # TODO: The default inputs only work for text models. We need to add support for visio",
      "file": ".codex/change_log-large.md",
      "line": 7661,
      "match": "\\bTODO\\b"
    },
    {
      "context": "nv/lib/python3.12/site-packages/transformers/integrations/executorch.py:1077: # TODO (tmanlaibaatar) This won't be needed in torch 2.7.\n- .venv/lib/python3.12/site-",
      "file": ".codex/change_log-large.md",
      "line": 7662,
      "match": "\\bTODO\\b"
    },
    {
      "context": "8.py:385: # when changing a layer the TP PLAN for that layer should be updated. TODO\n- .venv/lib/python3.12/site-packages/transformers/integrations/flash_attention.",
      "file": ".codex/change_log-large.md",
      "line": 7663,
      "match": "\\bTODO\\b"
    },
    {
      "context": "lib/python3.12/site-packages/transformers/integrations/flex_attention.py:108: # TODO: deprecate / rename to make_flex_block_mask for clarity as it's not only causal",
      "file": ".codex/change_log-large.md",
      "line": 7665,
      "match": "\\bTODO\\b"
    },
    {
      "context": "s\n- .venv/lib/python3.12/site-packages/transformers/integrations/mxfp4.py:76: # TODO: Add absolute link when the repo is public\n- .venv/lib/python3.12/site-packages",
      "file": ".codex/change_log-large.md",
      "line": 7677,
      "match": "\\bTODO\\b"
    },
    {
      "context": "s/transformers/integrations/mxfp4.py:82: rows_per_chunk: int = 32768 * 1024,  # TODO these values are not here by mistake ;)\n- .venv/lib/python3.12/site-packages/tr",
      "file": ".codex/change_log-large.md",
      "line": 7678,
      "match": "\\bTODO\\b"
    },
    {
      "context": "transformers/integrations/mxfp4.py:95: scales = scales.to(torch.int32) - 127  # TODO that's because 128=2**7\n- .venv/lib/python3.12/site-packages/transformers/integ",
      "file": ".codex/change_log-large.md",
      "line": 7680,
      "match": "\\bTODO\\b"
    },
    {
      "context": "\n- .venv/lib/python3.12/site-packages/transformers/integrations/mxfp4.py:203: # TODO: Add absolute link when the repo is public\n- .venv/lib/python3.12/site-packages",
      "file": ".codex/change_log-large.md",
      "line": 7681,
      "match": "\\bTODO\\b"
    },
    {
      "context": "ib/python3.12/site-packages/transformers/integrations/tensor_parallel.py:473: # TODO: figure out dynamo support for instance method and switch this to instance meth",
      "file": ".codex/change_log-large.md",
      "line": 7697,
      "match": "\\bTODO\\b"
    },
    {
      "context": "mers/integrations/tensor_parallel.py:480: param = param / device_mesh.size()  # TODO should be optionable\n- .venv/lib/python3.12/site-packages/transformers/integrat",
      "file": ".codex/change_log-large.md",
      "line": 7698,
      "match": "\\bTODO\\b"
    },
    {
      "context": "ib/python3.12/site-packages/transformers/integrations/tensor_parallel.py:481: # TODO: assumes parent module will allreduce the output afterwards (e.g rowlinear bias",
      "file": ".codex/change_log-large.md",
      "line": 7699,
      "match": "\\bTODO\\b"
    },
    {
      "context": "ib/python3.12/site-packages/transformers/integrations/tensor_parallel.py:508: # TODO: figure out dynamo support for instance method and switch this to instance meth",
      "file": ".codex/change_log-large.md",
      "line": 7700,
      "match": "\\bTODO\\b"
    },
    {
      "context": "ib/python3.12/site-packages/transformers/integrations/tensor_parallel.py:550: # TODO: figure out dynamo support for instance method and switch this to instance meth",
      "file": ".codex/change_log-large.md",
      "line": 7701,
      "match": "\\bTODO\\b"
    },
    {
      "context": "ib/python3.12/site-packages/transformers/integrations/tensor_parallel.py:889: # TODO: i'd like for this to be the default\n- .venv/lib/python3.12/site-packages/trans",
      "file": ".codex/change_log-large.md",
      "line": 7706,
      "match": "\\bTODO\\b"
    },
    {
      "context": "ib/python3.12/site-packages/transformers/integrations/tensor_parallel.py:896: # TODO: need an abstract Parallel class that is different from TensorParallelLayer\n- .",
      "file": ".codex/change_log-large.md",
      "line": 7707,
      "match": "\\bTODO\\b"
    },
    {
      "context": "ib/python3.12/site-packages/transformers/integrations/tensor_parallel.py:945: # TODO: this logic should be wrapped in a function, this is copied from corresponding ",
      "file": ".codex/change_log-large.md",
      "line": 7708,
      "match": "\\bTODO\\b"
    },
    {
      "context": "thon3.12/site-packages/transformers/integrations/tensor_parallel.py:1001: ):  # TODO: rename to shard_and_distribute_param\n- .venv/lib/python3.12/site-packages/tran",
      "file": ".codex/change_log-large.md",
      "line": 7710,
      "match": "\\bTODO\\b"
    },
    {
      "context": "ions/tensor_parallel.py:1017: module_to_tp = model.get_submodule(param_name)  # TODO: can i loop over modules?\n- .venv/lib/python3.12/site-packages/transformers/int",
      "file": ".codex/change_log-large.md",
      "line": 7711,
      "match": "\\bTODO\\b"
    },
    {
      "context": "n.\n- .venv/lib/python3.12/site-packages/transformers/commands/serving.py:201: # TODO (joao, matt): streamline tool token detection logic\n- .venv/lib/python3.12/site",
      "file": ".codex/change_log-large.md",
      "line": 7759,
      "match": "\\bTODO\\b"
    },
    {
      "context": " \"\n- .venv/lib/python3.12/site-packages/transformers/commands/serving.py:424: # TODO\n- .venv/lib/python3.12/site-packages/transformers/commands/serving.py:684: # TO",
      "file": ".codex/change_log-large.md",
      "line": 7762,
      "match": "\\bTODO\\b"
    },
    {
      "context": "DO\n- .venv/lib/python3.12/site-packages/transformers/commands/serving.py:684: # TODO: add other fields\n- .venv/lib/python3.12/site-packages/transformers/commands/se",
      "file": ".codex/change_log-large.md",
      "line": 7763,
      "match": "\\bTODO\\b"
    },
    {
      "context": "ds\n- .venv/lib/python3.12/site-packages/transformers/commands/serving.py:789: # TODO (Joao, Lysandre): the logits processors should be fixed in continuous batching\n",
      "file": ".codex/change_log-large.md",
      "line": 7764,
      "match": "\\bTODO\\b"
    },
    {
      "context": "ng\n- .venv/lib/python3.12/site-packages/transformers/commands/serving.py:794: # TODO (Joao, Lysandre): this should also work with tool support\n- .venv/lib/python3.1",
      "file": ".codex/change_log-large.md",
      "line": 7765,
      "match": "\\bTODO\\b"
    },
    {
      "context": "rt\n- .venv/lib/python3.12/site-packages/transformers/commands/serving.py:930: # TODO: trigger 2 constrained generations after the tool call start token is emitted:\n",
      "file": ".codex/change_log-large.md",
      "line": 7766,
      "match": "\\bTODO\\b"
    },
    {
      "context": ":\n- .venv/lib/python3.12/site-packages/transformers/commands/serving.py:1055: # TODO: other models will likely need more elaborate processing here\n- .venv/lib/pytho",
      "file": ".codex/change_log-large.md",
      "line": 7767,
      "match": "\\bTODO\\b"
    },
    {
      "context": "e\n- .venv/lib/python3.12/site-packages/transformers/commands/serving.py:1105: # TODO -- Implement non-streaming mode\n- .venv/lib/python3.12/site-packages/transforme",
      "file": ".codex/change_log-large.md",
      "line": 7768,
      "match": "\\bTODO\\b"
    },
    {
      "context": "formers/commands/serving.py:1275: logprobs=[{\"token\": \"\", \"logprob\": 99.9}],  # TODO: add actual logprobs\n- .venv/lib/python3.12/site-packages/transformers/commands",
      "file": ".codex/change_log-large.md",
      "line": 7769,
      "match": "\\bTODO\\b"
    },
    {
      "context": "formers/commands/serving.py:1288: logprobs=[{\"token\": \"\", \"logprob\": 99.9}],  # TODO: add actual logprobs\n- .venv/lib/python3.12/site-packages/transformers/commands",
      "file": ".codex/change_log-large.md",
      "line": 7770,
      "match": "\\bTODO\\b"
    },
    {
      "context": "s\n- .venv/lib/python3.12/site-packages/transformers/commands/serving.py:1397: # TODO: implement streaming transcription (currently, it's not streaming)\n- .venv/lib/",
      "file": ".codex/change_log-large.md",
      "line": 7771,
      "match": "\\bTODO\\b"
    },
    {
      "context": "dError\n- .venv/lib/python3.12/site-packages/transformers/onnx/convert.py:142: # TODO: Check when exporting QA we provide \"is_pair=True\"\n- .venv/lib/python3.12/site-",
      "file": ".codex/change_log-large.md",
      "line": 7775,
      "match": "\\bTODO\\b"
    },
    {
      "context": "Error()\n- .venv/lib/python3.12/site-packages/transformers/onnx/config.py:519: # TODO: should we set seq_length = 1 when self.use_past = True?\n- .venv/lib/python3.12",
      "file": ".codex/change_log-large.md",
      "line": 7777,
      "match": "\\bTODO\\b"
    },
    {
      "context": "= True?\n- .venv/lib/python3.12/site-packages/transformers/onnx/config.py:705: # TODO: test this.\n- .venv/lib/python3.12/site-packages/transformers/onnx/utils.py:94:",
      "file": ".codex/change_log-large.md",
      "line": 7778,
      "match": "\\bTODO\\b"
    },
    {
      "context": "v/lib/python3.12/site-packages/transformers/pipelines/mask_generation.py:224: # TODO: Identifying the model by the type of its returned embeddings is brittle.\n- .ve",
      "file": ".codex/change_log-large.md",
      "line": 7801,
      "match": "\\bTODO\\b"
    },
    {
      "context": "3.12/site-packages/transformers/pipelines/document_question_answering.py:104: # TODO: Update task_summary docs to include an example with document QA and then updat",
      "file": ".codex/change_log-large.md",
      "line": 7818,
      "match": "\\bTODO\\b"
    },
    {
      "context": "3.12/site-packages/transformers/pipelines/document_question_answering.py:409: # TODO: check why slower `LayoutLMTokenizer` and `LayoutLMv2Tokenizer` don't have this",
      "file": ".codex/change_log-large.md",
      "line": 7821,
      "match": "\\bTODO\\b"
    },
    {
      "context": "3.12/site-packages/transformers/pipelines/document_question_answering.py:497: # TODO: A lot of this logic is specific to Donut and should probably be handled in the",
      "file": ".codex/change_log-large.md",
      "line": 7823,
      "match": "\\bTODO\\b"
    },
    {
      "context": "ed.\n- .venv/lib/python3.12/site-packages/transformers/pipelines/base.py:1080: # TODO (joao): no PT model should reach this line. However, some audio models with com",
      "file": ".codex/change_log-large.md",
      "line": 7857,
      "match": "\\bTODO\\b"
    },
    {
      "context": "d\")\n- .venv/lib/python3.12/site-packages/transformers/pipelines/base.py:1397: # TODO hack by collating feature_extractor and image_processor\n- .venv/lib/python3.12/",
      "file": ".codex/change_log-large.md",
      "line": 7863,
      "match": "\\bTODO\\b"
    },
    {
      "context": "sor\n- .venv/lib/python3.12/site-packages/transformers/pipelines/base.py:1440: # TODO make the get_iterator work also for `tf` (and `flax`).\n- .venv/lib/python3.12/s",
      "file": ".codex/change_log-large.md",
      "line": 7864,
      "match": "\\bTODO\\b"
    },
    {
      "context": "`).\n- .venv/lib/python3.12/site-packages/transformers/pipelines/base.py:1515: # TODO hack by collating feature_extractor and image_processor\n- .venv/lib/python3.12/",
      "file": ".codex/change_log-large.md",
      "line": 7865,
      "match": "\\bTODO\\b"
    },
    {
      "context": "3.12/site-packages/transformers/pipelines/automatic_speech_recognition.py:88: # TODO  Use a faster algorithm this can probably be done in O(n)\n- .venv/lib/python3.1",
      "file": ".codex/change_log-large.md",
      "line": 7876,
      "match": "\\bTODO\\b"
    },
    {
      "context": "b/python3.12/site-packages/transformers/pipelines/text_classification.py:169: # TODO try and retrieve it in a nicer way from _sanitize_parameters.\n- .venv/lib/pytho",
      "file": ".codex/change_log-large.md",
      "line": 7916,
      "match": "\\bTODO\\b"
    },
    {
      "context": "/python3.12/site-packages/transformers/distributed/configuration_utils.py:29: # TODO: add tp_plan, pp_plan, device_mesh etc..\n- .venv/lib/python3.12/site-packages/t",
      "file": ".codex/change_log-large.md",
      "line": 7920,
      "match": "\\bTODO\\b"
    },
    {
      "context": "v/lib/python3.12/site-packages/transformers/sagemaker/training_args_sm.py:29: # TODO: should be moved to `utils` after refactoring of SageMakerTrainer\n- .venv/lib/p",
      "file": ".codex/change_log-large.md",
      "line": 7921,
      "match": "\\bTODO\\b"
    },
    {
      "context": "python3.12/site-packages/transformers/data/datasets/language_modeling.py:210: # TODO: randomness could apply a random seed, ex. rng = random.Random(random_seed)\n- .",
      "file": ".codex/change_log-large.md",
      "line": 7924,
      "match": "\\bTODO\\b"
    },
    {
      "context": "sformers/data/processors/squad.py:181: encoded_dict = tokenizer.encode_plus(  # TODO(thom) update this logic\n- .venv/lib/python3.12/site-packages/transformers/data/",
      "file": ".codex/change_log-large.md",
      "line": 7925,
      "match": "\\bTODO\\b"
    },
    {
      "context": " the active one\n- .venv/lib/python3.12/site-packages/accelerate/state.py:985: # TODO: Siro - remove when axolotl fixes their side\n- .venv/lib/python3.12/site-packag",
      "file": ".codex/change_log-large.md",
      "line": 7941,
      "match": "\\bTODO\\b"
    },
    {
      "context": "dlers`.\")\n- .venv/lib/python3.12/site-packages/accelerate/accelerator.py:455: # TODO: Remove after deprecating tp_plugin\n- .venv/lib/python3.12/site-packages/accele",
      "file": ".codex/change_log-large.md",
      "line": 8003,
      "match": "\\bTODO\\b"
    },
    {
      "context": "tp_plugin\n- .venv/lib/python3.12/site-packages/accelerate/accelerator.py:518: # TODO: S1ro - this is probably gonna be a problem with other fp8 backends too\n- .venv",
      "file": ".codex/change_log-large.md",
      "line": 8004,
      "match": "\\bTODO\\b"
    },
    {
      "context": "tedError(\n- .venv/lib/python3.12/site-packages/accelerate/accelerator.py:781: # TODO: S1ro - this is a temporary solution until we figure out why `save_safe_file` i",
      "file": ".codex/change_log-large.md",
      "line": 8008,
      "match": "\\bTODO\\b"
    },
    {
      "context": "assed).\"\n- .venv/lib/python3.12/site-packages/accelerate/accelerator.py:1465: # TODO: Look at enabling native TP training directly with a proper config\n- .venv/lib/",
      "file": ".codex/change_log-large.md",
      "line": 8025,
      "match": "\\bTODO\\b"
    },
    {
      "context": "hedulers\n- .venv/lib/python3.12/site-packages/accelerate/accelerator.py:1733: # TODO: Look at enabling native TP training directly with a proper config\n- .venv/lib/",
      "file": ".codex/change_log-large.md",
      "line": 8035,
      "match": "\\bTODO\\b"
    },
    {
      "context": "82: pass\n- .venv/lib/python3.12/site-packages/accelerate/accelerator.py:1809: # TODO: Look at enabling native TP training directly with a proper config\n- .venv/lib/",
      "file": ".codex/change_log-large.md",
      "line": 8037,
      "match": "\\bTODO\\b"
    },
    {
      "context": "epare`].\n- .venv/lib/python3.12/site-packages/accelerate/accelerator.py:2821: # TODO: this unscales all optimizers where we should only unscale the one where parame",
      "file": ".codex/change_log-large.md",
      "line": 8053,
      "match": "\\bTODO\\b"
    },
    {
      "context": " This is\n- .venv/lib/python3.12/site-packages/accelerate/accelerator.py:4068: # TODO: should the `yield` be in a try/finally block?\n- .venv/lib/python3.12/site-pack",
      "file": ".codex/change_log-large.md",
      "line": 8065,
      "match": "\\bTODO\\b"
    },
    {
      "context": "6: pass\n- .venv/lib/python3.12/site-packages/accelerate/utils/imports.py:497: # TODO: Remove this function once stateful_dataloader is a stable feature in torchdata",
      "file": ".codex/change_log-large.md",
      "line": 8109,
      "match": "\\bTODO\\b"
    },
    {
      "context": "chdata.\n- .venv/lib/python3.12/site-packages/accelerate/utils/imports.py:522: # TODO: Rework this into `utils.deepspeed` and migrate the \"core\" chunks into `acceler",
      "file": ".codex/change_log-large.md",
      "line": 8110,
      "match": "\\bTODO\\b"
    },
    {
      "context": "load\n- .venv/lib/python3.12/site-packages/accelerate/utils/fsdp_utils.py:706: # TODO(siro1): Add a warning for each parameter that was upcasted\n- .venv/lib/python3.",
      "file": ".codex/change_log-large.md",
      "line": 8118,
      "match": "\\bTODO\\b"
    },
    {
      "context": "er`.\"\n- .venv/lib/python3.12/site-packages/accelerate/utils/modeling.py:1993: # TODO: group all errors and raise at the end.\n- .venv/lib/python3.12/site-packages/ac",
      "file": ".codex/change_log-large.md",
      "line": 8137,
      "match": "\\bTODO\\b"
    },
    {
      "context": "he\n- .venv/lib/python3.12/site-packages/accelerate/utils/dataclasses.py:2068: # TODO(s1ro1): `cast_forward_inputs` for FSDP2?\n- .venv/lib/python3.12/site-packages/a",
      "file": ".codex/change_log-large.md",
      "line": 8170,
      "match": "\\bTODO\\b"
    },
    {
      "context": "lib/python3.12/site-packages/accelerate/test_utils/scripts/test_script.py:49: # TODO: remove RegressionModel4XPU once ccl support empty buffer in broadcasting.\n- .v",
      "file": ".codex/change_log-large.md",
      "line": 8258,
      "match": "\\bTODO\\b"
    },
    {
      "context": "ite-packages/accelerate/test_utils/scripts/test_distributed_data_loop.py:256: # TODO: Maybe this should be implemented as __eq__ for AcceleratorState?\n- .venv/lib/p",
      "file": ".codex/change_log-large.md",
      "line": 8265,
      "match": "\\bTODO\\b"
    },
    {
      "context": "packages/accelerate/test_utils/scripts/external_deps/test_performance.py:236: # TODO: skip saving of the model test for TP until the feature lands\n- .venv/lib/pytho",
      "file": ".codex/change_log-large.md",
      "line": 8267,
      "match": "\\bTODO\\b"
    },
    {
      "context": "venv/lib/python3.12/site-packages/sympy/utilities/tests/test_pickling.py:382: # TODO: fix pickling of Options class (see GroebnerBasis._options)\n- .venv/lib/python3",
      "file": ".codex/change_log-large.md",
      "line": 8358,
      "match": "\\bTODO\\b"
    },
    {
      "context": "lities/tests/test_pickling.py:409: check(c, exclude=[0, 1], check_attr=False) # TODO: Py3k\n- .venv/lib/python3.12/site-packages/sympy/utilities/tests/test_pickling.",
      "file": ".codex/change_log-large.md",
      "line": 8359,
      "match": "\\bTODO\\b"
    },
    {
      "context": "venv/lib/python3.12/site-packages/sympy/utilities/tests/test_pickling.py:420: # TODO: AssertionError: assert id(obj) not in self.memo\n- .venv/lib/python3.12/site-pa",
      "file": ".codex/change_log-large.md",
      "line": 8361,
      "match": "\\bTODO\\b"
    },
    {
      "context": "venv/lib/python3.12/site-packages/sympy/utilities/tests/test_pickling.py:424: # TODO: AssertionError: assert id(obj) not in self.memo\n- .venv/lib/python3.12/site-pa",
      "file": ".codex/change_log-large.md",
      "line": 8362,
      "match": "\\bTODO\\b"
    },
    {
      "context": "venv/lib/python3.12/site-packages/sympy/utilities/tests/test_pickling.py:438: # TODO: fix pickling of ModularInteger\n- .venv/lib/python3.12/site-packages/sympy/util",
      "file": ".codex/change_log-large.md",
      "line": 8363,
      "match": "\\bTODO\\b"
    },
    {
      "context": "venv/lib/python3.12/site-packages/sympy/utilities/tests/test_pickling.py:444: # TODO: fix pickling of RealElement\n- .venv/lib/python3.12/site-packages/sympy/utiliti",
      "file": ".codex/change_log-large.md",
      "line": 8364,
      "match": "\\bTODO\\b"
    },
    {
      "context": "venv/lib/python3.12/site-packages/sympy/utilities/tests/test_pickling.py:448: # TODO: fix pickling of ComplexElement\n- .venv/lib/python3.12/site-packages/sympy/util",
      "file": ".codex/change_log-large.md",
      "line": 8365,
      "match": "\\bTODO\\b"
    },
    {
      "context": "venv/lib/python3.12/site-packages/sympy/utilities/tests/test_pickling.py:457: # TODO: fix pickling of ModularInteger\n- .venv/lib/python3.12/site-packages/sympy/util",
      "file": ".codex/change_log-large.md",
      "line": 8366,
      "match": "\\bTODO\\b"
    },
    {
      "context": "venv/lib/python3.12/site-packages/sympy/utilities/tests/test_pickling.py:472: # TODO: fix pickling of ModularInteger\n- .venv/lib/python3.12/site-packages/sympy/util",
      "file": ".codex/change_log-large.md",
      "line": 8367,
      "match": "\\bTODO\\b"
    },
    {
      "context": "venv/lib/python3.12/site-packages/sympy/utilities/tests/test_pickling.py:489: # TODO: fix pickling of RealElement\n- .venv/lib/python3.12/site-packages/sympy/utiliti",
      "file": ".codex/change_log-large.md",
      "line": 8368,
      "match": "\\bTODO\\b"
    },
    {
      "context": "venv/lib/python3.12/site-packages/sympy/utilities/tests/test_pickling.py:493: # TODO: fix pickling of ComplexElement\n- .venv/lib/python3.12/site-packages/sympy/util",
      "file": ".codex/change_log-large.md",
      "line": 8369,
      "match": "\\bTODO\\b"
    },
    {
      "context": "venv/lib/python3.12/site-packages/sympy/utilities/tests/test_pickling.py:500: # TODO: AssertionError\n- .venv/lib/python3.12/site-packages/sympy/utilities/tests/test",
      "file": ".codex/change_log-large.md",
      "line": 8370,
      "match": "\\bTODO\\b"
    },
    {
      "context": "venv/lib/python3.12/site-packages/sympy/utilities/tests/test_pickling.py:504: # TODO: AttributeError: 'PolyElement' object has no attribute 'ring'\n- .venv/lib/pytho",
      "file": ".codex/change_log-large.md",
      "line": 8371,
      "match": "\\bTODO\\b"
    },
    {
      "context": "venv/lib/python3.12/site-packages/sympy/utilities/tests/test_pickling.py:526: # TODO: Argh, Python is so naive. No lambdas nor inner function support in\n- .venv/lib",
      "file": ".codex/change_log-large.md",
      "line": 8372,
      "match": "\\bTODO\\b"
    },
    {
      "context": "venv/lib/python3.12/site-packages/sympy/utilities/tests/test_pickling.py:559: # TODO: TypeError: __init__() takes at least 3 arguments (1 given)\n- .venv/lib/python3",
      "file": ".codex/change_log-large.md",
      "line": 8373,
      "match": "\\bTODO\\b"
    },
    {
      "context": "venv/lib/python3.12/site-packages/sympy/utilities/tests/test_pickling.py:563: # TODO: TypeError: can't pickle instancemethod objects\n- .venv/lib/python3.12/site-pac",
      "file": ".codex/change_log-large.md",
      "line": 8374,
      "match": "\\bTODO\\b"
    },
    {
      "context": "venv/lib/python3.12/site-packages/sympy/utilities/tests/test_pickling.py:612: # TODO: PicklingError: Can't pickle <function <lambda> at 0x38578c0>: it's not found a",
      "file": ".codex/change_log-large.md",
      "line": 8375,
      "match": "\\bTODO\\b"
    },
    {
      "context": "venv/lib/python3.12/site-packages/sympy/utilities/tests/test_pickling.py:622: # TODO: TypeError: __init__() takes at least 3 arguments (1 given)\n- .venv/lib/python3",
      "file": ".codex/change_log-large.md",
      "line": 8376,
      "match": "\\bTODO\\b"
    },
    {
      "context": "venv/lib/python3.12/site-packages/sympy/utilities/tests/test_pickling.py:635: # TODO: fix pickling of `symbols' flag\n- .venv/lib/python3.12/site-packages/sympy/util",
      "file": ".codex/change_log-large.md",
      "line": 8377,
      "match": "\\bTODO\\b"
    },
    {
      "context": "venv/lib/python3.12/site-packages/sympy/utilities/tests/test_pickling.py:639: # TODO: def test_pickling_polys_rootisolation():\n- .venv/lib/python3.12/site-packages/",
      "file": ".codex/change_log-large.md",
      "line": 8378,
      "match": "\\bTODO\\b"
    },
    {
      "context": " .venv/lib/python3.12/site-packages/sympy/utilities/tests/test_wester.py:920: # TODO: Replace solve with solveset, as of now test fails for solveset\n- .venv/lib/pyt",
      "file": ".codex/change_log-large.md",
      "line": 8402,
      "match": "\\bTODO\\b"
    },
    {
      "context": " .venv/lib/python3.12/site-packages/sympy/utilities/tests/test_wester.py:956: # TODO: Replace solve with solveset when it gives Lambert solution\n- .venv/lib/python3",
      "file": ".codex/change_log-large.md",
      "line": 8404,
      "match": "\\bTODO\\b"
    },
    {
      "context": " .venv/lib/python3.12/site-packages/sympy/utilities/tests/test_wester.py:966: # TODO: x = [-1, 2*(+/-asinh(1)*I + n*pi}, 3*(pi/6 + n*pi/3)]\n- .venv/lib/python3.12/s",
      "file": ".codex/change_log-large.md",
      "line": 8405,
      "match": "\\bTODO\\b"
    },
    {
      "context": " .venv/lib/python3.12/site-packages/sympy/utilities/tests/test_wester.py:967: # TODO: Replace solve with solveset, as of now test fails for solveset\n- .venv/lib/pyt",
      "file": ".codex/change_log-large.md",
      "line": 8406,
      "match": "\\bTODO\\b"
    },
    {
      "context": ".venv/lib/python3.12/site-packages/sympy/utilities/tests/test_wester.py:1014: # TODO: Replace solve with solveset, as of now test fails for solveset\n- .venv/lib/pyt",
      "file": ".codex/change_log-large.md",
      "line": 8407,
      "match": "\\bTODO\\b"
    },
    {
      "context": ".venv/lib/python3.12/site-packages/sympy/utilities/tests/test_wester.py:1032: # TODO: Replace solve with solveset, as of now test fails for solveset\n- .venv/lib/pyt",
      "file": ".codex/change_log-large.md",
      "line": 8408,
      "match": "\\bTODO\\b"
    },
    {
      "context": ".venv/lib/python3.12/site-packages/sympy/utilities/tests/test_wester.py:1038: # TODO: Replace solve with solveset, as of now test fails for solveset\n- .venv/lib/pyt",
      "file": ".codex/change_log-large.md",
      "line": 8409,
      "match": "\\bTODO\\b"
    },
    {
      "context": ".venv/lib/python3.12/site-packages/sympy/utilities/tests/test_wester.py:1047: # TODO: Replace solve with solveset, as of now test fails for solveset\n- .venv/lib/pyt",
      "file": ".codex/change_log-large.md",
      "line": 8410,
      "match": "\\bTODO\\b"
    },
    {
      "context": ".venv/lib/python3.12/site-packages/sympy/utilities/tests/test_wester.py:1052: # TODO: Replace solve with solveset, as of now test fails for solveset\n- .venv/lib/pyt",
      "file": ".codex/change_log-large.md",
      "line": 8411,
      "match": "\\bTODO\\b"
    },
    {
      "context": ".venv/lib/python3.12/site-packages/sympy/utilities/tests/test_wester.py:1059: # TODO: Replace solve with solveset which gives both [+/- current answer]\n- .venv/lib/",
      "file": ".codex/change_log-large.md",
      "line": 8412,
      "match": "\\bTODO\\b"
    },
    {
      "context": ".venv/lib/python3.12/site-packages/sympy/utilities/tests/test_wester.py:1077: # TODO: Replace solve with solveset, as of now\n- .venv/lib/python3.12/site-packages/sy",
      "file": ".codex/change_log-large.md",
      "line": 8413,
      "match": "\\bTODO\\b"
    },
    {
      "context": ".venv/lib/python3.12/site-packages/sympy/utilities/tests/test_wester.py:1084: # TODO: Replace solve with solveset, as of now\n- .venv/lib/python3.12/site-packages/sy",
      "file": ".codex/change_log-large.md",
      "line": 8414,
      "match": "\\bTODO\\b"
    },
    {
      "context": ".venv/lib/python3.12/site-packages/sympy/utilities/tests/test_wester.py:1092: # TODO: Replace solve with solveset, as of now\n- .venv/lib/python3.12/site-packages/sy",
      "file": ".codex/change_log-large.md",
      "line": 8415,
      "match": "\\bTODO\\b"
    },
    {
      "context": ".venv/lib/python3.12/site-packages/sympy/utilities/tests/test_wester.py:1099: # TODO: Replace solve with solveset, as of now\n- .venv/lib/python3.12/site-packages/sy",
      "file": ".codex/change_log-large.md",
      "line": 8416,
      "match": "\\bTODO\\b"
    },
    {
      "context": ".venv/lib/python3.12/site-packages/sympy/utilities/tests/test_wester.py:1118: # TODO: Replace solve with solveset, as of now\n- .venv/lib/python3.12/site-packages/sy",
      "file": ".codex/change_log-large.md",
      "line": 8417,
      "match": "\\bTODO\\b"
    },
    {
      "context": ".venv/lib/python3.12/site-packages/sympy/utilities/tests/test_wester.py:1194: # TODO: Replace solve with solveset, as of now\n- .venv/lib/python3.12/site-packages/sy",
      "file": ".codex/change_log-large.md",
      "line": 8418,
      "match": "\\bTODO\\b"
    },
    {
      "context": ".venv/lib/python3.12/site-packages/sympy/utilities/tests/test_wester.py:2252: # TODO: Replace solve with solveset, current test fails for solveset\n- .venv/lib/pytho",
      "file": ".codex/change_log-large.md",
      "line": 8432,
      "match": "\\bTODO\\b"
    },
    {
      "context": ".venv/lib/python3.12/site-packages/sympy/utilities/tests/test_wester.py:3082: # TODO: Replace solve with solveset, when it works for solveset\n- .venv/lib/python3.12",
      "file": ".codex/change_log-large.md",
      "line": 8447,
      "match": "\\bTODO\\b"
    },
    {
      "context": " .venv/lib/python3.12/site-packages/sympy/utilities/_compilation/__init__.py:9: TODO:\n- .venv/lib/python3.12/site-packages/sympy/utilities/_compilation/util.py:22: ",
      "file": ".codex/change_log-large.md",
      "line": 8473,
      "match": "\\bTODO\\b"
    },
    {
      "context": "15: pass\n- .venv/lib/python3.12/site-packages/sympy/testing/runtests.py:1936: # TODO parse integers as well ?\n- .venv/lib/python3.12/site-packages/sympy/testing/run",
      "file": ".codex/change_log-large.md",
      "line": 8521,
      "match": "\\bTODO\\b"
    },
    {
      "context": "ssed = 0\n- .venv/lib/python3.12/site-packages/sympy/testing/runtests.py:2023: # TODO: Should these be protected?\n- .venv/lib/python3.12/site-packages/sympy/testing/",
      "file": ".codex/change_log-large.md",
      "line": 8525,
      "match": "\\bTODO\\b"
    },
    {
      "context": "50: pass\n- .venv/lib/python3.12/site-packages/sympy/assumptions/refine.py:53: # TODO: this will probably not work with Integral or Polynomial\n- .venv/lib/python3.12",
      "file": ".codex/change_log-large.md",
      "line": 8550,
      "match": "\\bTODO\\b"
    },
    {
      "context": "s well.\n- .venv/lib/python3.12/site-packages/sympy/assumptions/satask.py:103: # TODO: Run additional checks to see which combination of the\n- .venv/lib/python3.12/s",
      "file": ".codex/change_log-large.md",
      "line": 8553,
      "match": "\\bTODO\\b"
    },
    {
      "context": "- .venv/lib/python3.12/site-packages/sympy/assumptions/handlers/order.py:218: # TODO: This should be deducible from the nonzero handler\n- .venv/lib/python3.12/site-",
      "file": ".codex/change_log-large.md",
      "line": 8600,
      "match": "\\bTODO\\b"
    },
    {
      "context": ".venv/lib/python3.12/site-packages/sympy/assumptions/handlers/matrices.py:47: # TODO: implement sathandlers system for the matrices.\n- .venv/lib/python3.12/site-pac",
      "file": ".codex/change_log-large.md",
      "line": 8603,
      "match": "\\bTODO\\b"
    },
    {
      "context": ".venv/lib/python3.12/site-packages/sympy/assumptions/handlers/matrices.py:77: # TODO: implement sathandlers system for the matrices.\n- .venv/lib/python3.12/site-pac",
      "file": ".codex/change_log-large.md",
      "line": 8604,
      "match": "\\bTODO\\b"
    },
    {
      "context": ".venv/lib/python3.12/site-packages/sympy/assumptions/handlers/matrices.py:94: # TODO: implement sathandlers system for the matrices.\n- .venv/lib/python3.12/site-pac",
      "file": ".codex/change_log-large.md",
      "line": 8605,
      "match": "\\bTODO\\b"
    },
    {
      "context": " .venv/lib/python3.12/site-packages/sympy/assumptions/predicates/sets.py:238: # TODO: Add examples\n- .venv/lib/python3.12/site-packages/sympy/assumptions/predicates",
      "file": ".codex/change_log-large.md",
      "line": 8606,
      "match": "\\bTODO\\b"
    },
    {
      "context": " .venv/lib/python3.12/site-packages/sympy/assumptions/predicates/sets.py:337: # TODO: Add examples\n- .venv/lib/python3.12/site-packages/sympy/assumptions/predicates",
      "file": ".codex/change_log-large.md",
      "line": 8607,
      "match": "\\bTODO\\b"
    },
    {
      "context": " .venv/lib/python3.12/site-packages/sympy/assumptions/predicates/sets.py:394: # TODO: Add examples\n- .venv/lib/python3.12/site-packages/sympy/assumptions/predicates",
      "file": ".codex/change_log-large.md",
      "line": 8608,
      "match": "\\bTODO\\b"
    },
    {
      "context": ".venv/lib/python3.12/site-packages/sympy/assumptions/predicates/common.py:17: # TODO: Add examples\n- .venv/lib/python3.12/site-packages/sympy/assumptions/predicates",
      "file": ".codex/change_log-large.md",
      "line": 8609,
      "match": "\\bTODO\\b"
    },
    {
      "context": "env/lib/python3.12/site-packages/sympy/assumptions/predicates/calculus.py:57: # TODO: Add examples\n- .venv/lib/python3.12/site-packages/sympy/assumptions/predicates",
      "file": ".codex/change_log-large.md",
      "line": 8610,
      "match": "\\bTODO\\b"
    },
    {
      "context": "env/lib/python3.12/site-packages/sympy/assumptions/predicates/matrices.py:70: # TODO: Add handlers to make these keys work with\n- .venv/lib/python3.12/site-packages",
      "file": ".codex/change_log-large.md",
      "line": 8611,
      "match": "\\bTODO\\b"
    },
    {
      "context": "ave\n- .venv/lib/python3.12/site-packages/sympy/physics/optics/gaussopt.py:526: #TODO A class Complex may be implemented. The BeamParameter may\n- .venv/lib/python3.1",
      "file": ".codex/change_log-large.md",
      "line": 8708,
      "match": "\\bTODO\\b"
    },
    {
      "context": "may\n- .venv/lib/python3.12/site-packages/sympy/physics/optics/gaussopt.py:885: #TODO add the other possible arguments\n- .venv/lib/python3.12/site-packages/sympy/phy",
      "file": ".codex/change_log-large.md",
      "line": 8709,
      "match": "\\bTODO\\b"
    },
    {
      "context": "'''\n- .venv/lib/python3.12/site-packages/sympy/physics/optics/gaussopt.py:906: #TODO\n- .venv/lib/python3.12/site-packages/sympy/physics/optics/gaussopt.py:909: #   ",
      "file": ".codex/change_log-large.md",
      "line": 8712,
      "match": "\\bTODO\\b"
    },
    {
      "context": "ass\n- .venv/lib/python3.12/site-packages/sympy/physics/optics/gaussopt.py:911: #TODO\n- .venv/lib/python3.12/site-packages/sympy/physics/optics/gaussopt.py:923: #   ",
      "file": ".codex/change_log-large.md",
      "line": 8714,
      "match": "\\bTODO\\b"
    },
    {
      "context": "s\n- .venv/lib/python3.12/site-packages/sympy/physics/units/dimensions.py:351: # TODO: should this raise a warning?\n- .venv/lib/python3.12/site-packages/sympy/physic",
      "file": ".codex/change_log-large.md",
      "line": 8720,
      "match": "\\bTODO\\b"
    },
    {
      "context": "g?\n- .venv/lib/python3.12/site-packages/sympy/physics/units/dimensions.py:522: #TODO: the inversion will fail if the system is inconsistent, for\n- .venv/lib/python3",
      "file": ".codex/change_log-large.md",
      "line": 8721,
      "match": "\\bTODO\\b"
    },
    {
      "context": "ib/python3.12/site-packages/sympy/physics/units/tests/test_quantities.py:206: # TODO: decide whether to allow such expression in the future\n- .venv/lib/python3.12/s",
      "file": ".codex/change_log-large.md",
      "line": 8722,
      "match": "\\bTODO\\b"
    },
    {
      "context": "ib/python3.12/site-packages/sympy/physics/units/tests/test_quantities.py:227: # TODO: Pow only support structural equality:\n- .venv/lib/python3.12/site-packages/sym",
      "file": ".codex/change_log-large.md",
      "line": 8723,
      "match": "\\bTODO\\b"
    },
    {
      "context": "ib/python3.12/site-packages/sympy/physics/units/tests/test_quantities.py:244: # TODO: need better simplification routine:\n- .venv/lib/python3.12/site-packages/sympy",
      "file": ".codex/change_log-large.md",
      "line": 8724,
      "match": "\\bTODO\\b"
    },
    {
      "context": "ib/python3.12/site-packages/sympy/physics/units/tests/test_quantities.py:249: # TODO: need a better way to simplify expressions containing units:\n- .venv/lib/python",
      "file": ".codex/change_log-large.md",
      "line": 8725,
      "match": "\\bTODO\\b"
    },
    {
      "context": "ib/python3.12/site-packages/sympy/physics/units/tests/test_quantities.py:253: # TODO: fix this, it should give `m` without `Abs`\n- .venv/lib/python3.12/site-package",
      "file": ".codex/change_log-large.md",
      "line": 8726,
      "match": "\\bTODO\\b"
    },
    {
      "context": ".\")\n- .venv/lib/python3.12/site-packages/sympy/physics/mechanics/kane.py:630: # TODO : Remove `new_method` after 1.1 has been released.\n- .venv/lib/python3.12/site-",
      "file": ".codex/change_log-large.md",
      "line": 8796,
      "match": "\\bTODO\\b"
    },
    {
      "context": "b/python3.12/site-packages/sympy/physics/mechanics/tests/test_particle.py:55: # TODO make the result not be system-dependent\n- .venv/lib/python3.12/site-packages/sy",
      "file": ".codex/change_log-large.md",
      "line": 8838,
      "match": "\\bTODO\\b"
    },
    {
      "context": " .venv/lib/python3.12/site-packages/sympy/physics/quantum/matrixutils.py:141: # TODO: Move this into sympy.matrices.\n- .venv/lib/python3.12/site-packages/sympy/phys",
      "file": ".codex/change_log-large.md",
      "line": 8954,
      "match": "\\bTODO\\b"
    },
    {
      "context": "rror(\n- .venv/lib/python3.12/site-packages/sympy/physics/quantum/gate.py:242: # TODO: This can be optimized to reduce the number of Qubit\n- .venv/lib/python3.12/sit",
      "file": ".codex/change_log-large.md",
      "line": 8958,
      "match": "\\bTODO\\b"
    },
    {
      "context": "rror:\n- .venv/lib/python3.12/site-packages/sympy/physics/quantum/operator.py:3: TODO:\n- .venv/lib/python3.12/site-packages/sympy/physics/quantum/operator.py:183: ra",
      "file": ".codex/change_log-large.md",
      "line": 8989,
      "match": "\\bTODO\\b"
    },
    {
      "context": "+\n- .venv/lib/python3.12/site-packages/sympy/physics/quantum/operator.py:428: # TODO: make sure the hilbert spaces of the bra and ket are\n- .venv/lib/python3.12/sit",
      "file": ".codex/change_log-large.md",
      "line": 8993,
      "match": "\\bTODO\\b"
    },
    {
      "context": "e\n- .venv/lib/python3.12/site-packages/sympy/physics/quantum/operator.py:491: # TODO if operands are tensorproducts this may be will be handled\n- .venv/lib/python3.",
      "file": ".codex/change_log-large.md",
      "line": 8994,
      "match": "\\bTODO\\b"
    },
    {
      "context": "- .venv/lib/python3.12/site-packages/sympy/physics/quantum/matrixcache.py:78: # TODO: explore different sparse formats. But sparse.kron will use\n- .venv/lib/python3",
      "file": ".codex/change_log-large.md",
      "line": 9000,
      "match": "\\bTODO\\b"
    },
    {
      "context": "n will use\n- .venv/lib/python3.12/site-packages/sympy/physics/quantum/cg.py:1: #TODO:\n- .venv/lib/python3.12/site-packages/sympy/physics/quantum/cg.py:486: #TODO: I",
      "file": ".codex/change_log-large.md",
      "line": 9001,
      "match": "\\bTODO\\b"
    },
    {
      "context": ": #TODO:\n- .venv/lib/python3.12/site-packages/sympy/physics/quantum/cg.py:486: #TODO: Improve simplification method\n- .venv/lib/python3.12/site-packages/sympy/physi",
      "file": ".codex/change_log-large.md",
      "line": 9002,
      "match": "\\bTODO\\b"
    },
    {
      "context": "cg_part\n- .venv/lib/python3.12/site-packages/sympy/physics/quantum/cg.py:675: # TODO: Check for symmetries\n- .venv/lib/python3.12/site-packages/sympy/physics/quantu",
      "file": ".codex/change_log-large.md",
      "line": 9004,
      "match": "\\bTODO\\b"
    },
    {
      "context": "Pow')\n- .venv/lib/python3.12/site-packages/sympy/physics/quantum/density.py:21: TODO: Density operator support for Qubits\n- .venv/lib/python3.12/site-packages/sympy",
      "file": ".codex/change_log-large.md",
      "line": 9006,
      "match": "\\bTODO\\b"
    },
    {
      "context": "dError(\n- .venv/lib/python3.12/site-packages/sympy/physics/quantum/trace.py:35: TODO: Handle condition such as symbols have subscripts/superscripts\n- .venv/lib/pyth",
      "file": ".codex/change_log-large.md",
      "line": 9017,
      "match": "\\bTODO\\b"
    },
    {
      "context": "ripts\n- .venv/lib/python3.12/site-packages/sympy/physics/quantum/trace.py:94: # TODO: Need to handle printing\n- .venv/lib/python3.12/site-packages/sympy/physics/qua",
      "file": ".codex/change_log-large.md",
      "line": 9018,
      "match": "\\bTODO\\b"
    },
    {
      "context": "nting\n- .venv/lib/python3.12/site-packages/sympy/physics/quantum/trace.py:172: #TODO: Current version ignores the indices set for partial trace.\n- .venv/lib/python3",
      "file": ".codex/change_log-large.md",
      "line": 9019,
      "match": "\\bTODO\\b"
    },
    {
      "context": "ace.\n- .venv/lib/python3.12/site-packages/sympy/physics/quantum/trace.py:189: # TODO : improve this implementation\n- .venv/lib/python3.12/site-packages/sympy/physic",
      "file": ".codex/change_log-large.md",
      "line": 9020,
      "match": "\\bTODO\\b"
    },
    {
      "context": "ation\n- .venv/lib/python3.12/site-packages/sympy/physics/quantum/trace.py:192: #TODO: Review if the permute method is needed\n- .venv/lib/python3.12/site-packages/sy",
      "file": ".codex/change_log-large.md",
      "line": 9021,
      "match": "\\bTODO\\b"
    },
    {
      "context": "eded\n- .venv/lib/python3.12/site-packages/sympy/physics/quantum/cartesian.py:3: TODO:\n- .venv/lib/python3.12/site-packages/sympy/physics/quantum/represent.py:3: TOD",
      "file": ".codex/change_log-large.md",
      "line": 9022,
      "match": "\\bTODO\\b"
    },
    {
      "context": "ODO:\n- .venv/lib/python3.12/site-packages/sympy/physics/quantum/represent.py:3: TODO:\n- .venv/lib/python3.12/site-packages/sympy/physics/quantum/represent.py:91: Ke",
      "file": ".codex/change_log-large.md",
      "line": 9023,
      "match": "\\bTODO\\b"
    },
    {
      "context": ".\n- .venv/lib/python3.12/site-packages/sympy/physics/quantum/represent.py:417: #TODO: Add support for sets of operators\n- .venv/lib/python3.12/site-packages/sympy/p",
      "file": ".codex/change_log-large.md",
      "line": 9038,
      "match": "\\bTODO\\b"
    },
    {
      "context": "s.\n- .venv/lib/python3.12/site-packages/sympy/physics/quantum/represent.py:451: TODO (?): Support for Muls and other types of expressions?\n- .venv/lib/python3.12/si",
      "file": ".codex/change_log-large.md",
      "line": 9040,
      "match": "\\bTODO\\b"
    },
    {
      "context": "29: pass\n- .venv/lib/python3.12/site-packages/sympy/physics/quantum/shor.py:36: TODO: implement a decompose property that returns how to do this in terms\n- .venv/li",
      "file": ".codex/change_log-large.md",
      "line": 9048,
      "match": "\\bTODO\\b"
    },
    {
      "context": "+\n- .venv/lib/python3.12/site-packages/sympy/physics/quantum/operatorset.py:13: TODO List:\n- .venv/lib/python3.12/site-packages/sympy/physics/quantum/operatorset.py",
      "file": ".codex/change_log-large.md",
      "line": 9056,
      "match": "\\bTODO\\b"
    },
    {
      "context": "venv/lib/python3.12/site-packages/sympy/physics/quantum/tensorproduct.py:151: # TODO: disallow nested TensorProducts.\n- .venv/lib/python3.12/site-packages/sympy/phy",
      "file": ".codex/change_log-large.md",
      "line": 9070,
      "match": "\\bTODO\\b"
    },
    {
      "context": "ts.\n- .venv/lib/python3.12/site-packages/sympy/physics/quantum/qapply.py:104: # TODO: don't expand the scalars in front of each Mul.\n- .venv/lib/python3.12/site-pac",
      "file": ".codex/change_log-large.md",
      "line": 9071,
      "match": "\\bTODO\\b"
    },
    {
      "context": "or:\n- .venv/lib/python3.12/site-packages/sympy/physics/quantum/qapply.py:251: # TODO: I may need to expand before returning the final result.\n- .venv/lib/python3.12",
      "file": ".codex/change_log-large.md",
      "line": 9074,
      "match": "\\bTODO\\b"
    },
    {
      "context": "Error\n- .venv/lib/python3.12/site-packages/sympy/physics/quantum/spin.py:145: # TODO: add methods for uncoupling operators\n- .venv/lib/python3.12/site-packages/symp",
      "file": ".codex/change_log-large.md",
      "line": 9078,
      "match": "\\bTODO\\b"
    },
    {
      "context": "Error\n- .venv/lib/python3.12/site-packages/sympy/physics/quantum/spin.py:157: # TODO: move this to qapply_Mul\n- .venv/lib/python3.12/site-packages/sympy/physics/qua",
      "file": ".codex/change_log-large.md",
      "line": 9080,
      "match": "\\bTODO\\b"
    },
    {
      "context": "dError\n- .venv/lib/python3.12/site-packages/sympy/physics/quantum/spin.py:165: #TODO: use options to use different j values\n- .venv/lib/python3.12/site-packages/sym",
      "file": ".codex/change_log-large.md",
      "line": 9082,
      "match": "\\bTODO\\b"
    },
    {
      "context": "inate\n- .venv/lib/python3.12/site-packages/sympy/physics/quantum/spin.py:616: # TODO: move evaluation up to represent function/implement elsewhere\n- .venv/lib/pytho",
      "file": ".codex/change_log-large.md",
      "line": 9084,
      "match": "\\bTODO\\b"
    },
    {
      "context": "here\n- .venv/lib/python3.12/site-packages/sympy/physics/quantum/spin.py:1017: # TODO: better way to get angles of rotation\n- .venv/lib/python3.12/site-packages/symp",
      "file": ".codex/change_log-large.md",
      "line": 9085,
      "match": "\\bTODO\\b"
    },
    {
      "context": "pass\n- .venv/lib/python3.12/site-packages/sympy/physics/quantum/spin.py:1487: # TODO: Need hilbert space fix, see issue 5732\n- .venv/lib/python3.12/site-packages/sy",
      "file": ".codex/change_log-large.md",
      "line": 9088,
      "match": "\\bTODO\\b"
    },
    {
      "context": "/lib/python3.12/site-packages/sympy/physics/quantum/tests/test_density.py:269: #TODO: test for invalid arguments\n- .venv/lib/python3.12/site-packages/sympy/physics/",
      "file": ".codex/change_log-large.md",
      "line": 9100,
      "match": "\\bTODO\\b"
    },
    {
      "context": "env/lib/python3.12/site-packages/sympy/physics/quantum/tests/test_trace.py:82: #TODO: needed while testing reduced density operations, etc.\n- .venv/lib/python3.12/s",
      "file": ".codex/change_log-large.md",
      "line": 9101,
      "match": "\\bTODO\\b"
    },
    {
      "context": "nv/lib/python3.12/site-packages/sympy/physics/quantum/tests/test_printing.py:3: TODO:\n- .venv/lib/python3.12/site-packages/sympy/physics/quantum/tests/test_printing",
      "file": ".codex/change_log-large.md",
      "line": 9102,
      "match": "\\bTODO\\b"
    },
    {
      "context": "ib/python3.12/site-packages/sympy/physics/quantum/tests/test_printing.py:678: # TODO: Fix non-unicode pretty printing\n- .venv/lib/python3.12/site-packages/sympy/phy",
      "file": ".codex/change_log-large.md",
      "line": 9103,
      "match": "\\bTODO\\b"
    },
    {
      "context": "b/python3.12/site-packages/sympy/physics/quantum/tests/test_cartesian.py:113: # TODO: Add tests for representations\n- .venv/lib/python3.12/site-packages/sympy/physi",
      "file": ".codex/change_log-large.md",
      "line": 9109,
      "match": "\\bTODO\\b"
    },
    {
      "context": "pass\n- .venv/lib/python3.12/site-packages/sympy/physics/vector/vector.py:735: # TODO : Circular dependency if imported at top. Should move\n- .venv/lib/python3.12/si",
      "file": ".codex/change_log-large.md",
      "line": 9112,
      "match": "\\bTODO\\b"
    },
    {
      "context": "/lib/python3.12/site-packages/sympy/physics/vector/tests/test_printing.py:47: # TODO : The unit vectors should print with subscripts but they just\n- .venv/lib/pytho",
      "file": ".codex/change_log-large.md",
      "line": 9117,
      "match": "\\bTODO\\b"
    },
    {
      "context": "/lib/python3.12/site-packages/sympy/physics/vector/tests/test_printing.py:50: # TODO : The pretty print division does not print correctly here:\n- .venv/lib/python3.",
      "file": ".codex/change_log-large.md",
      "line": 9118,
      "match": "\\bTODO\\b"
    },
    {
      "context": "q1, q2]))\n- .venv/lib/python3.12/site-packages/sympy/diffgeom/diffgeom.py:26: # TODO you are a bit excessive in the use of Dummies\n- .venv/lib/python3.12/site-packa",
      "file": ".codex/change_log-large.md",
      "line": 9121,
      "match": "\\bTODO\\b"
    },
    {
      "context": "f Dummies\n- .venv/lib/python3.12/site-packages/sympy/diffgeom/diffgeom.py:27: # TODO dummy point, literal field\n- .venv/lib/python3.12/site-packages/sympy/diffgeom/",
      "file": ".codex/change_log-large.md",
      "line": 9122,
      "match": "\\bTODO\\b"
    },
    {
      "context": "ral field\n- .venv/lib/python3.12/site-packages/sympy/diffgeom/diffgeom.py:28: # TODO too often one needs to call doit or simplify on the output, check the\n- .venv/l",
      "file": ".codex/change_log-large.md",
      "line": 9123,
      "match": "\\bTODO\\b"
    },
    {
      "context": "n which\n- .venv/lib/python3.12/site-packages/sympy/diffgeom/diffgeom.py:1101: # TODO: you need a real dummy function for the next line\n- .venv/lib/python3.12/site-p",
      "file": ".codex/change_log-large.md",
      "line": 9131,
      "match": "\\bTODO\\b"
    },
    {
      "context": "\n- .venv/lib/python3.12/site-packages/sympy/diffgeom/diffgeom.py:1309: if c:  # TODO this is ugly - the Commutator can be Zero and\n- .venv/lib/python3.12/site-packa",
      "file": ".codex/change_log-large.md",
      "line": 9132,
      "match": "\\bTODO\\b"
    },
    {
      "context": "ero and\n- .venv/lib/python3.12/site-packages/sympy/diffgeom/diffgeom.py:1438: # TODO the calculation of signatures is slow\n- .venv/lib/python3.12/site-packages/symp",
      "file": ".codex/change_log-large.md",
      "line": 9133,
      "match": "\\bTODO\\b"
    },
    {
      "context": "is slow\n- .venv/lib/python3.12/site-packages/sympy/diffgeom/diffgeom.py:1439: # TODO you do not need all these permutations (neither the prefactor)\n- .venv/lib/pyth",
      "file": ".codex/change_log-large.md",
      "line": 9134,
      "match": "\\bTODO\\b"
    },
    {
      "context": "Error()\n- .venv/lib/python3.12/site-packages/sympy/diffgeom/diffgeom.py:1594: # TODO: you need a real dummy function for the next line\n- .venv/lib/python3.12/site-p",
      "file": ".codex/change_log-large.md",
      "line": 9136,
      "match": "\\bTODO\\b"
    },
    {
      "context": "Error()\n- .venv/lib/python3.12/site-packages/sympy/diffgeom/diffgeom.py:1892: # TODO Is this a good idea?\n- .venv/lib/python3.12/site-packages/sympy/diffgeom/diffge",
      "file": ".codex/change_log-large.md",
      "line": 9138,
      "match": "\\bTODO\\b"
    },
    {
      "context": "d idea?\n- .venv/lib/python3.12/site-packages/sympy/diffgeom/diffgeom.py:1919: # TODO move some of this to class methods.\n- .venv/lib/python3.12/site-packages/sympy/",
      "file": ".codex/change_log-large.md",
      "line": 9139,
      "match": "\\bTODO\\b"
    },
    {
      "context": "ethods.\n- .venv/lib/python3.12/site-packages/sympy/diffgeom/diffgeom.py:1920: # TODO rewrite using the .as_blah_blah methods\n- .venv/lib/python3.12/site-packages/sy",
      "file": ".codex/change_log-large.md",
      "line": 9140,
      "match": "\\bTODO\\b"
    },
    {
      "context": "methods\n- .venv/lib/python3.12/site-packages/sympy/diffgeom/diffgeom.py:1965: # TODO move some of this to class methods.\n- .venv/lib/python3.12/site-packages/sympy/",
      "file": ".codex/change_log-large.md",
      "line": 9141,
      "match": "\\bTODO\\b"
    },
    {
      "context": "ethods.\n- .venv/lib/python3.12/site-packages/sympy/diffgeom/diffgeom.py:1966: # TODO rewrite using the .as_blah_blah methods\n- .venv/lib/python3.12/site-packages/sy",
      "file": ".codex/change_log-large.md",
      "line": 9142,
      "match": "\\bTODO\\b"
    },
    {
      "context": "/lib/python3.12/site-packages/sympy/diffgeom/tests/test_class_structure.py:19: #TODO assert point.subs(x, 2) == Point(cs, [2, y])\n- .venv/lib/python3.12/site-packag",
      "file": ".codex/change_log-large.md",
      "line": 9145,
      "match": "\\bTODO\\b"
    },
    {
      "context": "/lib/python3.12/site-packages/sympy/diffgeom/tests/test_class_structure.py:20: #TODO assert point.free_symbols == set([x, y])\n- .venv/lib/python3.12/site-packages/s",
      "file": ".codex/change_log-large.md",
      "line": 9146,
      "match": "\\bTODO\\b"
    },
    {
      "context": " .venv/lib/python3.12/site-packages/sympy/diffgeom/tests/test_diffgeom.py:103: #TODO assert m == R2_r.transform(R2_p, R2_p.transform(R2_r, [a, b])).applyfunc(simpli",
      "file": ".codex/change_log-large.md",
      "line": 9148,
      "match": "\\bTODO\\b"
    },
    {
      "context": " .venv/lib/python3.12/site-packages/sympy/diffgeom/tests/test_diffgeom.py:117: #TODO assert m == R3_r.transform(R3_c, R3_c.transform(R3_r, m)).applyfunc(simplify)\n-",
      "file": ".codex/change_log-large.md",
      "line": 9149,
      "match": "\\bTODO\\b"
    },
    {
      "context": " .venv/lib/python3.12/site-packages/sympy/diffgeom/tests/test_diffgeom.py:120: #TODO assert m == R3_r.transform(R3_s, R3_s.transform(R3_r, m)).applyfunc(simplify)\n-",
      "file": ".codex/change_log-large.md",
      "line": 9150,
      "match": "\\bTODO\\b"
    },
    {
      "context": " .venv/lib/python3.12/site-packages/sympy/diffgeom/tests/test_diffgeom.py:123: #TODO assert m == R3_c.transform(R3_s, R3_s.transform(R3_c, m)).applyfunc(simplify)\n-",
      "file": ".codex/change_log-large.md",
      "line": 9151,
      "match": "\\bTODO\\b"
    },
    {
      "context": " .venv/lib/python3.12/site-packages/sympy/diffgeom/tests/test_diffgeom.py:128: #TODO assert m == R3_r.coord_tuple_transform_to(R3_c, R3_c.coord_tuple_transform_to(R",
      "file": ".codex/change_log-large.md",
      "line": 9152,
      "match": "\\bTODO\\b"
    },
    {
      "context": " .venv/lib/python3.12/site-packages/sympy/diffgeom/tests/test_diffgeom.py:131: #TODO assert m == R3_r.coord_tuple_transform_to(R3_s, R3_s.coord_tuple_transform_to(R",
      "file": ".codex/change_log-large.md",
      "line": 9153,
      "match": "\\bTODO\\b"
    },
    {
      "context": " .venv/lib/python3.12/site-packages/sympy/diffgeom/tests/test_diffgeom.py:134: #TODO assert m == R3_c.coord_tuple_transform_to(R3_s, R3_s.coord_tuple_transform_to(R",
      "file": ".codex/change_log-large.md",
      "line": 9154,
      "match": "\\bTODO\\b"
    },
    {
      "context": "lib/python3.12/site-packages/sympy/diffgeom/tests/test_hyperbolic_space.py:86: #TODO - it would be nice to have index contraction built-in\n- .venv/lib/python3.12/si",
      "file": ".codex/change_log-large.md",
      "line": 9155,
      "match": "\\bTODO\\b"
    },
    {
      "context": " it pass.\n- .venv/lib/python3.12/site-packages/sympy/solvers/solveset.py:694: # TODO : We should not blindly recurse through all args of arbitrary expressions like ",
      "file": ".codex/change_log-large.md",
      "line": 9160,
      "match": "\\bTODO\\b"
    },
    {
      "context": "dError):\n- .venv/lib/python3.12/site-packages/sympy/solvers/solveset.py:1662: # TODO Case: A-> function of symbol, can be extended here\n- .venv/lib/python3.12/site-",
      "file": ".codex/change_log-large.md",
      "line": 9165,
      "match": "\\bTODO\\b"
    },
    {
      "context": "`, pass it\n- .venv/lib/python3.12/site-packages/sympy/solvers/solvers.py:359: # TODO: improve solution testing\n- .venv/lib/python3.12/site-packages/sympy/solvers/so",
      "file": ".codex/change_log-large.md",
      "line": 9192,
      "match": "\\bTODO\\b"
    },
    {
      "context": " coverage\n- .venv/lib/python3.12/site-packages/sympy/solvers/solvers.py:2902: # TODO: option for calculating J numerically\n- .venv/lib/python3.12/site-packages/symp",
      "file": ".codex/change_log-large.md",
      "line": 9244,
      "match": "\\bTODO\\b"
    },
    {
      "context": " value of that\n- .venv/lib/python3.12/site-packages/sympy/solvers/pde.py:175: # TODO : 'best' hint should be implemented when adequate\n- .venv/lib/python3.12/site-p",
      "file": ".codex/change_log-large.md",
      "line": 9295,
      "match": "\\bTODO\\b"
    },
    {
      "context": "only partial \"\n- .venv/lib/python3.12/site-packages/sympy/solvers/pde.py:284: # TODO : For now pde.py uses support offered by the ode_order function\n- .venv/lib/pyt",
      "file": ".codex/change_log-large.md",
      "line": 9298,
      "match": "\\bTODO\\b"
    },
    {
      "context": "filldedent('''\n- .venv/lib/python3.12/site-packages/sympy/solvers/pde.py:521: # TODO : For now homogeneous first order linear PDE's having\n- .venv/lib/python3.12/si",
      "file": ".codex/change_log-large.md",
      "line": 9302,
      "match": "\\bTODO\\b"
    },
    {
      "context": "r PDE's having\n- .venv/lib/python3.12/site-packages/sympy/solvers/pde.py:611: # TODO : For now homogeneous first order linear PDE's having\n- .venv/lib/python3.12/si",
      "file": ".codex/change_log-large.md",
      "line": 9303,
      "match": "\\bTODO\\b"
    },
    {
      "context": "tion due\"\n- .venv/lib/python3.12/site-packages/sympy/solvers/bivariate.py:34: # TODO it would be good to pick the smallest divisible power\n- .venv/lib/python3.12/si",
      "file": ".codex/change_log-large.md",
      "line": 9309,
      "match": "\\bTODO\\b"
    },
    {
      "context": " .venv/lib/python3.12/site-packages/sympy/solvers/tests/test_solvers.py:1727: # TODO: Investigate why currently solution [0] is preferred over [1].\n- .venv/lib/pyth",
      "file": ".codex/change_log-large.md",
      "line": 9330,
      "match": "\\bTODO\\b"
    }
  ],
  "stubs_total": 9018,
  "stubs_truncated": 8018,
  "tokenizer_info": {
    "tokenizer_modules": [
      "codex_update_runner.py",
      "src/tokenizer/fast_tokenizer.py",
      "src/codex_ml/hf_loader.py",
      "src/codex_ml/utils/modeling.py",
      "src/codex_ml/tokenization/__init__.py",
      "src/codex_ml/tokenization/adapter.py",
      "src/codex_ml/tokenization/pipeline.py",
      "src/codex_ml/tokenization/hf_tokenizer.py",
      "src/codex_ml/eval/evaluator.py",
      "src/codex_ml/training/__init__.py",
      "src/codex_ml/training/functional_training.py",
      "src/codex_ml/cli/infer.py",
      "src/codex_ml/cli/generate.py",
      "src/codex_ml/interfaces/tokenizer.py",
      "src/tokenization/train_tokenizer.py",
      "src/tokenization/cli.py",
      "tools/codex_make_smoke_tests.py",
      "tests/test_symbolic_pipeline.py",
      "tests/test_modeling_utils.py",
      "tests/test_tokenizer.py",
      "tests/test_sentencepiece_adapter.py",
      "tests/test_training_integration_flags.py",
      "tests/test_run_functional_training_tokenizer.py",
      "tests/test_tokenizer_ids.py",
      "tests/test_hf_trainer_lora_config.py",
      "tests/test_engine_hf_trainer.py",
      "tests/test_interfaces_hf_tokenizer.py",
      "tests/test_engine_hf_trainer_lora.py",
      "tests/test_tokenizer_batch_encode.py",
      "tests/test_tokenizer_wrapper.py",
      "tests/test_hf_tokenizer_padding.py",
      "tests/smoke/test_hf_trainer_hello.py",
      "tests/utils/test_modeling.py",
      "tests/data/test_hf_factory_compat.py",
      "tests/data/test_cache_roundtrip.py",
      "tests/tokenization/test_roundtrip.py",
      "tests/tokenization/test_train_tokenizer_smoke.py",
      "tests/tokenization/test_sentencepiece_tokenizer.py",
      "tests/tokenization/conftest.py",
      "tests/tokenization/test_tokenizer_training_streaming_equivalence.py",
      "tests/training/test_run_functional_training_resume.py",
      "tests/training/test_strict_determinism.py",
      "tests/training/test_functional_training_evaluation.py",
      "tests/training/test_functional_training_main.py",
      "tests/training/test_engine_hf_trainer_lora_cfg.py",
      "tests/interfaces/test_tokenizer_hf.py",
      "training/engine_hf_trainer.py",
      "training/functional_training.py",
      "scripts/repo_audit.py",
      "scripts/make_quickstart_notebook.py"
    ],
    "vocab_files": [
      "artifacts/models/tiny_tokenizer/vocab.json"
    ]
  }
}
