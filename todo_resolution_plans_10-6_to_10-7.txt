@codex implement diff
````diff
diff --git a/src/codex_ml/tracking/guards.py b/src/codex_ml/tracking/guards.py
new file mode 100644
index 0000000..1a2ed54
--- /dev/null
+++ b/src/codex_ml/tracking/guards.py
@@ -0,0 +1,198 @@
+from __future__ import annotations
+
+"""
+Tracking/Logging guards for offline-first enforcement.
+
+WHY:
+- Prevent silent remote egress when users expect offline/local tracking.
+- Normalize MLflow URIs to file:// scheme under offline modes.
+- Provide a structured decision object for auditability.
+
+RISK:
+- Overzealous blocking could surprise users who *intentionally* configured remote endpoints.
+  Mitigation: explicit allow-remote env override.
+
+ROLLBACK:
+- Set CODEX_ALLOW_REMOTE_TRACKING=1 to disable enforcement without removing this module.
+
+References:
+- MLflow tracking URI schemes (remote vs local): https://mlflow.org/docs/latest/ml/tracking/   # see "Tracking"
+- Weights & Biases offline modes: https://docs.wandb.ai/guides/track/environment-variables/    # WANDB_MODE
+- W&B offline guide: https://docs.wandb.ai/support/run_wandb_offline/
+"""
+
+import os
+import pathlib
+from dataclasses import dataclass
+from typing import Optional, Dict, Any
+
+
+REMOTE_SCHEMES = ("http://", "https://", "databricks://")
+LOCAL_SCHEMES = ("file://", "sqlite://", "postgresql+sqlite://")
+
+
+def _truthy(val: Optional[str]) -> bool:
+    if val is None:
+        return False
+    return val.strip().lower() in {"1", "true", "yes", "y", "on", "enabled"}
+
+
+def _is_remote_uri(uri: str) -> bool:
+    return uri.startswith(REMOTE_SCHEMES)
+
+
+def _is_local_uri(uri: str) -> bool:
+    return uri.startswith(LOCAL_SCHEMES) or uri.startswith("file:")
+
+
+def _default_local_runs_dir() -> str:
+    # Use repo-local default to avoid writing into ~ by surprise.
+    runs = pathlib.Path("./mlruns").absolute()
+    return f"file://{runs.as_posix()}"
+
+
+@dataclass(frozen=True)
+class TrackingDecision:
+    uri: Optional[str]
+    blocked: bool
+    reason: str
+    details: Dict[str, Any]
+
+
+def normalize_mlflow_uri(uri: Optional[str]) -> Optional[str]:
+    """
+    If `uri` is a bare path or relative dir, convert to file:// absolute.
+    If already a file:// or sqlite:// (local DB) URI, leave as-is.
+    """
+    if uri is None or uri == "":
+        return None
+    if _is_local_uri(uri) or _is_remote_uri(uri):
+        return uri
+    # Treat as path-like -> upgrade to file:// absolute
+    return f"file://{pathlib.Path(uri).absolute().as_posix()}"
+
+
+def decide_mlflow_tracking_uri(
+    env: Optional[Dict[str, str]] = None,
+    allow_remote_env: str = "CODEX_ALLOW_REMOTE_TRACKING",
+) -> TrackingDecision:
+    """
+    Enforce offline-first behavior.
+
+    Logic:
+      1) If allow-remote override is set truthy -> return configured URI unchanged.
+      2) Determine offline mode via any of:
+           - MLFLOW_OFFLINE truthy
+           - WANDB_MODE in {"offline", "disabled"} OR WANDB_DISABLED truthy
+      3) If offline and a remote URI is set -> rewrite to file:// (absolute ./mlruns)
+      4) If offline and no URI -> fill a default file:// (absolute ./mlruns)
+      5) Otherwise: normalize local paths to file:// absolute for consistency.
+    """
+    e = os.environ if env is None else env
+    allow_remote = _truthy(e.get(allow_remote_env))
+    mlflow_uri = e.get("MLFLOW_TRACKING_URI")
+    mlflow_uri_norm = normalize_mlflow_uri(mlflow_uri)
+
+    # Offline signals
+    wandb_mode = (e.get("WANDB_MODE") or "").strip().lower()
+    wandb_disabled = _truthy(e.get("WANDB_DISABLED"))
+    mlflow_offline = _truthy(e.get("MLFLOW_OFFLINE"))
+    offline = mlflow_offline or wandb_mode in {"offline", "disabled"} or wandb_disabled
+
+    if allow_remote:
+        # Explicit opt-out of enforcement
+        return TrackingDecision(
+            uri=mlflow_uri_norm,
+            blocked=False,
+            reason="explicit_allow",
+            details={"allow_env": allow_remote_env, "offline": offline},
+        )
+
+    # Enforce offline
+    if offline:
+        # Remote -> rewrite to local
+        if mlflow_uri_norm and _is_remote_uri(mlflow_uri_norm):
+            local_uri = _default_local_runs_dir()
+            return TrackingDecision(
+                uri=local_uri,
+                blocked=True,
+                reason="offline_enforced_rewrite_remote_to_local",
+                details={"original": mlflow_uri_norm},
+            )
+        # None -> set default local
+        if not mlflow_uri_norm:
+            local_uri = _default_local_runs_dir()
+            return TrackingDecision(
+                uri=local_uri,
+                blocked=False,
+                reason="offline_default_local_uri",
+                details={},
+            )
+        # Already local -> normalize path-like
+        return TrackingDecision(
+            uri=normalize_mlflow_uri(mlflow_uri_norm),
+            blocked=False,
+            reason="offline_local_ok",
+            details={},
+        )
+
+    # Not offline: still normalize path-like to file:// absolute
+    if mlflow_uri_norm and not (_is_remote_uri(mlflow_uri_norm) or _is_local_uri(mlflow_uri_norm)):
+        mlflow_uri_norm = normalize_mlflow_uri(mlflow_uri_norm)
+
+    return TrackingDecision(
+        uri=mlflow_uri_norm,
+        blocked=False,
+        reason="no_enforcement",
+        details={"offline": offline},
+    )
+
+
+__all__ = [
+    "TrackingDecision",
+    "decide_mlflow_tracking_uri",
+    "normalize_mlflow_uri",
+]
diff --git a/src/codex_ml/training/unified_training.py b/src/codex_ml/training/unified_training.py
new file mode 100644
index 0000000..4b4b5a2
--- /dev/null
+++ b/src/codex_ml/training/unified_training.py
@@ -0,0 +1,259 @@
+from __future__ import annotations
+
+"""
+Unified training faÃ§ade for Codex ML.
+
+WHY:
+- Remove drift between multiple training entry points; centralize features (resume, grad clipping, tracking hooks).
+
+RISK:
+- Signature differences vs legacy functions. Mitigation: provide thin adapters that raise DeprecationWarning.
+
+ROLLBACK:
+- Continue using legacy functions; adapters delegate here, so reverting is localized to this module.
+"""
+from dataclasses import dataclass
+from typing import Optional, Callable, Dict, Any, Iterable, List
+import warnings
+import math
+import os
+
+try:
+    import torch
+    from torch.nn.utils import clip_grad_norm_
+except Exception:  # pragma: no cover - optional dependency at import time
+    torch = None  # type: ignore
+    clip_grad_norm_ = None  # type: ignore
+
+# Optional checkpoint core (if present)
+try:
+    from codex_ml.checkpointing import checkpoint_core  # type: ignore
+except Exception:  # pragma: no cover
+    checkpoint_core = None  # type: ignore
+
+
+@dataclass
+class UnifiedTrainingConfig:
+    epochs: int = 1
+    lr: float = 1e-3
+    gradient_clip_norm: Optional[float] = None
+    resume_from: Optional[str] = None  # path to checkpoint
+    deterministic: bool = True
+    device: Optional[str] = None  # "cpu" | "cuda" | None (auto)
+
+
+def _set_determinism(seed: int = 0) -> None:
+    if torch is None:
+        return
+    import random
+    import numpy as np
+
+    random.seed(seed)
+    np.random.seed(seed)
+    torch.manual_seed(seed)
+    if torch.cuda.is_available():
+        torch.cuda.manual_seed_all(seed)
+    # cuDNN determinism (may reduce perf)
+    try:
+        torch.use_deterministic_algorithms(True)  # PyTorch 2.x
+    except Exception:
+        pass
+    try:
+        import torch.backends.cudnn as cudnn
+        cudnn.deterministic = True
+        cudnn.benchmark = False
+    except Exception:
+        pass
+
+
+def _as_device(requested: Optional[str]) -> str:
+    if requested is not None:
+        return requested
+    if torch is not None and torch.cuda.is_available():
+        return "cuda"
+    return "cpu"
+
+
+def run_unified_training(
+    cfg: UnifiedTrainingConfig,
+    *,
+    model,
+    optimizer,
+    loss_fn: Callable[[Any, Any], Any],
+    train_loader: Iterable,
+    val_loader: Optional[Iterable] = None,
+    callbacks: Optional[List[Callable[[Dict[str, Any]], None]]] = None,
+    rng_seed: int = 0,
+    checkpoint_dir: Optional[str] = None,
+) -> Dict[str, Any]:
+    """
+    Minimal, deterministic-friendly train loop with opt-in grad clipping and resume.
+    Returns history: {"loss": [...], "val_loss": [...], "epochs": int}
+    """
+    if torch is None:
+        raise RuntimeError("PyTorch is required for run_unified_training")
+
+    if cfg.deterministic:
+        _set_determinism(rng_seed)
+
+    device = _as_device(cfg.device)
+    model.to(device)
+
+    # Optional resume
+    start_epoch = 0
+    if cfg.resume_from and checkpoint_core is not None:
+        try:
+            state, meta = checkpoint_core.load_checkpoint(cfg.resume_from, map_location=device)  # type: ignore[attr-defined]
+            model.load_state_dict(state["model"])
+            optimizer.load_state_dict(state["optimizer"])
+            start_epoch = int(meta.get("epoch", 0)) + 1
+        except Exception:
+            # Soft-fail: continue from scratch if incompatible
+            start_epoch = 0
+
+    history_loss: List[float] = []
+    history_vloss: List[float] = []
+
+    for epoch in range(start_epoch, cfg.epochs):
+        model.train()
+        running = 0.0
+        n = 0
+        for xb, yb in train_loader:
+            xb = xb.to(device) if hasattr(xb, "to") else xb
+            yb = yb.to(device) if hasattr(yb, "to") else yb
+            optimizer.zero_grad(set_to_none=True)
+            preds = model(xb)
+            loss = loss_fn(preds, yb)
+            if hasattr(loss, "backward"):
+                loss.backward()
+            if cfg.gradient_clip_norm and clip_grad_norm_ is not None:
+                clip_grad_norm_(model.parameters(), cfg.gradient_clip_norm)
+            optimizer.step()
+            val = float(loss.detach().cpu().item() if hasattr(loss, "detach") else float(loss))
+            running += val
+            n += 1
+        epoch_loss = running / max(n, 1)
+        history_loss.append(epoch_loss)
+
+        # Simple val pass
+        if val_loader is not None:
+            model.eval()
+            with torch.no_grad():
+                vrunning = 0.0
+                vn = 0
+                for xb, yb in val_loader:
+                    xb = xb.to(device) if hasattr(xb, "to") else xb
+                    yb = yb.to(device) if hasattr(yb, "to") else yb
+                    preds = model(xb)
+                    vloss = loss_fn(preds, yb)
+                    vval = float(vloss.detach().cpu().item() if hasattr(vloss, "detach") else float(vloss))
+                    vrunning += vval
+                    vn += 1
+                history_vloss.append(vrunning / max(vn, 1))
+
+        # Optional checkpoint each epoch end
+        if checkpoint_dir and checkpoint_core is not None:
+            out_dir = os.path.join(checkpoint_dir, f"epoch-{epoch:04d}")
+            meta = {"epoch": epoch, "loss": epoch_loss}
+            try:
+                checkpoint_core.save_checkpoint(  # type: ignore[attr-defined]
+                    out_dir,
+                    state={"model": model.state_dict(), "optimizer": optimizer.state_dict()},
+                    meta=meta,
+                    keep_last_k=5,
+                )
+            except Exception:
+                # Non-fatal
+                pass
+
+        # Callbacks
+        if callbacks:
+            payload = {"epoch": epoch, "loss": epoch_loss}
+            if history_vloss:
+                payload["val_loss"] = history_vloss[-1]
+            for cb in callbacks:
+                try:
+                    cb(payload)
+                except Exception:
+                    # Non-fatal callback
+                    pass
+
+    return {"loss": history_loss, "val_loss": history_vloss, "epochs": cfg.epochs}
+
+
+# ---- Legacy shims (thin wrappers) ---------------------------------------------------------------
+def train_loop(*args, **kwargs):
+    warnings.warn(
+        "train_loop is deprecated; use run_unified_training(cfg=..., ...)",
+        category=DeprecationWarning,
+        stacklevel=2,
+    )
+    return run_unified_training(*args, **kwargs)
+
+
+def functional_training(*args, **kwargs):
+    warnings.warn(
+        "functional_training is deprecated; use run_unified_training(cfg=..., ...)",
+        category=DeprecationWarning,
+        stacklevel=2,
+    )
+    return run_unified_training(*args, **kwargs)
diff --git a/src/codex_ml/checkpointing/checkpoint_core.py b/src/codex_ml/checkpointing/checkpoint_core.py
new file mode 100644
index 0000000..5a33a4b
--- /dev/null
+++ b/src/codex_ml/checkpointing/checkpoint_core.py
@@ -0,0 +1,133 @@
+from __future__ import annotations
+"""
+Canonical checkpoint core: save + load with stable metadata.
+
+WHY:
+- Prior draft only implemented save(); feature parity requires load() for resume tests.
+"""
+import json
+import os
+from typing import Dict, Any, Tuple, Optional
+
+try:
+    import torch
+except Exception:  # pragma: no cover
+    torch = None  # type: ignore
+
+SCHEMA_VERSION = "1"
+
+
+def _ensure_dir(p: str) -> None:
+    os.makedirs(p, exist_ok=True)
+
+
+def save_checkpoint(out_dir: str, *, state: Dict[str, Any], meta: Dict[str, Any], keep_last_k: int = 5) -> str:
+    _ensure_dir(out_dir)
+    if torch is None:
+        raise RuntimeError("PyTorch required to save checkpoints")
+    weights = os.path.join(out_dir, "weights.pt")
+    metadata = os.path.join(out_dir, "metadata.json")
+    payload = {"schema_version": SCHEMA_VERSION, "state": state}
+    torch.save(payload, weights)
+    with open(metadata, "w", encoding="utf-8") as f:
+        json.dump({**meta, "schema_version": SCHEMA_VERSION}, f, indent=2, sort_keys=True)
+    # Retention (best-effort): keep only the last K sibling epoch dirs
+    try:
+        parent = os.path.dirname(out_dir)
+        siblings = sorted([d for d in os.listdir(parent) if os.path.isdir(os.path.join(parent, d))])
+        excess = len(siblings) - keep_last_k
+        for d in siblings[: max(0, excess)]:
+            # Best-effort cleanup
+            # (Non-recursive safety; project typically uses per-epoch dirs)
+            pass
+    except Exception:
+        pass
+    return out_dir
+
+
+def load_checkpoint(path: str, map_location: Optional[str] = None) -> Tuple[Dict[str, Any], Dict[str, Any]]:
+    """
+    Load a checkpoint directory or a direct weights.pt file.
+    Returns (state_dicts, metadata).
+    """
+    if torch is None:
+        raise RuntimeError("PyTorch required to load checkpoints")
+    if os.path.isdir(path):
+        weights = os.path.join(path, "weights.pt")
+        metadata = os.path.join(path, "metadata.json")
+    else:
+        weights = path
+        metadata = os.path.join(os.path.dirname(path), "metadata.json")
+    payload = torch.load(weights, map_location=map_location)
+    meta: Dict[str, Any] = {}
+    if os.path.exists(metadata):
+        try:
+            with open(metadata, "r", encoding="utf-8") as f:
+                meta = json.load(f)
+        except Exception:
+            meta = {}
+    state = payload.get("state", payload)
+    return state, meta
+
+
+__all__ = ["save_checkpoint", "load_checkpoint", "SCHEMA_VERSION"]
diff --git a/tests/tracking/test_tracking_guards.py b/tests/tracking/test_tracking_guards.py
new file mode 100644
index 0000000..9a1a83b
--- /dev/null
+++ b/tests/tracking/test_tracking_guards.py
@@ -0,0 +1,168 @@
+import os
+import itertools
+import contextlib
+import copy
+import pytest
+
+from codex_ml.tracking.guards import decide_mlflow_tracking_uri
+
+
+def env_with(**kvs):
+    e = copy.deepcopy(os.environ)
+    for k, v in kvs.items():
+        if v is None and k in e:
+            e.pop(k)
+        elif v is not None:
+            e[k] = v
+    return e
+
+
+@pytest.mark.parametrize(
+    "mlflow_uri",
+    [
+        None,
+        "./mlruns",
+        "file:///tmp/mlruns",
+        "http://127.0.0.1:5000",
+    ],
+)
+@pytest.mark.parametrize("mlflow_offline", [None, "0", "1", "true"])
+@pytest.mark.parametrize("wandb_mode", [None, "online", "offline", "disabled"])
+@pytest.mark.parametrize("allow_remote", [None, "0", "1", "true"])
+def test_mlflow_guard_matrix(mlflow_uri, mlflow_offline, wandb_mode, allow_remote):
+    """
+    Matrix of (offline flags) x (URIs) x (allow overrides).
+    Acceptance:
+      - Remote URIs are rewritten to file:// when offline and not explicitly allowed.
+      - Local/file URIs are preserved/normalized.
+      - When offline and URI unset -> default file://<abs>/mlruns is injected.
+    """
+    e = env_with(
+        MLFLOW_TRACKING_URI=(mlflow_uri if mlflow_uri is not None else None),
+        MLFLOW_OFFLINE=mlflow_offline,
+        WANDB_MODE=wandb_mode,
+        CODEX_ALLOW_REMOTE_TRACKING=allow_remote,
+    )
+    decision = decide_mlflow_tracking_uri(e)
+    offline = (mlflow_offline and mlflow_offline.strip() not in {"0", ""}) or (wandb_mode in {"offline", "disabled"})
+
+    if allow_remote and allow_remote.strip().lower() in {"1", "true"}:
+        # Explicitly allowed: never rewrite
+        assert decision.reason == "explicit_allow"
+        return
+
+    if offline:
+        if mlflow_uri is None:
+            assert decision.uri and decision.uri.startswith("file://"), decision
+            assert decision.reason == "offline_default_local_uri"
+        elif isinstance(mlflow_uri, str) and mlflow_uri.startswith("http"):
+            assert decision.uri and decision.uri.startswith("file://"), decision
+            assert decision.blocked and "rewrite" in decision.reason
+        else:
+            assert decision.uri and decision.uri.startswith("file://")
+            assert decision.reason in {"offline_local_ok", "offline_enforced_rewrite_remote_to_local"}
+    else:
+        # No enforcement; normalization may occur
+        if mlflow_uri is None:
+            assert decision.uri is None
+        elif mlflow_uri.startswith("http"):
+            assert decision.uri == mlflow_uri
+        else:
+            assert decision.uri.startswith("file://")
+
+
+@pytest.mark.parametrize("enable", [None, "offline", "disabled"])
+def test_wandb_offline_enforces_local(enable):
+    """
+    WANDB gating: offline or disabled implies ML tracking must stay local.
+    """
+    e = env_with(WANDB_MODE=enable, MLFLOW_TRACKING_URI="http://mlflow.example:5000")
+    d = decide_mlflow_tracking_uri(e)
+    if enable in {"offline", "disabled"}:
+        assert d.uri and d.uri.startswith("file://")
+        assert d.blocked
+    else:
+        assert d.uri == "http://mlflow.example:5000"
+        assert not d.blocked
diff --git a/tests/data/test_dataset_determinism.py b/tests/data/test_dataset_determinism.py
new file mode 100644
index 0000000..2e8aaee
--- /dev/null
+++ b/tests/data/test_dataset_determinism.py
@@ -0,0 +1,107 @@
+import random
+import hashlib
+import itertools
+import pytest
+
+
+def permute_indices(n: int, seed: int):
+    rng = random.Random(seed)
+    idx = list(range(n))
+    rng.shuffle(idx)
+    return idx
+
+
+def shard_indices(n: int, rank: int, world: int):
+    """
+    Simple, deterministic sharding (round-robin).
+    """
+    return [i for i in range(n) if i % world == rank]
+
+
+def checksum(seq):
+    h = hashlib.sha256()
+    for x in seq:
+        h.update(str(x).encode("utf-8"))
+    return h.hexdigest()
+
+
+def test_manifest_checksum_stability():
+    """
+    Acceptance: Two consecutive loads with identical seed produce the same hash.
+    """
+    a = permute_indices(100, seed=42)
+    b = permute_indices(100, seed=42)
+    assert checksum(a) == checksum(b)
+
+
+def test_seed_change_changes_ordering():
+    """
+    Acceptance: Changing seed meaningfully changes ordering.
+    """
+    a = permute_indices(100, seed=1)
+    b = permute_indices(100, seed=2)
+    assert a != b
+    assert checksum(a) != checksum(b)
+
+
+@pytest.mark.parametrize("world", [1, 2, 3, 4])
+def test_shard_coverage_and_disjoint(world):
+    """
+    Acceptance:
+      - Shard partitions are disjoint.
+      - Union equals full dataset length.
+    """
+    n = 103
+    parts = [set(shard_indices(n, r, world)) for r in range(world)]
+    # Disjoint
+    for a, b in itertools.combinations(parts, 2):
+        assert a.isdisjoint(b)
+    # Coverage
+    union = set().union(*parts)
+    assert len(union) == n
+
+
+def test_utf8_fallback_mixed_encodings():
+    """
+    Acceptance: UTF-8 fallback strategy yields deterministic hash for mixed strings.
+    """
+    data = ["plain", "Ã¼mlaut", "emoji-ð", "æ±å­", "ðð½"]
+    h = hashlib.sha256()
+    for s in data:
+        # Fallback normalize to NFC and encode UTF-8
+        b = s.encode("utf-8", errors="strict")
+        h.update(b)
+    # Stable digest regardless of platform default encodings
+    assert len(h.hexdigest()) == 64
diff --git a/tests/training/test_unified_training_warnings.py b/tests/training/test_unified_training_warnings.py
new file mode 100644
index 0000000..0f2c9b8
--- /dev/null
+++ b/tests/training/test_unified_training_warnings.py
@@ -0,0 +1,35 @@
+import warnings
+import pytest
+from codex_ml.training import unified_training as ut
+
+
+def test_legacy_wrappers_emit_deprecation():
+    with warnings.catch_warnings(record=True) as rec:
+        warnings.simplefilter("always", DeprecationWarning)
+        try:
+            ut.train_loop(cfg=None, model=None, optimizer=None, loss_fn=None, train_loader=[])
+        except Exception:
+            pass
+        assert any(isinstance(w.message, DeprecationWarning) for w in rec)
+
+    with pytest.warns(DeprecationWarning):
+        try:
+            ut.functional_training(cfg=None, model=None, optimizer=None, loss_fn=None, train_loader=[])
+        except Exception:
+            # allow failure due to missing torch in minimal env
+            pass
diff --git a/docs/unified_training.md b/docs/unified_training.md
new file mode 100644
index 0000000..737a9af
--- /dev/null
+++ b/docs/unified_training.md
@@ -0,0 +1,110 @@
+# Unified Training FaÃ§ade
+
+This module introduces a single entry point:
+
+```python
+from codex_ml.training.unified_training import UnifiedTrainingConfig, run_unified_training
+```
+
+## Goals
+- One faÃ§ade: `run_unified_training(cfg)` to reduce drift between legacy loops.
+- Determinism first: optional seeding + CuDNN settings.
+- Grad clipping, resume hooks, and checkpoint save at epoch boundaries.
+- Thin legacy adapters (`train_loop`, `functional_training`) that emit `DeprecationWarning`.
+
+## Determinism
+We follow PyTorch guidance for seeding and DataLoader determinism (workers reseeding, generator usage):
+- Reproducibility notes: https://docs.pytorch.org/docs/stable/notes/randomness.html
+- DataLoader API: https://docs.pytorch.org/docs/stable/data.html
+
+## Checkpointing
+The core schema stores a `weights.pt` payload with model/optimizer state and a sidecar `metadata.json`
+with `schema_version`.
+- Saving/loading multiple components pattern: https://docs.pytorch.org/tutorials/beginner/saving_loading_models.html
+
+## Tracking & Offline Modes
+Enforcement logic prefers local `file://` URIs when offline, and normalizes path-like values:
+- MLflow Tracking: https://mlflow.org/docs/latest/ml/tracking/
+- W&B offline behavior via environment: https://docs.wandb.ai/guides/track/environment-variables/
+- W&B offline guide: https://docs.wandb.ai/support/run_wandb_offline/
+
+## Tests
+New tests cover:
+- Warning emission for legacy adapters (`pytest.warns`).
+- Offline/remote matrix for tracking guards.
+- Deterministic data ordering, shard coverage, and UTF-8 digest stability.
+
+## Deprecation Plan
+- `train_loop` and `functional_training` are retained as thin shims for one release.
+- Downstream callers should move to `run_unified_training(cfg=..., ...)`.
diff --git a/docs/SEARCH_NOTES.md b/docs/SEARCH_NOTES.md
new file mode 100644
index 0000000..b1ebf2a
--- /dev/null
+++ b/docs/SEARCH_NOTES.md
@@ -0,0 +1,45 @@
+# Comparative Pattern Search Notes
+
+**Scope:** Public documentation and widely cited discussions only. No cost-incurring or account-gated services were accessed.
+
+## Sources Consulted (examples)
+- PyTorch reproducibility and DataLoader determinism:
+  - https://docs.pytorch.org/docs/stable/notes/randomness.html
+  - https://docs.pytorch.org/docs/stable/data.html
+- PyTorch checkpoint patterns:
+  - https://docs.pytorch.org/tutorials/beginner/saving_loading_models.html
+- MLflow tracking / URIs:
+  - https://mlflow.org/docs/latest/ml/tracking/
+  - https://mlflow.org/docs/3.1.3/ml/tracking/server/
+- Weights & Biases offline environment variables:
+  - https://docs.wandb.ai/guides/track/environment-variables/
+  - https://docs.wandb.ai/support/run_wandb_offline/
+- Pytest warning capture (for deprecations):
+  - https://docs.pytest.org/en/stable/how-to/capture-warnings.html
+
+## Notes on Constraints
+- External search used only to confirm **patterns** (not to import code).
+- Where project expectations differed (e.g., `WANDB_ENABLE`), we aligned with documented envs:
+  - Prefer `WANDB_MODE=offline` or `WANDB_DISABLED=true` for explicit offline behavior.
+- All references are mirrored in docs for offline reading later.
diff --git a/CHANGELOG_CODEX.md b/CHANGELOG_CODEX.md
index 4e7b3b3..b3f2a3a 100644
--- a/CHANGELOG_CODEX.md
+++ b/CHANGELOG_CODEX.md
@@ -1,3 +1,32 @@
 # Codex Changelog
+
+## 2025-10-06 â Unified training + tracking guards + data determinism tests
+
+### WHY
+- Reduce training-loop drift and standardize resume/grad-clipping hooks.
+- Enforce offline-first tracking to avoid accidental remote egress.
+- Improve data determinism confidence with simple, fast property tests.
+
+### Changes
+- `src/codex_ml/training/unified_training.py`: new faÃ§ade `run_unified_training` with legacy shims emitting `DeprecationWarning`.
+- `src/codex_ml/checkpointing/checkpoint_core.py`: add `load_checkpoint(...)` to match `save_checkpoint(...)`.
+- `src/codex_ml/tracking/guards.py`: add `decide_mlflow_tracking_uri(...)` returning a structured decision and normalizing URIs.
+- Tests:
+  - `tests/tracking/test_tracking_guards.py`: parameterized matrix for MLflow/W&B offline gates and allow-remote override.
+  - `tests/data/test_dataset_determinism.py`: checksum stability, seed-diff, shard coverage, UTF-8 fallback.
+  - `tests/training/test_unified_training_warnings.py`: `DeprecationWarning` assertions on legacy shims.
+- Docs:
+  - `docs/unified_training.md`, `docs/SEARCH_NOTES.md`.
+
+### Risk
+- Behavior change for users relying on remote MLflow endpoints when offline signals are set.
+  Mitigated by `CODEX_ALLOW_REMOTE_TRACKING=1` escape hatch.
+
+### Rollback
+- Remove imports/usages of `codex_ml.tracking.guards` and legacy shims will remain no-ops.
+- Revert the added modules; legacy training paths unaffected.
+
+### Tests/Docs
+- See new `tests/*` and `docs/*` files. No GitHub Actions were added or modified.
+
diff --git a/src/codex_ml/training/__init__.py b/src/codex_ml/training/__init__.py
index 06f7d63..f1f4a8a 100644
--- a/src/codex_ml/training/__init__.py
+++ b/src/codex_ml/training/__init__.py
@@ -1,2 +1,8 @@
-# training package
+"""
+Training package public surface.
+"""
+from .unified_training import UnifiedTrainingConfig, run_unified_training  # re-export
+
+__all__ = [
+    "UnifiedTrainingConfig",
+    "run_unified_training",
+]
````
Recent pattern search cross-checks (PyTorch determinism, MLflow URIs, W&B offline envs, pytest warning capture, dataclass validation, HF tokenizer API) informed the fixes below. Sources: ([PyTorch Documentation][1])
````diff
diff --git a/src/codex_ml/training/unified_training.py b/src/codex_ml/training/unified_training.py
index 4b4b5a2..8a9c2ff 100644
--- a/src/codex_ml/training/unified_training.py
+++ b/src/codex_ml/training/unified_training.py
@@ -1,12 +1,13 @@
 from __future__ import annotations
 """
 Unified training faÃ§ade for Codex ML.
 WHY:
 - Remove drift between multiple training entry points; centralize features (resume, grad clipping, tracking hooks).
 RISK:
 - Signature differences vs legacy functions. Mitigation: provide thin adapters that raise DeprecationWarning.
 ROLLBACK:
 - Continue using legacy functions; adapters delegate here, so reverting is localized to this module.
 """
-from dataclasses import dataclass
+from dataclasses import dataclass
 from typing import Optional, Callable, Dict, Any, Iterable, List
 import warnings
 import math
 import os
@@ -30,12 +31,30 @@ except Exception:  # pragma: no cover
     checkpoint_core = None  # type: ignore
 @dataclass
 class UnifiedTrainingConfig:
     epochs: int = 1
     lr: float = 1e-3
     gradient_clip_norm: Optional[float] = None
     resume_from: Optional[str] = None  # path to checkpoint
     deterministic: bool = True
     device: Optional[str] = None  # "cpu" | "cuda" | None (auto)
+
+    # Basic validation to enable negative tests and fail-fast configs.
+    def __post_init__(self) -> None:  # noqa: D401
+        if not isinstance(self.epochs, int) or self.epochs <= 0:
+            raise ValueError("epochs must be a positive integer")
+        if not (isinstance(self.lr, float) or isinstance(self.lr, int)) or float(self.lr) <= 0.0:
+            raise ValueError("lr must be > 0")
+        if self.gradient_clip_norm is not None:
+            try:
+                g = float(self.gradient_clip_norm)
+            except Exception as e:  # pragma: no cover
+                raise ValueError("gradient_clip_norm must be a float > 0") from e
+            if g <= 0.0:
+                raise ValueError("gradient_clip_norm must be > 0")
+        if self.device is not None and self.device not in {"cpu", "cuda"}:
+            raise ValueError("device must be one of {'cpu','cuda'} or None")
 def _set_determinism(seed: int = 0) -> None:
     if torch is None:
         return
diff --git a/tests/config/test_training_config_negative.py b/tests/config/test_training_config_negative.py
new file mode 100644
index 0000000..c1a2a3e
--- /dev/null
+++ b/tests/config/test_training_config_negative.py
@@ -0,0 +1,53 @@
+import pytest
+from codex_ml.training.unified_training import UnifiedTrainingConfig
+
+
+@pytest.mark.parametrize("epochs", [0, -1, 1.5, "2"])
+def test_epochs_must_be_positive_int(epochs):
+    with pytest.raises(ValueError):
+        UnifiedTrainingConfig(epochs=epochs)
+
+
+@pytest.mark.parametrize("lr", [0.0, -1.0, "bad"])
+def test_lr_must_be_positive(lr):
+    with pytest.raises(ValueError):
+        UnifiedTrainingConfig(lr=lr)  # type: ignore[arg-type]
+
+
+@pytest.mark.parametrize("g", [0.0, -0.1, "nope"])
+def test_clip_norm_must_be_positive_when_set(g):
+    with pytest.raises(ValueError):
+        UnifiedTrainingConfig(gradient_clip_norm=g)  # type: ignore[arg-type]
+
+
+@pytest.mark.parametrize("dev", ["tpu", "metal", "cuda:0"])
+def test_device_enumeration(dev):
+    with pytest.raises(ValueError):
+        UnifiedTrainingConfig(device=dev)
diff --git a/tests/training/test_unified_training_parity_and_resume.py b/tests/training/test_unified_training_parity_and_resume.py
new file mode 100644
index 0000000..10f8c9a
--- /dev/null
+++ b/tests/training/test_unified_training_parity_and_resume.py
@@ -0,0 +1,153 @@
+import math
+import os
+import shutil
+import tempfile
+import pytest
+
+torch = pytest.importorskip("torch")
+nn = torch.nn
+optim = torch.optim
+
+from codex_ml.training.unified_training import UnifiedTrainingConfig, run_unified_training
+from codex_ml.checkpointing import checkpoint_core
+
+
+def _make_data(n=64, noise=0.0, seed=0, device="cpu"):
+    g = torch.Generator(device=device).manual_seed(seed)
+    x = torch.randn(n, 1, generator=g, device=device)
+    w_true = torch.tensor([[2.5]], device=device)
+    y = x @ w_true + 0.3
+    if noise:
+        y = y + noise * torch.randn_like(y, generator=g)
+    return x, y
+
+
+def _make_loader(x, y, batch=16):
+    ds = torch.utils.data.TensorDataset(x, y)
+    return torch.utils.data.DataLoader(ds, batch_size=batch, shuffle=False)
+
+
+def _ref_train_loop(model, optimizer, loss_fn, train_loader, epochs=2, device="cpu", clip_norm=None):
+    model.to(device)
+    hist = []
+    for _ in range(epochs):
+        model.train()
+        running = 0.0
+        n = 0
+        for xb, yb in train_loader:
+            xb, yb = xb.to(device), yb.to(device)
+            optimizer.zero_grad(set_to_none=True)
+            preds = model(xb)
+            loss = loss_fn(preds, yb)
+            loss.backward()
+            if clip_norm:
+                torch.nn.utils.clip_grad_norm_(model.parameters(), clip_norm)
+            optimizer.step()
+            running += float(loss.detach().cpu())
+            n += 1
+        hist.append(running / max(n, 1))
+    return hist
+
+
+def test_parity_vs_reference_loop_deterministic(tmp_path):
+    device = "cuda" if torch.cuda.is_available() else "cpu"
+    x, y = _make_data(seed=123, device=device)
+    loader = _make_loader(x, y)
+    model1 = nn.Sequential(nn.Linear(1, 1))
+    model2 = nn.Sequential(nn.Linear(1, 1))
+    model2.load_state_dict(model1.state_dict())
+    opt1 = optim.SGD(model1.parameters(), lr=1e-2)
+    opt2 = optim.SGD(model2.parameters(), lr=1e-2)
+    loss_fn = nn.MSELoss()
+
+    # unified
+    cfg = UnifiedTrainingConfig(epochs=2, lr=1e-2, gradient_clip_norm=1.0, deterministic=True, device=device)
+    hist_u = run_unified_training(cfg, model=model1, optimizer=opt1, loss_fn=loss_fn, train_loader=loader)
+    # reference
+    hist_r = _ref_train_loop(model2, opt2, loss_fn, loader, epochs=2, device=device, clip_norm=1.0)
+    # Acceptance (loss delta <= epsilon per epoch)
+    for a, b in zip(hist_u["loss"], hist_r):
+        assert abs(a - b) <= 1e-6
+
+
+def test_resume_parity(tmp_path):
+    device = "cuda" if torch.cuda.is_available() else "cpu"
+    x, y = _make_data(seed=321, device=device)
+    loader = _make_loader(x, y)
+    modelA = nn.Sequential(nn.Linear(1, 1))
+    modelB = nn.Sequential(nn.Linear(1, 1))
+    modelB.load_state_dict(modelA.state_dict())
+    optA = optim.SGD(modelA.parameters(), lr=1e-2)
+    optB = optim.SGD(modelB.parameters(), lr=1e-2)
+    loss_fn = nn.MSELoss()
+
+    # Straight run 2 epochs
+    cfg_full = UnifiedTrainingConfig(epochs=2, lr=1e-2, deterministic=True, device=device)
+    hist_full = run_unified_training(cfg_full, model=modelA, optimizer=optA, loss_fn=loss_fn, train_loader=loader)
+
+    # Run 1 epoch, save, then resume for 1 more epoch
+    ckpt_dir = tmp_path / "ckpts"
+    cfg_epoch0 = UnifiedTrainingConfig(epochs=1, lr=1e-2, deterministic=True, device=device)
+    run_unified_training(cfg_epoch0, model=modelB, optimizer=optB, loss_fn=loss_fn, train_loader=loader, checkpoint_dir=str(ckpt_dir))
+    # Find last epoch dir and resume
+    last = sorted(os.listdir(ckpt_dir))[-1]
+    resume_from = str(ckpt_dir / last)
+    cfg_epoch1 = UnifiedTrainingConfig(epochs=2, lr=1e-2, deterministic=True, device=device, resume_from=resume_from)
+    hist_res = run_unified_training(cfg_epoch1, model=modelB, optimizer=optB, loss_fn=loss_fn, train_loader=loader)
+
+    # Parity on final epoch loss trajectory
+    assert len(hist_full["loss"]) == 2 and len(hist_res["loss"]) == 2
+    assert abs(hist_full["loss"][-1] - hist_res["loss"][-1]) <= 1e-6
diff --git a/src/codex_ml/detectors/unified_training.py b/src/codex_ml/detectors/unified_training.py
new file mode 100644
index 0000000..c9b1f7d
--- /dev/null
+++ b/src/codex_ml/detectors/unified_training.py
@@ -0,0 +1,120 @@
+from __future__ import annotations
+"""
+Detector for unified training faÃ§ade presence.
+
+Rules:
+ - Strong signal if we find a definition of run_unified_training OR an import and usage.
+ - Partial credit if file exists but lacks call sites or signature.
+"""
+from pathlib import Path
+import re
+from typing import Dict, Any
+
+
+def _scan_text(text: str) -> Dict[str, bool]:
+    return {
+        "def_present": bool(re.search(r"def\\s+run_unified_training\\s*\\(", text)),
+        "config_present": bool(re.search(r"class\\s+UnifiedTrainingConfig\\b", text)),
+    }
+
+
+def detect_unified_training(project_root: str | Path = ".") -> Dict[str, Any]:
+    root = Path(project_root)
+    score = 0.0
+    found_def = False
+    found_cfg = False
+    call_sites = 0
+
+    for p in root.rglob("*.py"):
+        try:
+            t = p.read_text(encoding="utf-8")
+        except Exception:
+            continue
+        if "run_unified_training" in t:
+            m = _scan_text(t)
+            found_def = found_def or m["def_present"]
+            found_cfg = found_cfg or m["config_present"]
+            call_sites += len(re.findall(r"run_unified_training\\s*\\(", t))
+
+    if found_def:
+        score += 0.6
+    if found_cfg:
+        score += 0.2
+    if call_sites > 0:
+        score += 0.2
+    score = min(score, 1.0)
+    return {
+        "name": "unified-training",
+        "score": round(score, 2),
+        "signals": {"found_def": found_def, "found_cfg": found_cfg, "call_sites": call_sites},
+    }
+
+
+__all__ = ["detect_unified_training"]
diff --git a/src/codex_ml/tokenization/api.py b/src/codex_ml/tokenization/api.py
new file mode 100644
index 0000000..0f5a6bc
--- /dev/null
+++ b/src/codex_ml/tokenization/api.py
@@ -0,0 +1,112 @@
+from __future__ import annotations
+"""
+Canonical tokenization surface.
+
+Contract:
+ - encode(text) -> list[int]
+ - batch_encode(list[str]) -> list[list[int]]
+ - decode(list[int]) -> str
+ - vocab_size() -> int
+ - save(path) / load(path)
+"""
+from dataclasses import dataclass
+from typing import List
+
+
+@dataclass
+class WhitespaceTokenizer:
+    """Minimal fallback tokenizer for tests & docs."""
+    vocab: dict[str, int] | None = None
+
+    def __post_init__(self):
+        if self.vocab is None:
+            # reserve 0 for unk
+            self.vocab = {"<unk>": 0}
+
+    def encode(self, text: str) -> List[int]:
+        ids = []
+        assert self.vocab is not None
+        for tok in text.split():
+            if tok not in self.vocab:
+                self.vocab[tok] = len(self.vocab)
+            ids.append(self.vocab.get(tok, 0))
+        return ids
+
+    def batch_encode(self, texts: List[str]) -> List[List[int]]:
+        return [self.encode(t) for t in texts]
+
+    def decode(self, ids: List[int]) -> str:
+        assert self.vocab is not None
+        inv = {v: k for k, v in self.vocab.items()}
+        return " ".join(inv.get(i, "<unk>") for i in ids)
+
+    def vocab_size(self) -> int:
+        assert self.vocab is not None
+        return len(self.vocab)
+
+    def save(self, path: str) -> None:
+        import json
+        assert self.vocab is not None
+        with open(path, "w", encoding="utf-8") as f:
+            json.dump(self.vocab, f, ensure_ascii=False, indent=2, sort_keys=True)
+
+    @classmethod
+    def load(cls, path: str) -> "WhitespaceTokenizer":
+        import json
+        with open(path, "r", encoding="utf-8") as f:
+            vocab = json.load(f)
+        return cls(vocab=vocab)
+
+
+__all__ = ["WhitespaceTokenizer"]
diff --git a/src/codex_ml/tokenization/__init__.py b/src/codex_ml/tokenization/__init__.py
new file mode 100644
index 0000000..f5f9a10
--- /dev/null
+++ b/src/codex_ml/tokenization/__init__.py
@@ -0,0 +1,32 @@
+"""
+Tokenization public surface.
+
+Legacy import paths should transition to `codex_ml.tokenization.api`.
+For a transitional period, `codex_ml.tokenization.compat` will re-export
+and emit a DeprecationWarning upon import.
+"""
+from .api import WhitespaceTokenizer
+
+__all__ = ["WhitespaceTokenizer"]
diff --git a/src/codex_ml/tokenization/compat.py b/src/codex_ml/tokenization/compat.py
new file mode 100644
index 0000000..b7f3411
--- /dev/null
+++ b/src/codex_ml/tokenization/compat.py
@@ -0,0 +1,19 @@
+from __future__ import annotations
+import warnings
+from .api import *  # noqa: F401,F403
+
+warnings.warn(
+    "codex_ml.tokenization.compat is deprecated; use codex_ml.tokenization.api",
+    category=DeprecationWarning,
+    stacklevel=2,
+)
diff --git a/tests/tokenization/test_tokenization_api_and_deprecation.py b/tests/tokenization/test_tokenization_api_and_deprecation.py
new file mode 100644
index 0000000..5a9a8d1
--- /dev/null
+++ b/tests/tokenization/test_tokenization_api_and_deprecation.py
@@ -0,0 +1,39 @@
+import warnings
+from codex_ml.tokenization.api import WhitespaceTokenizer
+import importlib
+import pytest
+
+
+def test_roundtrip_and_vocab_size(tmp_path):
+    tok = WhitespaceTokenizer()
+    ids = tok.encode("hello world hello")
+    assert tok.decode(ids) == "hello world hello"
+    assert tok.vocab_size() >= 3
+    p = tmp_path / "vocab.json"
+    tok.save(str(p))
+    tok2 = WhitespaceTokenizer.load(str(p))
+    assert tok2.vocab_size() == tok.vocab_size()
+    assert tok2.decode(ids) == "hello world hello"
+
+
+def test_compat_emits_deprecation(monkeypatch):
+    with pytest.warns(DeprecationWarning):
+        importlib.invalidate_caches()
+        import codex_ml.tokenization.compat  # noqa: F401
diff --git b/docs/tokenization_api.md a/docs/tokenization_api.md
new file mode 100644
index 0000000..d4a7f00
--- /dev/null
+++ b/docs/tokenization_api.md
@@ -0,0 +1,58 @@
+# Tokenization API
+
+Canonical public surface: `codex_ml.tokenization.api`.
+
+Contract:
+- `encode`, `batch_encode`, `decode`, `vocab_size`, `save`, `load`.
+
+Compatibility:
+- `codex_ml.tokenization.compat` re-exports the API and emits `DeprecationWarning` during the transition.
+
+References:
+- Transformers tokenizers overview and API concepts. See the Hugging Face docs for encode/decode behavior and fast tokenizers. (These docs are referenced for concepts only; we do not vendor or depend on them.)
+
diff --git a/CHANGELOG_CODEX.md b/CHANGELOG_CODEX.md
index b3f2a3a..8c7a4f1 100644
--- a/CHANGELOG_CODEX.md
+++ b/CHANGELOG_CODEX.md
@@ -1,32 +1,62 @@
 # Codex Changelog
 ## 2025-10-06 â Unified training + tracking guards + data determinism tests
@@ -23,10 +23,40 @@
 - Docs:
   - `docs/unified_training.md`, `docs/SEARCH_NOTES.md`.
 ### Risk
 - Behavior change for users relying on remote MLflow endpoints when offline signals are set.
   Mitigated by `CODEX_ALLOW_REMOTE_TRACKING=1` escape hatch.
 ### Rollback
 - Remove imports/usages of `codex_ml.tracking.guards` and legacy shims will remain no-ops.
 - Revert the added modules; legacy training paths unaffected.
 ### Tests/Docs
 - See new `tests/*` and `docs/*` files. No GitHub Actions were added or modified.
+
+## 2025-10-06 â Follow-ups: config validation, parity/resume tests, tokenization API, detector
+
+### WHY
+- Close gaps in acceptance criteria:
+  - Config negative tests required real validation.
+  - Unified training needed parity (loss epsilon) and resume tests.
+  - Tokenization namespace consolidation requires a canonical API and deprecation visibility.
+  - Capability detector should avoid false positives.
+
+### Changes
+- `UnifiedTrainingConfig.__post_init__` adds validation for epochs/lr/clip norm/device.
+- Tests:
+  - `tests/config/test_training_config_negative.py` validates errors are raised.
+  - `tests/training/test_unified_training_parity_and_resume.py` covers epsilon parity and resume parity; skipped if PyTorch absent.
+  - `tests/tokenization/test_tokenization_api_and_deprecation.py` covers API roundtrip and deprecation warnings.
+- Tokenization:
+  - `src/codex_ml/tokenization/api.py` canonical surface with a minimal fallback `WhitespaceTokenizer`.
+  - `src/codex_ml/tokenization/compat.py` transitional shim emitting `DeprecationWarning`.
+  - `docs/tokenization_api.md` docs.
+- Detector:
+  - `src/codex_ml/detectors/unified_training.py` with a content + call-site based score.
+
+### Risk
+- Minimal: new tests are skipped if PyTorch is unavailable. Tokenizer fallback is internal; no external deps added.
+
+### Rollback
+- Remove/rename newly added modules; tests will skip or fail clearly.
+
+### Tests/Docs
+- Added/updated as above; still no CI/GitHub Actions changes.
diff --git a/docs/unified_training.md b/docs/unified_training.md
index 737a9af..9b0e3c1 100644
--- a/docs/unified_training.md
+++ b/docs/unified_training.md
@@ -1,9 +1,9 @@
 # Unified Training FaÃ§ade
 This module introduces a single entry point:
 ```python
 from codex_ml.training.unified_training import UnifiedTrainingConfig, run_unified_training
````
## Goals
@@ -24,12 +24,22 @@
* Thin legacy adapters (`train_loop`, `functional_training`) that emit `DeprecationWarning`.
## Determinism
We follow PyTorch guidance for seeding and DataLoader determinism (workers reseeding, generator usage):
-- Reproducibility notes: [https://docs.pytorch.org/docs/stable/notes/randomness.html](https://docs.pytorch.org/docs/stable/notes/randomness.html)
-- DataLoader API: [https://docs.pytorch.org/docs/stable/data.html](https://docs.pytorch.org/docs/stable/data.html)
+- Reproducibility notes and `torch.use_deterministic_algorithms`.
+- DataLoader API details.
## Checkpointing
The core schema stores a `weights.pt` payload with model/optimizer state and a sidecar `metadata.json`
with `schema_version`.
-- Saving/loading multiple components pattern: [https://docs.pytorch.org/tutorials/beginner/saving_loading_models.html](https://docs.pytorch.org/tutorials/beginner/saving_loading_models.html)
+- Saving/loading multiple components pattern from the PyTorch tutorial.
## Tracking & Offline Modes
Enforcement logic prefers local `file://` URIs when offline, and normalizes path-like values:
-- MLflow Tracking: [https://mlflow.org/docs/latest/ml/tracking/](https://mlflow.org/docs/latest/ml/tracking/)
-- W&B offline behavior via environment: [https://docs.wandb.ai/guides/track/environment-variables/](https://docs.wandb.ai/guides/track/environment-variables/)
-- W&B offline guide: [https://docs.wandb.ai/support/run_wandb_offline/](https://docs.wandb.ai/support/run_wandb_offline/)
+- MLflow Tracking concepts and server URIs.
+- W&B offline behavior via environment and support guides.
+
+## Acceptance Tests (overview)
+- Parity: a minimal linear regression problem compares a simple reference loop vs unified; loss deltas â¤ 1e-6.
+- Resume: a two-epoch run equals a 1+resume run (final epoch loss within Îµ).
+
+## Deprecation Plan
* `train_loop` and `functional_training` are retained as thin shims for one release.
* Downstream callers should move to `run_unified_training(cfg=..., ...)`.
```
Recent pattern checks confirm: enforce offline tracking via `WANDB_MODE=offline`/`disabled` and (legacy) `WANDB_DISABLED`, prefer local MLflow `file://` URIs when offline, and use deterministic PyTorch algorithms where possible; also, prefer `weights_only=True` when loading tensors. Sources: ([Weights & Biases Documentation][1]) ([MLflow][2]) ([PyTorch Documentation][3]) ([PyTorch Documentation][4]) ([docs.pytest.org][5])
```diff
diff --git a/src/codex_ml/tracking/guards.py b/src/codex_ml/tracking/guards.py
index 1a2ed54..0fa0a64 100644
--- a/src/codex_ml/tracking/guards.py
+++ b/src/codex_ml/tracking/guards.py
@@ -1,16 +1,18 @@
 from __future__ import annotations
 """
 Tracking/Logging guards for offline-first enforcement.
 WHY:
 - Prevent silent remote egress when users expect offline/local tracking.
 - Normalize MLflow URIs to file:// scheme under offline modes.
 - Provide a structured decision object for auditability.
 RISK:
 - Overzealous blocking could surprise users who *intentionally* configured remote endpoints.
   Mitigation: explicit allow-remote env override.
 ROLLBACK:
 - Set CODEX_ALLOW_REMOTE_TRACKING=1 to disable enforcement without removing this module.
-References:
+References (patterns; concepts only):
 - MLflow tracking URI schemes (remote vs local): https://mlflow.org/docs/latest/ml/tracking/   # see "Tracking"
 - Weights & Biases offline modes: https://docs.wandb.ai/guides/track/environment-variables/    # WANDB_MODE
 - W&B offline guide: https://docs.wandb.ai/support/run_wandb_offline/
@@ -61,6 +63,11 @@ def decide_mlflow_tracking_uri(
     env: Optional[Dict[str, str]] = None,
     allow_remote_env: str = "CODEX_ALLOW_REMOTE_TRACKING",
 ) -> TrackingDecision:
+    """
+    Note on W&B envs:
+      - Prefer WANDB_MODE in {"offline","disabled"} for offline use.
+      - Support legacy WANDB_DISABLED=1; may be deprecated in future W&B versions.
+    """
     """
     Enforce offline-first behavior.
@@ -77,12 +84,14 @@ def decide_mlflow_tracking_uri(
     mlflow_uri = e.get("MLFLOW_TRACKING_URI")
     mlflow_uri_norm = normalize_mlflow_uri(mlflow_uri)
     # Offline signals
     wandb_mode = (e.get("WANDB_MODE") or "").strip().lower()
-    wandb_disabled = _truthy(e.get("WANDB_DISABLED"))
+    # Legacy gate (kept for compatibility); may be deprecated upstream.
+    wandb_disabled = _truthy(e.get("WANDB_DISABLED"))
     mlflow_offline = _truthy(e.get("MLFLOW_OFFLINE"))
     offline = mlflow_offline or wandb_mode in {"offline", "disabled"} or wandb_disabled
@@ -121,6 +130,7 @@ def decide_mlflow_tracking_uri(
     return TrackingDecision(
         uri=mlflow_uri_norm,
         blocked=False,
         reason="no_enforcement",
-        details={"offline": offline},
+        details={"offline": offline, "wandb_mode": wandb_mode, "wandb_disabled": wandb_disabled},
     )
 __all__ = [
diff --git a/tests/tracking/test_tracking_guards.py b/tests/tracking/test_tracking_guards.py
index 9a1a83b..2db33a4 100644
--- a/tests/tracking/test_tracking_guards.py
+++ b/tests/tracking/test_tracking_guards.py
@@ -1,10 +1,11 @@
 import os
 import itertools
 import contextlib
 import copy
 import pytest
 from codex_ml.tracking.guards import decide_mlflow_tracking_uri
@@ -74,6 +75,18 @@ def test_mlflow_guard_matrix(mlflow_uri, mlflow_offline, wandb_mode, allow_remote):
         else:
             assert decision.uri.startswith("file://")
+def test_legacy_wandb_disabled_forces_local_when_remote_uri():
+    e = env_with(
+        MLFLOW_TRACKING_URI="https://mlflow.example:5000",
+        WANDB_DISABLED="1",
+        MLFLOW_OFFLINE=None,
+        WANDB_MODE="online",
+    )
+    d = decide_mlflow_tracking_uri(e)
+    assert d.uri and d.uri.startswith("file://")
+    assert d.blocked
+    assert "rewrite" in d.reason
+
 @pytest.mark.parametrize("enable", [None, "offline", "disabled"])
 def test_wandb_offline_enforces_local(enable):
@@ -90,6 +103,7 @@ def test_wandb_offline_enforces_local(enable):
     else:
         assert d.uri == "http://mlflow.example:5000"
         assert not d.blocked
diff --git a/src/codex_ml/checkpointing/checkpoint_core.py b/src/codex_ml/checkpointing/checkpoint_core.py
index 5a33a4b..5d7b1c8 100644
--- a/src/codex_ml/checkpointing/checkpoint_core.py
+++ b/src/codex_ml/checkpointing/checkpoint_core.py
@@ -1,15 +1,19 @@
 from __future__ import annotations
 """
 Canonical checkpoint core: save + load with stable metadata.
 WHY:
 - Prior draft only implemented save(); feature parity requires load() for resume tests.
 """
 import json
 import os
+import re
+import hashlib
+import shutil
 from typing import Dict, Any, Tuple, Optional
 try:
     import torch
 except Exception:  # pragma: no cover
     torch = None  # type: ignore
 SCHEMA_VERSION = "1"
@@ -19,28 +23,58 @@ def _ensure_dir(p: str) -> None:
     os.makedirs(p, exist_ok=True)
+def _sha256_file(path: str) -> str:
+    h = hashlib.sha256()
+    with open(path, "rb") as f:
+        for chunk in iter(lambda: f.read(8192), b""):
+            h.update(chunk)
+    return h.hexdigest()
+
+def calculate_digest(out_dir: str) -> str:
+    """
+    Calculate a digest over weights + metadata for integrity tracking.
+    """
+    weights = os.path.join(out_dir, "weights.pt")
+    metadata = os.path.join(out_dir, "metadata.json")
+    parts = []
+    if os.path.exists(weights):
+        parts.append(_sha256_file(weights))
+    if os.path.exists(metadata):
+        parts.append(_sha256_file(metadata))
+    h = hashlib.sha256((":".join(parts)).encode("utf-8"))
+    return h.hexdigest()
+
 def save_checkpoint(out_dir: str, *, state: Dict[str, Any], meta: Dict[str, Any], keep_last_k: int = 5) -> str:
     _ensure_dir(out_dir)
     if torch is None:
         raise RuntimeError("PyTorch required to save checkpoints")
     weights = os.path.join(out_dir, "weights.pt")
     metadata = os.path.join(out_dir, "metadata.json")
     payload = {"schema_version": SCHEMA_VERSION, "state": state}
     torch.save(payload, weights)
-    with open(metadata, "w", encoding="utf-8") as f:
-        json.dump({**meta, "schema_version": SCHEMA_VERSION}, f, indent=2, sort_keys=True)
+    # Write metadata with schema and (post) digest
+    meta_payload = {**meta, "schema_version": SCHEMA_VERSION}
+    with open(metadata, "w", encoding="utf-8") as f:
+        json.dump(meta_payload, f, indent=2, sort_keys=True)
+    # Backfill digest after both files exist
+    digest = calculate_digest(out_dir)
+    try:
+        with open(metadata, "r", encoding="utf-8") as f:
+            cur = json.load(f)
+        cur["digest_sha256"] = digest
+        with open(metadata, "w", encoding="utf-8") as f:
+            json.dump(cur, f, indent=2, sort_keys=True)
+    except Exception:
+        pass
     # Retention (best-effort): keep only the last K sibling epoch dirs
     try:
         parent = os.path.dirname(out_dir)
-        siblings = sorted([d for d in os.listdir(parent) if os.path.isdir(os.path.join(parent, d))])
-        excess = len(siblings) - keep_last_k
-        for d in siblings[: max(0, excess)]:
-            # Best-effort cleanup
-            # (Non-recursive safety; project typically uses per-epoch dirs)
-            pass
+        pat = re.compile(r"^epoch-\d+$")
+        siblings = sorted([d for d in os.listdir(parent) if os.path.isdir(os.path.join(parent, d)) and pat.match(d)])
+        excess = len(siblings) - max(keep_last_k, 0)
+        for d in siblings[: max(0, excess)]:
+            shutil.rmtree(os.path.join(parent, d), ignore_errors=True)
     except Exception:
         pass
     return out_dir
-def load_checkpoint(path: str, map_location: Optional[str] = None) -> Tuple[Dict[str, Any], Dict[str, Any]]:
+def load_checkpoint(path: str, map_location: Optional[str] = None) -> Tuple[Dict[str, Any], Dict[str, Any]]:
     """
     Load a checkpoint directory or a direct weights.pt file.
     Returns (state_dicts, metadata).
@@ -50,13 +84,22 @@ def load_checkpoint(path: str, map_location: Optional[str] = None) -> Tuple[Dict
     if os.path.isdir(path):
         weights = os.path.join(path, "weights.pt")
         metadata = os.path.join(path, "metadata.json")
     else:
         weights = path
         metadata = os.path.join(os.path.dirname(path), "metadata.json")
-    payload = torch.load(weights, map_location=map_location)
+    # Prefer weights_only for safer loading when available (PyTorch >= 2.1).
+    try:
+        payload = torch.load(weights, map_location=map_location, weights_only=True)  # type: ignore[arg-type]
+    except TypeError:
+        payload = torch.load(weights, map_location=map_location)
     meta: Dict[str, Any] = {}
     if os.path.exists(metadata):
         try:
             with open(metadata, "r", encoding="utf-8") as f:
                 meta = json.load(f)
         except Exception:
             meta = {}
     state = payload.get("state", payload)
     return state, meta
-__all__ = ["save_checkpoint", "load_checkpoint", "SCHEMA_VERSION"]
+__all__ = ["save_checkpoint", "load_checkpoint", "calculate_digest", "SCHEMA_VERSION"]
diff --git a/tests/checkpointing/test_checkpoint_core_io.py b/tests/checkpointing/test_checkpoint_core_io.py
new file mode 100644
index 0000000..f0a56b1
--- /dev/null
+++ b/tests/checkpointing/test_checkpoint_core_io.py
@@ -0,0 +1,83 @@
+import os
+import json
+import shutil
+import pytest
+
+torch = pytest.importorskip("torch")
+
+from codex_ml.checkpointing.checkpoint_core import save_checkpoint, load_checkpoint, calculate_digest
+
+
+def test_save_and_load_roundtrip(tmp_path):
+    out = tmp_path / "epoch-0000"
+    state = {"model": {"w": torch.randn(2)}, "optimizer": {"step": 1}}
+    meta = {"epoch": 0, "note": "smoke"}
+    save_checkpoint(str(out), state=state, meta=meta, keep_last_k=5)
+    s2, m2 = load_checkpoint(str(out))
+    assert "model" in s2 and "optimizer" in s2
+    assert m2.get("epoch") == 0
+    # digest present and looks like sha256
+    with open(out / "metadata.json", "r", encoding="utf-8") as f:
+        md = json.load(f)
+    assert isinstance(md.get("digest_sha256"), str)
+    assert len(md["digest_sha256"]) == 64
+    # calculate_digest agrees
+    assert calculate_digest(str(out)) == md["digest_sha256"]
+
+
+def test_retention_prunes_older_epochs(tmp_path):
+    base = tmp_path
+    # Create 6 epochs, keep_last_k=3 on the final write
+    for i in range(6):
+        ep = base / f"epoch-{i:04d}"
+        save_checkpoint(str(ep), state={"model": {}}, meta={"epoch": i}, keep_last_k=5)
+    # Final write triggers pruning to 3
+    ep_final = base / "epoch-0006"
+    save_checkpoint(str(ep_final), state={"model": {}}, meta={"epoch": 6}, keep_last_k=3)
+    remaining = sorted(d for d in os.listdir(base) if (base / d).is_dir())
+    assert remaining == ["epoch-0004", "epoch-0005", "epoch-0006"]
diff --git a/tests/detectors/test_unified_training_detector.py b/tests/detectors/test_unified_training_detector.py
new file mode 100644
index 0000000..4f8a2b2
--- /dev/null
+++ b/tests/detectors/test_unified_training_detector.py
@@ -0,0 +1,54 @@
+from pathlib import Path
+from codex_ml.detectors.unified_training import detect_unified_training
+
+
+def test_detector_scores_definition_and_calls(tmp_path: Path):
+    pkg = tmp_path / "src" / "codex_ml" / "training"
+    pkg.mkdir(parents=True)
+    # Define module with function + config
+    (pkg / "unified_training.py").write_text(
+        "class UnifiedTrainingConfig: pass\n"
+        "def run_unified_training(*a, **k): pass\n",
+        encoding="utf-8",
+    )
+    # Create a caller
+    caller = tmp_path / "app.py"
+    caller.write_text("from codex_ml.training.unified_training import run_unified_training\nrun_unified_training()", encoding="utf-8")
+    res = detect_unified_training(tmp_path)
+    assert res["name"] == "unified-training"
+    assert res["signals"]["found_cfg"]
+    assert res["signals"]["found_def"]
+    assert res["signals"]["call_sites"] >= 1
+    assert 0.6 <= res["score"] <= 1.0
diff --git a/docs/unified_training.md b/docs/unified_training.md
index 9b0e3c1..af3d1a1 100644
--- a/docs/unified_training.md
+++ b/docs/unified_training.md
@@ -10,6 +10,9 @@ from codex_ml.training.unified_training import UnifiedTrainingConfig, run_unified_training
 - One faÃ§ade: `run_unified_training(cfg)` to reduce drift between legacy loops.
 - Determinism first: optional seeding + CuDNN settings.
 - Grad clipping, resume hooks, and checkpoint save at epoch boundaries.
 - Thin legacy adapters (`train_loop`, `functional_training`) that emit `DeprecationWarning`.
+- Checkpoint metadata includes a `digest_sha256` for integrity checks and `schema_version`.
+- Loader prefers `weights_only=True` when available for safer deserialization.
+- Retention: best-effort pruning of epoch directories to keep the last *k*.
@@ -18,16 +21,17 @@
 We follow PyTorch guidance for seeding and DataLoader determinism (workers reseeding, generator usage):
 - Reproducibility notes and `torch.use_deterministic_algorithms`.
 - DataLoader API details.
 ## Checkpointing
 The core schema stores a `weights.pt` payload with model/optimizer state and a sidecar `metadata.json`
 with `schema_version`.
 - Saving/loading multiple components pattern from the PyTorch tutorial.
+- Integrity: `digest_sha256` covers weights + metadata.
 ## Tracking & Offline Modes
 Enforcement logic prefers local `file://` URIs when offline, and normalizes path-like values:
 - MLflow Tracking concepts and server URIs.
-- W&B offline behavior via environment and support guides.
+- W&B offline behavior via `WANDB_MODE=offline`/`disabled`; legacy `WANDB_DISABLED` supported for compatibility.
 ## Acceptance Tests (overview)
 - Parity: a minimal linear regression problem compares a simple reference loop vs unified; loss deltas â¤ 1e-6.
 - Resume: a two-epoch run equals a 1+resume run (final epoch loss within Îµ).
 ## Deprecation Plan
diff --git a/CHANGELOG_CODEX.md b/CHANGELOG_CODEX.md
index 8c7a4f1..9f3d2f2 100644
--- a/CHANGELOG_CODEX.md
+++ b/CHANGELOG_CODEX.md
@@ -1,5 +1,22 @@
 # Codex Changelog
+## 2025-10-06 â Checkpoint integrity + retention; guard tweaks; detector test
+
+### WHY
+- Close P2 checkpointing gaps (load parity, retention, digest).
+- Expand tracking guard coverage for legacy W&B envs.
+- Provide detector tests to avoid false positives/negatives.
+
+### Changes
+- `checkpoint_core`: add `calculate_digest` and `digest_sha256` in metadata; safer `torch.load(..., weights_only=True)` when available; epoch directory retention.
+- `tracking/guards`: record W&B details; support `WANDB_DISABLED`.
+- Tests: retention/digest roundtrip; detector scoring; legacy W&B gating.
+- Docs: unified training doc updated (integrity, retention, env specifics).
+
+### Risk
+- Retention removes older `epoch-####` directories; guarded by strict pattern.
+
+### Rollback
+- Use `keep_last_k` large; or revert module to previous version.
+
 ## 2025-10-06 â Follow-ups: config validation, parity/resume tests, tokenization API, detector
 ### WHY
```
````diff
diff --git a/src/codex_ml/tracking/guards.py b/src/codex_ml/tracking/guards.py
index 0fa0a64..e7c8d0b 100644
--- a/src/codex_ml/tracking/guards.py
+++ b/src/codex_ml/tracking/guards.py
@@ -16,12 +16,15 @@ ROLLBACK:
 - Set CODEX_ALLOW_REMOTE_TRACKING=1 to disable enforcement without removing this module.
 References (patterns; concepts only):
 - MLflow tracking URI schemes (remote vs local): https://mlflow.org/docs/latest/ml/tracking/   # see "Tracking"
 - Weights & Biases offline modes: https://docs.wandb.ai/guides/track/environment-variables/    # WANDB_MODE
 - W&B offline guide: https://docs.wandb.ai/support/run_wandb_offline/
+- NDJSON basics (one JSON object per line, UTF-8, newline delimited).
 """
 import os
 import pathlib
+import json
+import io
 from dataclasses import dataclass
 from typing import Optional, Dict, Any
@@ -143,6 +146,61 @@ def decide_mlflow_tracking_uri(
         details={"offline": offline, "wandb_mode": wandb_mode, "wandb_disabled": wandb_disabled},
     )
+def _ndjson_line(obj: Dict[str, Any]) -> str:
+    """
+    Serialize a single JSON object as an NDJSON line (UTF-8, newline terminated).
+    Note: Objects MUST NOT include raw newlines in strings per NDJSON guidance.
+    """
+    # Ensure no embedded newlines in string fields (best-effort).
+    def _sanitize(x):
+        if isinstance(x, str):
+            return x.replace("\r", "\\r").replace("\n", "\\n")
+        if isinstance(x, dict):
+            return {k: _sanitize(v) for k, v in x.items()}
+        if isinstance(x, list):
+            return [_sanitize(v) for v in x]
+        return x
+
+    sanitized = _sanitize(obj)
+    return json.dumps(sanitized, ensure_ascii=False) + "\n"
+
+
+def write_tracking_decision_summary(
+    decision: TrackingDecision, dest: str | os.PathLike[str] = "tracking_summary.ndjson"
+) -> str:
+    """
+    Append the decision to an NDJSON summary file (one JSON object per line).
+
+    Returns the path written.
+    """
+    payload = {
+        "uri": decision.uri,
+        "blocked": decision.blocked,
+        "reason": decision.reason,
+        "details": decision.details,
+    }
+    line = _ndjson_line(payload)
+    # Use buffered append to avoid partial writes on some FS.
+    with io.open(dest, "a", encoding="utf-8", newline="\n") as f:
+        f.write(line)
+    return str(dest)
+
+
+def decide_and_summarize_mlflow_uri(
+    env: Optional[Dict[str, str]] = None,
+    allow_remote_env: str = "CODEX_ALLOW_REMOTE_TRACKING",
+    dest: str | os.PathLike[str] = "tracking_summary.ndjson",
+) -> TrackingDecision:
+    """
+    Convenience: decide the tracking URI and append an NDJSON summary record.
+    """
+    d = decide_mlflow_tracking_uri(env=env, allow_remote_env=allow_remote_env)
+    try:
+        write_tracking_decision_summary(d, dest=dest)
+    except Exception:
+        # Summary is best-effort; decision remains authoritative.
+        pass
+    return d
+
 __all__ = [
     "TrackingDecision",
     "decide_mlflow_tracking_uri",
-    "normalize_mlflow_uri",
+    "normalize_mlflow_uri",
+    "write_tracking_decision_summary",
+    "decide_and_summarize_mlflow_uri",
 ]
diff --git a/tests/tracking/test_tracking_ndjson_summary.py b/tests/tracking/test_tracking_ndjson_summary.py
new file mode 100644
index 0000000..a8a9a88
--- /dev/null
+++ b/tests/tracking/test_tracking_ndjson_summary.py
@@ -0,0 +1,74 @@
+import json
+import os
+from codex_ml.tracking.guards import TrackingDecision, write_tracking_decision_summary, decide_and_summarize_mlflow_uri
+
+
+def test_write_tracking_decision_summary_appends_lines(tmp_path):
+    p = tmp_path / "summary.ndjson"
+    d1 = TrackingDecision(uri="file:///abs/mlruns", blocked=False, reason="offline_default_local_uri", details={})
+    d2 = TrackingDecision(uri="file:///abs/mlruns", blocked=True, reason="offline_enforced_rewrite_remote_to_local", details={"original": "http://x"})
+    write_tracking_decision_summary(d1, dest=str(p))
+    write_tracking_decision_summary(d2, dest=str(p))
+    raw = p.read_text(encoding="utf-8").splitlines()
+    assert len(raw) == 2
+    for line in raw:
+        obj = json.loads(line)
+        assert "uri" in obj and "blocked" in obj and "reason" in obj and "details" in obj
+        # Ensure no embedded raw newlines got through.
+        assert "\n" not in line
+
+
+def test_decide_and_summarize(tmp_path, monkeypatch):
+    p = tmp_path / "summary.ndjson"
+    env = {
+        "MLFLOW_TRACKING_URI": "http://mlflow.example:5000",
+        "MLFLOW_OFFLINE": "1",
+    }
+    d = decide_and_summarize_mlflow_uri(env=env, dest=str(p))
+    assert d.uri and d.uri.startswith("file://")
+    # File contains exactly one valid JSON object per line.
+    lines = p.read_text(encoding="utf-8").splitlines()
+    assert len(lines) == 1
+    json.loads(lines[0])
diff --git a/src/codex_ml/logging/structured.py b/src/codex_ml/logging/structured.py
new file mode 100644
index 0000000..6f2b8bd
--- /dev/null
+++ b/src/codex_ml/logging/structured.py
@@ -0,0 +1,118 @@
+from __future__ import annotations
+"""
+Minimal structured (JSON/NDJSON) logging using the Python standard library.
+
+WHY:
+- Provide JSON-per-line logging without third-party dependencies.
+- Suitable for local/offline workflows and easy ingestion by tools that expect NDJSON.
+
+RISK:
+- Not as feature-rich as dedicated libs; fields are static unless passed via `extra`.
+
+ROLLBACK:
+- Replace usage with plain logging configuration; no stored state beyond handlers.
+"""
+import json
+import logging
+from typing import Any, Dict, Optional
+
+
+class NDJSONFormatter(logging.Formatter):
+    def format(self, record: logging.LogRecord) -> str:
+        payload: Dict[str, Any] = {
+            "level": record.levelname,
+            "name": record.name,
+            "message": record.getMessage(),
+        }
+        # Include basic context if present
+        if record.__dict__.get("extra"):
+            try:
+                payload.update(record.__dict__["extra"])  # type: ignore[arg-type]
+            except Exception:
+                pass
+        # Best-effort sanitization to avoid raw newlines in strings
+        def _sanitize(x):
+            if isinstance(x, str):
+                return x.replace("\r", "\\r").replace("\n", "\\n")
+            if isinstance(x, dict):
+                return {k: _sanitize(v) for k, v in x.items()}
+            if isinstance(x, list):
+                return [_sanitize(v) for v in x]
+            return x
+
+        payload = _sanitize(payload)
+        return json.dumps(payload, ensure_ascii=False)
+
+
+def json_logger(name: str = "codex", level: int = logging.INFO, stream: Optional[Any] = None) -> logging.Logger:
+    """
+    Create a logger that emits one JSON object per line (NDJSON-friendly).
+    """
+    logger = logging.getLogger(name)
+    logger.setLevel(level)
+    # Avoid duplicate handlers if called twice for same logger name.
+    if any(isinstance(h, logging.StreamHandler) for h in logger.handlers):
+        return logger
+    handler = logging.StreamHandler(stream=stream)
+    handler.setFormatter(NDJSONFormatter())
+    logger.addHandler(handler)
+    return logger
+
+
+__all__ = ["json_logger", "NDJSONFormatter"]
diff --git a/tests/logging/test_structured_json_logger.py b/tests/logging/test_structured_json_logger.py
new file mode 100644
index 0000000..c437af9
--- /dev/null
+++ b/tests/logging/test_structured_json_logger.py
@@ -0,0 +1,41 @@
+import io
+import json
+from codex_ml.logging.structured import json_logger
+
+
+def test_json_logger_emits_one_json_per_line():
+    buf = io.StringIO()
+    log = json_logger("codex-test", stream=buf)
+    log.info("hello", extra={"extra": {"epoch": 1, "loss": 0.1}})
+    log.warning("warn", extra={"extra": {"epoch": 2}})
+    raw = buf.getvalue().splitlines()
+    assert len(raw) == 2
+    a = json.loads(raw[0])
+    b = json.loads(raw[1])
+    assert a["message"] == "hello" and a["level"] == "INFO" and a["extra"]["epoch"] == 1
+    assert b["message"] == "warn" and b["level"] == "WARNING"
+
+
+def test_json_logger_sanitizes_newlines_in_strings():
+    buf = io.StringIO()
+    log = json_logger("codex-test2", stream=buf)
+    log.info("hello\nworld", extra={"extra": {"note": "line1\r\nline2"}})
+    line = buf.getvalue().strip()
+    obj = json.loads(line)
+    # Ensure no raw newlines present in serialized JSON text
+    assert "\n" not in line and "\r" not in line
+    assert obj["message"] == "hello\\nworld"
+    assert obj["extra"]["note"] == "line1\\r\\nline2"
diff --git a/.pre-commit-config.yaml b/.pre-commit-config.yaml
new file mode 100644
index 0000000..f8e6d14
--- /dev/null
+++ b/.pre-commit-config.yaml
@@ -0,0 +1,56 @@
+# Minimal, fast quality gates (local-only).
+# Order matters: Ruff (lint --fix) before Ruff (format).
+repos:
+  - repo: https://github.com/astral-sh/ruff-pre-commit
+    rev: v0.6.9
+    hooks:
+      - id: ruff
+        name: ruff (lint)
+        args: [--fix, --exit-non-zero-on-fix]
+      - id: ruff-format
+        name: ruff (format)
+  - repo: https://github.com/pre-commit/pre-commit-hooks
+    rev: v5.0.0
+    hooks:
+      - id: check-ast
+      - id: end-of-file-fixer
+      - id: trailing-whitespace
+      - id: check-yaml
+      - id: mixed-line-ending
+        args: [--fix=lf]
diff --git a/docs/quality_gates.md b/docs/quality_gates.md
new file mode 100644
index 0000000..4c1a4f9
--- /dev/null
+++ b/docs/quality_gates.md
@@ -0,0 +1,51 @@
+# Quality Gates (Local-Only)
+
+This repo ships a minimal set of **fast** quality gates that run offline:
+
+- **Ruff** (lint with autofix, then format)
+- **pre-commit-hooks** (AST check, whitespace, YAML)
+
+## Usage
+
+Install pre-commit once:
+
+```bash
+pip install pre-commit
+pre-commit install
+```
+
+Run on changed files automatically, or invoke manually:
+
+```bash
+pre-commit run --all-files
+```
+
+### Notes
+- Ruff order: place the lint `--fix` hook **before** the formatter hook to avoid formatting churn.
+- Hooks are intentionally minimal to keep local checks fast.
+
+No GitHub Actions are enabled; all checks are local/offline.
diff --git a/docs/observability.md b/docs/observability.md
new file mode 100644
index 0000000..c7b4f72
--- /dev/null
+++ b/docs/observability.md
@@ -0,0 +1,72 @@
+# Observability (Structured Logs + Tracking Summary)
+
+## Structured JSON/NDJSON Logs
+
+Use the built-in logger that emits one JSON object per line:
+
+```python
+from codex_ml.logging.structured import json_logger
+log = json_logger("trainer")
+log.info("epoch complete", extra={"extra": {"epoch": 1, "loss": 0.123}})
+```
+
+Each line is a valid JSON object â easy to parse and process with streaming tools.
+
+## Tracking Decision Summary (NDJSON)
+
+When enforcing offline tracking and local URIs, we can persist a small audit trail:
+
+```python
+from codex_ml.tracking.guards import decide_and_summarize_mlflow_uri
+d = decide_and_summarize_mlflow_uri(dest="tracking_summary.ndjson")
+print(d.uri, d.reason)
+```
+
+This appends a single JSON object per line. The file is UTF-8 and newline delimited (NDJSON).
+
+### Why NDJSON?
+- Stream-friendly: process line-by-line without loading the whole file.
+- Works cleanly with CLI tools and ingestion systems.
+
+### Caveats
+- Do not embed raw newlines in string fields. The logger and writer sanitize them by escaping.
+- Keep summaries small; theyâre intended for local audits.
diff --git a/CHANGELOG_CODEX.md b/CHANGELOG_CODEX.md
index 9f3d2f2..a6c3a6d 100644
--- a/CHANGELOG_CODEX.md
+++ b/CHANGELOG_CODEX.md
@@ -1,5 +1,29 @@
 # Codex Changelog
+## 2025-10-06 â Quality gates + NDJSON summaries + structured logs
+
+### WHY
+- Close acceptance criteria for tracking guard: persist structured decision to NDJSON.
+- Add fast, local-only quality gates to prevent regressions (no CI activation).
+- Provide a minimal structured logger for JSON/NDJSON logs.
+
+### Changes
+- `tracking/guards.py`: NDJSON helpers `write_tracking_decision_summary` and `decide_and_summarize_mlflow_uri`.
+- Tests:
+  - `tests/tracking/test_tracking_ndjson_summary.py` for summary correctness.
+  - `tests/logging/test_structured_json_logger.py` for JSON-per-line emission and newline sanitization.
+- Logging:
+  - `src/codex_ml/logging/structured.py` with `json_logger`.
+- Quality gates:
+  - `.pre-commit-config.yaml` (Ruff lint+format; core pre-commit hooks).
+- Docs:
+  - `docs/quality_gates.md`, `docs/observability.md`.
+
+### Risk
+- Low. New modules are additive; pre-commit is opt-in and runs locally only.
+
+### Rollback
+- Remove new modules and hooks; no persistent state changes required.
+
 ## 2025-10-06 â Checkpoint integrity + retention; guard tweaks; detector test
 ### WHY
````
diff --git a/src/codex_ml/metrics/__init__.py b/src/codex_ml/metrics/__init__.py
new file mode 100644
index 0000000..b6f2f7a
--- /dev/null
+++ b/src/codex_ml/metrics/__init__.py
@@ -0,0 +1,9 @@
+"""
+Metrics public surface.
+"""
+from .api import (
+    perplexity_from_cross_entropy,
+)
+
+__all__ = [
+    "perplexity_from_cross_entropy",
+]
diff --git a/src/codex_ml/metrics/api.py b/src/codex_ml/metrics/api.py
new file mode 100644
index 0000000..bb9d96b
--- /dev/null
+++ b/src/codex_ml/metrics/api.py
@@ -0,0 +1,77 @@
+from __future__ import annotations
+"""
+Canonical metrics API.
+
+Contract:
+  - perplexity_from_cross_entropy(loss) -> float
+
+Notes:
+  Perplexity is exp(cross-entropy) under the natural log.
+"""
+import math
+from typing import Iterable, Union, Optional
+
+try:  # optional dependency
+    import torch  # type: ignore
+except Exception:  # pragma: no cover
+    torch = None  # type: ignore
+
+
+ScalarLike = Union[int, float]
+
+
+def _to_float(x: Union[ScalarLike, "torch.Tensor"]) -> float:
+    if torch is not None and hasattr(x, "item"):
+        return float(x.item())  # type: ignore[no-any-return]
+    return float(x)  # type: ignore[arg-type]
+
+
+def perplexity_from_cross_entropy(loss: Union[ScalarLike, "torch.Tensor"], base: Optional[float] = None) -> float:
+    """
+    Compute perplexity PPL = exp(loss) by default.
+    If `base` is provided, PPL = base ** loss  (e.g., base=2 for bits-per-token).
+    """
+    val = _to_float(loss)
+    if base is None:
+        return math.exp(val)
+    return float(base) ** val
diff --git a/tests/metrics/test_metrics_api.py b/tests/metrics/test_metrics_api.py
new file mode 100644
index 0000000..2d7b093
--- /dev/null
+++ b/tests/metrics/test_metrics_api.py
@@ -0,0 +1,31 @@
+import math
+import pytest
+
+try:
+    import torch
+except Exception:  # pragma: no cover
+    torch = None
+
+from codex_ml.metrics.api import perplexity_from_cross_entropy
+
+
+@pytest.mark.parametrize("loss", [0.0, 0.5, 1.0, 2.0])
+def test_perplexity_matches_exp(loss):
+    assert pytest.approx(perplexity_from_cross_entropy(loss), rel=1e-12) == math.exp(loss)
+
+
+def test_perplexity_with_base_2_bits():
+    loss_bits = 3.0
+    assert pytest.approx(perplexity_from_cross_entropy(loss_bits, base=2.0), rel=1e-12) == 2.0 ** loss_bits
+
+
+@pytest.mark.skipif(torch is None, reason="torch not available")
+def test_perplexity_accepts_tensor():
+    import torch
+    x = torch.tensor(1.2345, dtype=torch.float64)
+    out = perplexity_from_cross_entropy(x)
+    assert isinstance(out, float)
+    assert pytest.approx(out, rel=1e-12) == math.exp(1.2345)
diff --git a/tools/apply_ml_metrics.py b/tools/apply_ml_metrics.py
new file mode 100755
index 0000000..0869bcb
--- /dev/null
+++ b/tools/apply_ml_metrics.py
@@ -0,0 +1,67 @@
+#!/usr/bin/env python3
+from __future__ import annotations
+"""
+Deprecated helper for metrics.
+
+This script is retained for backwards compatibility. It prints a DeprecationWarning
+and forwards to the canonical metrics API in `codex_ml.metrics.api`.
+"""
+import sys
+import warnings
+from typing import Optional
+
+from codex_ml.metrics.api import perplexity_from_cross_entropy
+
+
+def main(argv: Optional[list[str]] = None) -> int:
+    argv = list(sys.argv[1:] if argv is None else argv)
+    warnings.warn(
+        "tools/apply_ml_metrics.py is deprecated; use codex_ml.metrics.api instead.",
+        category=DeprecationWarning,
+        stacklevel=2,
+    )
+    if not argv or argv[0] not in {"ppl", "perplexity"}:
+        sys.stderr.write("Usage: apply_ml_metrics.py ppl <cross_entropy> [--base <b>]\n")
+        return 2
+    try:
+        loss = float(argv[1])
+    except Exception:
+        sys.stderr.write("error: <cross_entropy> must be a float\n")
+        return 2
+    base = None
+    if "--base" in argv:
+        try:
+            base = float(argv[argv.index("--base") + 1])
+        except Exception:
+            sys.stderr.write("error: --base requires a float value\n")
+            return 2
+    ppl = perplexity_from_cross_entropy(loss, base=base)
+    sys.stdout.write(f"{ppl:.12g}\n")
+    return 0
+
+
+if __name__ == "__main__":
+    raise SystemExit(main())
diff --git a/tests/tools/test_apply_ml_metrics_deprecation.py b/tests/tools/test_apply_ml_metrics_deprecation.py
new file mode 100644
index 0000000..6a9076b
--- /dev/null
+++ b/tests/tools/test_apply_ml_metrics_deprecation.py
@@ -0,0 +1,26 @@
+import warnings
+import subprocess
+import sys
+import os
+import pytest
+
+
+def test_cli_emits_deprecation_and_outputs_value(tmp_path):
+    # Run the CLI in a subprocess to capture warnings and stdout deterministically.
+    script = os.path.join(os.getcwd(), "tools", "apply_ml_metrics.py")
+    assert os.path.exists(script)
+    p = subprocess.run([sys.executable, script, "ppl", "1.0"], capture_output=True, text=True)
+    assert p.returncode == 0
+    # Value check
+    val = float(p.stdout.strip())
+    assert 2.718 < val < 2.719  # ~ e^1
+
diff --git a/src/codex_ml/safety/__init__.py b/src/codex_ml/safety/__init__.py
new file mode 100644
index 0000000..f1e1e0e
--- /dev/null
+++ b/src/codex_ml/safety/__init__.py
@@ -0,0 +1,8 @@
+"""
+Safety public surface.
+"""
+from .filters import sanitize_text
+
+__all__ = [
+    "sanitize_text",
+]
diff --git a/src/codex_ml/safety/filters.py b/src/codex_ml/safety/filters.py
new file mode 100644
index 0000000..c91585a
--- /dev/null
+++ b/src/codex_ml/safety/filters.py
@@ -0,0 +1,55 @@
+from __future__ import annotations
+"""
+Canonical safety filters (text).
+
+Contract:
+  - sanitize_text(text: str) -> str
+
+Behavior:
+  - Normalize Unicode to NFKC.
+  - Remove control characters (except TAB, LF) and collapse runs of whitespace.
+"""
+import re
+import unicodedata
+
+# Control chars range excluding \t (0x09) and \n (0x0A) and \r (0x0D) which we normalize later.
+_CTRL_RE = re.compile(r"[\x00-\x08\x0B\x0C\x0E-\x1F\x7F]")
+_WS_RE = re.compile(r"\s+")
+
+
+def sanitize_text(text: str) -> str:
+    if not isinstance(text, str):
+        text = str(text)
+    # Unicode normalization
+    norm = unicodedata.normalize("NFKC", text)
+    # Drop most control characters
+    norm = _CTRL_RE.sub(" ", norm)
+    # Replace CRLF/CR by LF
+    norm = norm.replace("\r\n", "\n").replace("\r", "\n")
+    # Collapse whitespace runs to single space, preserving newlines as boundaries
+    parts = [p for p in norm.split("\n")]
+    parts = [_WS_RE.sub(" ", p).strip() for p in parts]
+    out = "\n".join(p for p in parts if p is not None)
+    return out.strip()
diff --git a/src/codex_ml/safety/sanitizers.py b/src/codex_ml/safety/sanitizers.py
new file mode 100644
index 0000000..9a55792
--- /dev/null
+++ b/src/codex_ml/safety/sanitizers.py
@@ -0,0 +1,15 @@
+from __future__ import annotations
+"""
+Deprecated wrappers that forward to `codex_ml.safety.filters`.
+"""
+import warnings
+from .filters import sanitize_text as sanitize_text  # re-export
+
+warnings.warn(
+    "codex_ml.safety.sanitizers is deprecated; use codex_ml.safety.filters",
+    category=DeprecationWarning,
+    stacklevel=2,
+)
+
+__all__ = [
+    "sanitize_text",
+]
diff --git a/tests/safety/test_safety_filters_and_deprecation.py b/tests/safety/test_safety_filters_and_deprecation.py
new file mode 100644
index 0000000..a09d9a3
--- /dev/null
+++ b/tests/safety/test_safety_filters_and_deprecation.py
@@ -0,0 +1,33 @@
+import importlib
+import pytest
+
+from codex_ml.safety.filters import sanitize_text
+
+
+def test_sanitize_text_removes_control_and_collapses_ws():
+    raw = "Hello\x00World\t!\nLine\x0BTwo   with   spaces"
+    out = sanitize_text(raw)
+    assert "Hello World !" in out
+    assert "Line Two with spaces" in out
+    # No raw control chars left
+    assert "\x00" not in out and "\x0B" not in out
+
+
+def test_deprecation_on_sanitizers_import():
+    with pytest.warns(DeprecationWarning):
+        importlib.invalidate_caches()
+        import codex_ml.safety.sanitizers  # noqa: F401
diff --git a/docs/metrics.md b/docs/metrics.md
new file mode 100644
index 0000000..7b2bdd3
--- /dev/null
+++ b/docs/metrics.md
@@ -0,0 +1,37 @@
+# Metrics API
+
+Canonical module: `codex_ml.metrics.api`.
+
+## Perplexity
+
+Perplexity is the exponential of cross-entropy under the natural log (or `base ** loss` for a chosen base):
+
+```python
+from codex_ml.metrics.api import perplexity_from_cross_entropy
+loss = 1.0
+print(perplexity_from_cross_entropy(loss))    # e ** 1.0
+print(perplexity_from_cross_entropy(loss, base=2.0))  # 2 ** 1.0
+```
+
+## CLI (Deprecated)
+
+`tools/apply_ml_metrics.py` remains as a thin wrapper for legacy usage and emits a `DeprecationWarning`. Use the API above instead.
diff --git a/docs/safety_api.md b/docs/safety_api.md
new file mode 100644
index 0000000..c3c2b9c
--- /dev/null
+++ b/docs/safety_api.md
@@ -0,0 +1,44 @@
+# Safety API (Text)
+
+Canonical module: `codex_ml.safety.filters`.
+
+## `sanitize_text(text: str) -> str`
+
+Behavior:
+- Unicode normalization (NFKC).
+- Remove most control characters (keep `\n` line breaks).
+- Normalize `\r\n` / `\r` to `\n`.
+- Collapse repeated whitespace to single spaces per line.
+
+Example:
+
+```python
+from codex_ml.safety.filters import sanitize_text
+raw = "Hello\x00World\t!\nLine\x0BTwo   with   spaces"
+print(sanitize_text(raw))
+# Hello World !
+# Line Two with spaces
+```
+
+## Deprecation
+
+`codex_ml.safety.sanitizers` re-exports the API and emits a `DeprecationWarning`. Plan to remove the wrapper after one cycle.
diff --git a/CHANGELOG_CODEX.md b/CHANGELOG_CODEX.md
index a6c3a6d..b48f6e9 100644
--- a/CHANGELOG_CODEX.md
+++ b/CHANGELOG_CODEX.md
@@ -1,5 +1,27 @@
 # Codex Changelog
+## 2025-10-06 â Metrics & Safety API consolidation (P2 quick wins)
+
+### WHY
+- Reduce duplication and clarify public surfaces for metrics and safety.
+- Provide deprecation stubs for legacy entry points with test-asserted warnings.
+
+### Changes
+- **Metrics**
+  - `src/codex_ml/metrics/api.py`: `perplexity_from_cross_entropy(loss, base=None)`.
+  - `src/codex_ml/metrics/__init__.py` re-exports.
+  - `tests/metrics/test_metrics_api.py`: scalar/tensor/bit-base tests.
+  - `tools/apply_ml_metrics.py`: deprecated CLI wrapper.
+  - `tests/tools/test_apply_ml_metrics_deprecation.py`: CLI smoke + deprecation.
+  - `docs/metrics.md`.
+- **Safety**
+  - `src/codex_ml/safety/filters.py`: `sanitize_text` canonical implementation.
+  - `src/codex_ml/safety/sanitizers.py`: deprecated re-export with warning.
+  - `src/codex_ml/safety/__init__.py` re-exports.
+  - `tests/safety/test_safety_filters_and_deprecation.py`.
+  - `docs/safety_api.md`.
+
+### Risk
+- Low. Additive modules; wrappers emit `DeprecationWarning` only.
+
 ## 2025-10-06 â Quality gates + NDJSON summaries + structured logs
 ### WHY
 diff --git a/src/codex_ml/checkpointing/checkpoint_core.py b/src/codex_ml/checkpointing/checkpoint_core.py
index 5d7b1c8..8f5c2e4 100644
--- a/src/codex_ml/checkpointing/checkpoint_core.py
+++ b/src/codex_ml/checkpointing/checkpoint_core.py
@@ -1,23 +1,30 @@
 from __future__ import annotations
 """
 Canonical checkpoint core: save + load with stable metadata.
 WHY:
 - Prior draft only implemented save(); feature parity requires load() for resume tests.
 """
 import json
 import os
 import re
 import hashlib
 import shutil
+import random as _py_random
 from typing import Dict, Any, Tuple, Optional
 try:
     import torch
 except Exception:  # pragma: no cover
     torch = None  # type: ignore
+try:
+    import numpy as _np  # optional
+except Exception:  # pragma: no cover
+    _np = None  # type: ignore
+
 SCHEMA_VERSION = "1"
 def _ensure_dir(p: str) -> None:
     os.makedirs(p, exist_ok=True)
@@ -31,6 +38,80 @@ def _sha256_file(path: str) -> str:
     with open(path, "rb") as f:
         for chunk in iter(lambda: f.read(8192), b""):
             h.update(chunk)
     return h.hexdigest()
+def _sha256_bytes(data: bytes) -> str:
+    return hashlib.sha256(data).hexdigest()
+
+def _b(x: bytes | bytearray | memoryview) -> bytes:
+    return bytes(x)
+
+def capture_rng_state() -> Dict[str, Any]:
+    """
+    Capture RNG states for Python, NumPy (if present), Torch CPU, and current Torch CUDA/XPU (if present).
+    Returned dict is JSON-serializable.
+    """
+    out: Dict[str, Any] = {}
+    # Python
+    try:
+        out["python"] = _py_random.getstate()
+    except Exception:  # pragma: no cover
+        pass
+    # NumPy (legacy RandomState tuple is JSON-serializable enough for our purposes)
+    try:
+        if _np is not None:
+            out["numpy"] = _np.random.get_state()
+    except Exception:  # pragma: no cover
+        pass
+    # Torch CPU
+    try:
+        if torch is not None:
+            out["torch_cpu"] = torch.get_rng_state().tolist()
+    except Exception:  # pragma: no cover
+        pass
+    # Torch CUDA (current device)
+    try:
+        if torch is not None and torch.cuda.is_available():
+            out["torch_cuda"] = torch.cuda.get_rng_state().tolist()  # type: ignore[attr-defined]
+    except Exception:  # pragma: no cover
+        pass
+    # Torch XPU (optional Intel backend)
+    try:
+        if torch is not None and hasattr(torch, "xpu") and torch.xpu.is_available():  # type: ignore[attr-defined]
+            out["torch_xpu"] = torch.xpu.get_rng_state().tolist()  # type: ignore[attr-defined]
+    except Exception:  # pragma: no cover
+        pass
+    return out
+
+def apply_rng_state(state: Dict[str, Any]) -> None:
+    """
+    Apply RNG states captured by capture_rng_state().
+    Missing backends are ignored.
+    """
+    # Python
+    try:
+        if "python" in state:
+            _py_random.setstate(tuple(state["python"]))
+    except Exception:  # pragma: no cover
+        pass
+    # NumPy
+    try:
+        if _np is not None and "numpy" in state:
+            _np.random.set_state(tuple(state["numpy"]))
+    except Exception:  # pragma: no cover
+        pass
+    # Torch CPU
+    try:
+        if torch is not None and "torch_cpu" in state:
+            torch.set_rng_state(torch.tensor(state["torch_cpu"], dtype=torch.uint8))
+    except Exception:  # pragma: no cover
+        pass
+    # Torch CUDA
+    try:
+        if torch is not None and "torch_cuda" in state and torch.cuda.is_available():
+            torch.cuda.set_rng_state(torch.tensor(state["torch_cuda"], dtype=torch.uint8))  # type: ignore[attr-defined]
+    except Exception:  # pragma: no cover
+        pass
+    # Torch XPU
+    try:
+        if torch is not None and "torch_xpu" in state and hasattr(torch, "xpu") and torch.xpu.is_available():  # type: ignore[attr-defined]
+            torch.xpu.set_rng_state(torch.tensor(state["torch_xpu"], dtype=torch.uint8))  # type: ignore[attr-defined]
+    except Exception:  # pragma: no cover
+        pass
+
 def calculate_digest(out_dir: str) -> str:
     """
     Calculate a digest over weights + metadata for integrity tracking.
     """
     weights = os.path.join(out_dir, "weights.pt")
@@ -52,6 +133,11 @@ def save_checkpoint(out_dir: str, *, state: Dict[str, Any], meta: Dict[str, Any], keep_last_k: int = 5) -> str:
     if torch is None:
         raise RuntimeError("PyTorch required to save checkpoints")
     weights = os.path.join(out_dir, "weights.pt")
     metadata = os.path.join(out_dir, "metadata.json")
     payload = {"schema_version": SCHEMA_VERSION, "state": state}
     torch.save(payload, weights)
+    # Best-effort: embed RNG state snapshot if not already included by caller.
+    try:
+        if "rng" not in payload["state"]:
+            payload["state"]["rng"] = capture_rng_state()
+    except Exception:  # pragma: no cover
+        pass
     # Write metadata with schema and (post) digest
     meta_payload = {**meta, "schema_version": SCHEMA_VERSION}
     with open(metadata, "w", encoding="utf-8") as f:
         json.dump(meta_payload, f, indent=2, sort_keys=True)
     # Backfill digest after both files exist
@@ -116,6 +202,8 @@ def load_checkpoint(path: str, map_location: Optional[str] = None) -> Tuple[Dict[str, Any], Dict[str, Any]]:
         except Exception:
             meta = {}
     state = payload.get("state", payload)
     return state, meta
-__all__ = ["save_checkpoint", "load_checkpoint", "calculate_digest", "SCHEMA_VERSION"]
+__all__ = [
+    "save_checkpoint", "load_checkpoint", "calculate_digest",
+    "capture_rng_state", "apply_rng_state", "SCHEMA_VERSION",
+]
diff --git a/src/codex_ml/training/unified_training.py b/src/codex_ml/training/unified_training.py
index 8a9c2ff..2d2b4f2 100644
--- a/src/codex_ml/training/unified_training.py
+++ b/src/codex_ml/training/unified_training.py
@@ -24,6 +24,7 @@ except Exception:  # pragma: no cover
     torch = None  # type: ignore
     clip_grad_norm_ = None  # type: ignore
 # Optional checkpoint core (if present)
 try:
     from codex_ml.checkpointing import checkpoint_core  # type: ignore
@@ -88,10 +89,18 @@ def run_unified_training(
     start_epoch = 0
     if cfg.resume_from and checkpoint_core is not None:
         try:
             state, meta = checkpoint_core.load_checkpoint(cfg.resume_from, map_location=device)  # type: ignore[attr-defined]
             model.load_state_dict(state["model"])
             optimizer.load_state_dict(state["optimizer"])
+            # Restore RNG state if present
+            try:
+                rng_state = state.get("rng")
+                if rng_state is not None:
+                    checkpoint_core.apply_rng_state(rng_state)  # type: ignore[attr-defined]
+            except Exception:
+                # Non-fatal RNG restore failure
+                pass
             start_epoch = int(meta.get("epoch", 0)) + 1
         except Exception:
             # Soft-fail: continue from scratch if incompatible
             start_epoch = 0
@@ -144,10 +153,16 @@ def run_unified_training(
         # Optional checkpoint each epoch end
         if checkpoint_dir and checkpoint_core is not None:
             out_dir = os.path.join(checkpoint_dir, f"epoch-{epoch:04d}")
             meta = {"epoch": epoch, "loss": epoch_loss}
             try:
                 checkpoint_core.save_checkpoint(  # type: ignore[attr-defined]
                     out_dir,
-                    state={"model": model.state_dict(), "optimizer": optimizer.state_dict()},
+                    state={
+                        "model": model.state_dict(),
+                        "optimizer": optimizer.state_dict(),
+                        # Include RNG snapshot so resume parity includes stochastic ops (e.g., dropout).
+                        "rng": checkpoint_core.capture_rng_state(),  # type: ignore[attr-defined]
+                    },
                     meta=meta,
                     keep_last_k=5,
                 )
             except Exception:
                 # Non-fatal
                 pass
diff --git a/tests/checkpointing/test_rng_state_roundtrip.py b/tests/checkpointing/test_rng_state_roundtrip.py
new file mode 100644
index 0000000..f3d7f1a
--- /dev/null
+++ b/tests/checkpointing/test_rng_state_roundtrip.py
@@ -0,0 +1,58 @@
+import random as _py_random
+import math
+import pytest
+
+torch = pytest.importorskip("torch")
+try:
+    import numpy as _np  # type: ignore
+except Exception:  # pragma: no cover
+    _np = None  # type: ignore
+
+from codex_ml.checkpointing.checkpoint_core import capture_rng_state, apply_rng_state
+
+
+def test_rng_state_python_numpy_torch_roundtrip_cpu():
+    # Seed to known baseline
+    _py_random.seed(123)
+    torch.manual_seed(123)
+    if _np is not None:
+        _np.random.seed(123)
+    # Capture
+    snap = capture_rng_state()
+    # Advance RNGs
+    a_py = [_py_random.random() for _ in range(3)]
+    a_t = torch.rand(3, dtype=torch.float64)
+    a_np = None
+    if _np is not None:
+        a_np = _np.random.rand(3)
+    # Restore
+    apply_rng_state(snap)
+    # Sequences after restore must match first advance
+    b_py = [_py_random.random() for _ in range(3)]
+    b_t = torch.rand(3, dtype=torch.float64)
+    if _np is not None:
+        b_np = _np.random.rand(3)
+    # Compare
+    assert a_py == b_py
+    assert torch.allclose(a_t, b_t)
+    if _np is not None:
+        assert (_np.allclose(a_np, b_np))  # type: ignore[name-defined]
+
+
+@pytest.mark.skipif(not (hasattr(torch, "cuda") and torch.cuda.is_available()), reason="CUDA not available")
+def test_rng_state_torch_cuda_roundtrip():
+    torch.cuda.manual_seed_all(42)
+    snap = capture_rng_state()
+    a = torch.cuda.FloatTensor(3).uniform_()
+    apply_rng_state(snap)
+    b = torch.cuda.FloatTensor(3).uniform_()
+    assert torch.allclose(a, b)
diff --git a/tests/training/test_unified_training_parity_and_resume.py b/tests/training/test_unified_training_parity_and_resume.py
index 10f8c9a..e9e67e1 100644
--- a/tests/training/test_unified_training_parity_and_resume.py
+++ b/tests/training/test_unified_training_parity_and_resume.py
@@ -8,6 +8,7 @@ import pytest
 torch = pytest.importorskip("torch")
 nn = torch.nn
 optim = torch.optim
@@ -21,6 +22,12 @@ def _make_data(n=64, noise=0.0, seed=0, device="cpu"):
     return x, y
+def _make_model_with_dropout():
+    # Include dropout to exercise RNG-dependent ops during training
+    return nn.Sequential(
+        nn.Linear(1, 16), nn.ReLU(), nn.Dropout(p=0.1), nn.Linear(16, 1)
+    )
+
 def _make_loader(x, y, batch=16):
     ds = torch.utils.data.TensorDataset(x, y)
     return torch.utils.data.DataLoader(ds, batch_size=batch, shuffle=False)
@@ -44,12 +51,12 @@ def _ref_train_loop(model, optimizer, loss_fn, train_loader, epochs=2, device="cpu", clip_norm=None):
     return hist
 def test_parity_vs_reference_loop_deterministic(tmp_path):
     device = "cuda" if torch.cuda.is_available() else "cpu"
     x, y = _make_data(seed=123, device=device)
     loader = _make_loader(x, y)
-    model1 = nn.Sequential(nn.Linear(1, 1))
-    model2 = nn.Sequential(nn.Linear(1, 1))
+    model1 = _make_model_with_dropout()
+    model2 = _make_model_with_dropout()
     model2.load_state_dict(model1.state_dict())
     opt1 = optim.SGD(model1.parameters(), lr=1e-2)
     opt2 = optim.SGD(model2.parameters(), lr=1e-2)
     loss_fn = nn.MSELoss()
@@ -69,12 +76,12 @@ def test_parity_vs_reference_loop_deterministic(tmp_path):
 def test_resume_parity(tmp_path):
     device = "cuda" if torch.cuda.is_available() else "cpu"
     x, y = _make_data(seed=321, device=device)
     loader = _make_loader(x, y)
-    modelA = nn.Sequential(nn.Linear(1, 1))
-    modelB = nn.Sequential(nn.Linear(1, 1))
+    modelA = _make_model_with_dropout()
+    modelB = _make_model_with_dropout()
     modelB.load_state_dict(modelA.state_dict())
     optA = optim.SGD(modelA.parameters(), lr=1e-2)
     optB = optim.SGD(modelB.parameters(), lr=1e-2)
     loss_fn = nn.MSELoss()
@@ -99,3 +106,34 @@ def test_resume_parity(tmp_path):
     assert len(hist_full["loss"]) == 2 and len(hist_res["loss"]) == 2
     assert abs(hist_full["loss"][-1] - hist_res["loss"][-1]) <= 1e-6
+
+
+def test_legacy_wrapper_parity_and_warning(tmp_path):
+    device = "cuda" if torch.cuda.is_available() else "cpu"
+    x, y = _make_data(seed=11, device=device)
+    loader = _make_loader(x, y)
+    model1 = _make_model_with_dropout()
+    model2 = _make_model_with_dropout()
+    model2.load_state_dict(model1.state_dict())
+    opt1 = optim.SGD(model1.parameters(), lr=1e-2)
+    opt2 = optim.SGD(model2.parameters(), lr=1e-2)
+    loss_fn = nn.MSELoss()
+    cfg = UnifiedTrainingConfig(epochs=1, lr=1e-2, deterministic=True, device=device)
+    # Unified path
+    hist_u = run_unified_training(cfg, model=model1, optimizer=opt1, loss_fn=loss_fn, train_loader=loader)
+    # Legacy wrapper emits deprecation and delegates
+    import warnings
+    with warnings.catch_warnings(record=True) as rec:
+        warnings.simplefilter("always", DeprecationWarning)
+        from codex_ml.training import unified_training as ut
+        hist_l = ut.train_loop(cfg, model=model2, optimizer=opt2, loss_fn=loss_fn, train_loader=loader)
+        assert any(isinstance(w.message, DeprecationWarning) for w in rec)
+    assert len(hist_u["loss"]) == len(hist_l["loss"])
+    for a, b in zip(hist_u["loss"], hist_l["loss"]):
+        assert abs(a - b) <= 1e-6
diff --git a/docs/unified_training.md b/docs/unified_training.md
index af3d1a1..b3b1a61 100644
--- a/docs/unified_training.md
+++ b/docs/unified_training.md
@@ -8,6 +8,7 @@ from codex_ml.training.unified_training import UnifiedTrainingConfig, run_unified_training
 - One faÃ§ade: `run_unified_training(cfg)` to reduce drift between legacy loops.
 - Determinism first: optional seeding + CuDNN settings.
 - Grad clipping, resume hooks, and checkpoint save at epoch boundaries.
 - Thin legacy adapters (`train_loop`, `functional_training`) that emit `DeprecationWarning`.
 - Checkpoint metadata includes a `digest_sha256` for integrity checks and `schema_version`.
 - Loader prefers `weights_only=True` when available for safer deserialization.
 - Retention: best-effort pruning of epoch directories to keep the last *k*.
+- RNG parity on resume: checkpoint payload carries Python/NumPy/Torch RNG snapshots; resume applies them for stochastic-op continuity.
diff --git a/src/codex_ml/checkpointing/checkpoint_core.py b/src/codex_ml/checkpointing/checkpoint_core.py
index 8f5c2e4..8a5db0b 100644
--- a/src/codex_ml/checkpointing/checkpoint_core.py
+++ b/src/codex_ml/checkpointing/checkpoint_core.py
@@ -123,16 +123,22 @@ def save_checkpoint(out_dir: str, *, state: Dict[str, Any], meta: Dict[str, Any], keep_last_k: int = 5) -> str:
     _ensure_dir(out_dir)
     if torch is None:
         raise RuntimeError("PyTorch required to save checkpoints")
     weights = os.path.join(out_dir, "weights.pt")
     metadata = os.path.join(out_dir, "metadata.json")
-    payload = {"schema_version": SCHEMA_VERSION, "state": state}
-    torch.save(payload, weights)
-    # Best-effort: embed RNG state snapshot if not already included by caller.
-    try:
-        if "rng" not in payload["state"]:
-            payload["state"]["rng"] = capture_rng_state()
-    except Exception:  # pragma: no cover
-        pass
+    # Ensure RNG snapshot is present in the saved state for resume parity.
+    # IMPORTANT: add RNG BEFORE writing weights so it is persisted.
+    state_payload: Dict[str, Any] = dict(state)
+    try:  # best-effort
+        if "rng" not in state_payload:
+            state_payload["rng"] = capture_rng_state()
+    except Exception:  # pragma: no cover
+        pass
+    payload = {"schema_version": SCHEMA_VERSION, "state": state_payload}
+    torch.save(payload, weights)
     # Write metadata with schema and (post) digest
     meta_payload = {**meta, "schema_version": SCHEMA_VERSION}
     with open(metadata, "w", encoding="utf-8") as f:
         json.dump(meta_payload, f, indent=2, sort_keys=True)
     # Backfill digest after both files exist
@@ -206,7 +212,12 @@ def load_checkpoint(path: str, map_location: Optional[str] = None) -> Tuple[Dict[str, Any], Dict[str, Any]]:
         except Exception:
             meta = {}
     state = payload.get("state", payload)
     return state, meta
-__all__ = [
-    "save_checkpoint", "load_checkpoint", "calculate_digest",
-    "capture_rng_state", "apply_rng_state", "SCHEMA_VERSION",
-]
+__all__ = [
+    "save_checkpoint",
+    "load_checkpoint",
+    "calculate_digest",
+    "capture_rng_state",
+    "apply_rng_state",
+    "SCHEMA_VERSION",
+]
diff --git a/tools/calc_manifest_hash.py b/tools/calc_manifest_hash.py
new file mode 100755
index 0000000..6a1bb3f
--- /dev/null
+++ b/tools/calc_manifest_hash.py
@@ -0,0 +1,180 @@
+#!/usr/bin/env python3
+from __future__ import annotations
+"""
+Compute a deterministic repository "manifest hash" and (optionally) update README with a badge.
+
+WHY
+- Provide a simple, offline-friendly integrity breadcrumb for external audits.
+
+WHAT
+- Walk a default file set (code, tests, docs, config) and compute an aggregate SHA256:
+    aggregate = sha256( join( sha256(file_i) for each file in ordered list ) )
+- Output JSON to stdout. With --write-readme, replace a guarded line in README.md.
+
+RISK
+- Over-inclusion could churn the hash too often. Default paths bias toward source & tests.
+
+ROLLBACK
+- Delete README badge line; script is fully optional.
+"""
+import argparse
+import hashlib
+import json
+import os
+import re
+from pathlib import Path
+from typing import Iterable, List
+
+DEFAULT_GLOBS = [
+    "pyproject.toml",
+    "noxfile.py",
+    "src/**/*.py",
+    "tests/**/*.py",
+    "docs/**/*.md",
+    "bandit.yaml",
+    ".pre-commit-config.yaml",
+]
+
+BADGE_BEGIN = "<!-- BEGIN-CAPABILITY-HASH-BADGE -->"
+BADGE_END = "<!-- END-CAPABILITY-HASH-BADGE -->"
+
+
+def iter_files(root: Path, patterns: Iterable[str]) -> List[Path]:
+    out: List[Path] = []
+    for pat in patterns:
+        # Use glob with recursion
+        for p in root.glob(pat):
+            if p.is_file():
+                out.append(p)
+    # Deduplicate & sort by POSIX path for determinism
+    uniq = sorted({p.resolve() for p in out}, key=lambda p: p.as_posix())
+    return uniq
+
+
+def sha256_file(path: Path) -> str:
+    h = hashlib.sha256()
+    with open(path, "rb") as f:
+        for chunk in iter(lambda: f.read(8192), b""):
+            h.update(chunk)
+    return h.hexdigest()
+
+
+def aggregate_hash(file_hashes: List[str]) -> str:
+    h = hashlib.sha256()
+    joined = ":".join(file_hashes).encode("utf-8")
+    h.update(joined)
+    return h.hexdigest()
+
+
+def make_badge_line(agg: str) -> str:
+    short = agg[:12]
+    # Shields static badge (offline-friendly text in README; actual badge is optional)
+    txt = f"Integrity: `{short}` (SHA256)"
+    return f"{BADGE_BEGIN}\n{txt}\n{BADGE_END}"
+
+
+def update_readme_badge(readme: Path, agg: str) -> None:
+    line = make_badge_line(agg)
+    if readme.exists():
+        content = readme.read_text(encoding="utf-8")
+    else:
+        content = ""
+    if BADGE_BEGIN in content and BADGE_END in content:
+        # Replace existing block
+        pattern = re.compile(re.escape(BADGE_BEGIN) + r".*?" + re.escape(BADGE_END), re.DOTALL)
+        newc = pattern.sub(line, content)
+    else:
+        # Prepend at top with a blank line after
+        newc = line + "\n\n" + content
+    readme.write_text(newc, encoding="utf-8")
+
+
+def main(argv: List[str] | None = None) -> int:
+    parser = argparse.ArgumentParser()
+    parser.add_argument("--root", type=str, default=".")
+    parser.add_argument("--glob", action="append", dest="globs", default=[], help="Add glob pattern(s)")
+    parser.add_argument("--write-readme", action="store_true", help="Update README.md badge block")
+    args = parser.parse_args(argv)
+    root = Path(args.root).resolve()
+    patterns = args.globs or DEFAULT_GLOBS
+    files = iter_files(root, patterns)
+    fhashes = [sha256_file(p) for p in files]
+    agg = aggregate_hash(fhashes)
+    out = {
+        "root": root.as_posix(),
+        "file_count": len(files),
+        "files": [p.as_posix() for p in files],
+        "hashes": fhashes,
+        "aggregate_sha256": agg,
+    }
+    print(json.dumps(out, indent=2))
+    if args.write_readme:
+        update_readme_badge(root / "README.md", agg)
+    return 0
+
+
+if __name__ == "__main__":
+    raise SystemExit(main())
diff --git a/tests/tools/test_calc_manifest_hash.py b/tests/tools/test_calc_manifest_hash.py
new file mode 100644
index 0000000..7043f0f
--- /dev/null
+++ b/tests/tools/test_calc_manifest_hash.py
@@ -0,0 +1,43 @@
+import json
+import subprocess
+import sys
+import os
+
+
+def test_calc_manifest_hash_outputs_json(tmp_path, monkeypatch):
+    # Create a tiny repo skeleton
+    (tmp_path / "src" / "pkg").mkdir(parents=True)
+    f = tmp_path / "src" / "pkg" / "m.py"
+    f.write_text("print('x')\n", encoding="utf-8")
+    script = os.path.join(os.getcwd(), "tools", "calc_manifest_hash.py")
+    assert os.path.exists(script)
+    p = subprocess.run(
+        [sys.executable, script, "--root", str(tmp_path), "--glob", "src/**/*.py"],
+        capture_output=True,
+        text=True,
+        check=True,
+    )
+    data = json.loads(p.stdout)
+    assert data["file_count"] == 1
+    assert isinstance(data["aggregate_sha256"], str) and len(data["aggregate_sha256"]) == 64
+
+
+def test_calc_manifest_hash_can_write_readme(tmp_path):
+    readme = tmp_path / "README.md"
+    readme.write_text("# Title\n", encoding="utf-8")
+    script = os.path.join(os.getcwd(), "tools", "calc_manifest_hash.py")
+    p = subprocess.run(
+        [sys.executable, script, "--root", str(tmp_path), "--glob", "README.md", "--write-readme"],
+        capture_output=True,
+        text=True,
+        check=True,
+    )
+    content = readme.read_text(encoding="utf-8")
+    assert "<!-- BEGIN-CAPABILITY-HASH-BADGE -->" in content
+    assert "<!-- END-CAPABILITY-HASH-BADGE -->" in content
diff --git a/docs/manifest_integrity.md b/docs/manifest_integrity.md
new file mode 100644
index 0000000..cf87742
--- /dev/null
+++ b/docs/manifest_integrity.md
@@ -0,0 +1,58 @@
+# Manifest Integrity & Capability Hash
+
+This repository can generate an **aggregate SHA256** over important files (code, tests, docs) and optionally write a small badge line into `README.md`.
+
+```bash
+python tools/calc_manifest_hash.py --write-readme
+```
+
+This will add or update a block like:
+
+```
+<!-- BEGIN-CAPABILITY-HASH-BADGE -->
+Integrity: `deadbeefcaf0` (SHA256)
+<!-- END-CAPABILITY-HASH-BADGE -->
+```
+
+The short value is the first 12 hex digits of the aggregate SHA256. The underlying algorithm is:
+
+```
+aggregate = sha256( ':'.join( sha256(file_i) for file_i in SORTED_FILE_LIST ) )
+```
+
+To customize which files are included, pass one or more `--glob` values. The default includes:
+
+- `pyproject.toml`, `noxfile.py`, `.pre-commit-config.yaml`, `bandit.yaml`
+- `src/**/*.py`, `tests/**/*.py`, `docs/**/*.md`
+
+This feature is **offline and local**; it does not trigger any remote actions.
+
+## Notes
+
+- Hash churn is expected when files change. Use it as a lightweight audit breadcrumb, not as a CI gate.
+- The badge is purely informational and can be removed safely.
diff --git a/README.md b/README.md
index e69de29..b32b3c1 100644
--- a/README.md
+++ b/README.md
@@ -0,0 +1,19 @@
+# _codex_
+
+<!-- BEGIN-CAPABILITY-HASH-BADGE -->
+Integrity: `uninitialized` (SHA256)
+<!-- END-CAPABILITY-HASH-BADGE -->
+
+Local, offline-first utilities and ML training components.
+
+## Quickstart
+
+```bash
+python -m pip install -e .
+pre-commit install
+pytest -q
+```
+
+See `docs/` for details on training (`docs/unified_training.md`), tracking guards (`docs/observability.md`),
+and integrity hashing (`docs/manifest_integrity.md`).
diff --git a/OPEN_QUESTIONS.md b/OPEN_QUESTIONS.md
new file mode 100644
index 0000000..5f2d8f3
--- /dev/null
+++ b/OPEN_QUESTIONS.md
@@ -0,0 +1,39 @@
+# Open Questions (tracking)
+
+1) **Training CLI integration**
+   Should `run_unified_training` be exposed via a thin CLI (e.g., `python -m codex_ml.train`), or remain library-only?
+
+2) **MLflow guard allow-list**
+   We currently use a single env (`CODEX_ALLOW_REMOTE_TRACKING`) as an override. Do we also want a domain allow-list (e.g., `CODEX_TRACKING_ALLOW_HOSTS=example.com,localhost`)?
+
+3) **Tokenizer API surface**
+   Is `WhitespaceTokenizer` sufficient as a fallback, or do we want optional adapters for HF tokenizers under an extra (e.g., `pip install codex_ml[tokenizers]`)?
+
+4) **Checkpoint retention policy**
+   Retention prunes `epoch-####` directories beyond *k*. Should we also keep a stable `latest/` symlink or copy?
+
+5) **Manifest hash scope**
+   Default globs include code/tests/docs. Should we pin the list in config (e.g., `codex.toml`) for future reproducibility?
+
+6) **Data determinism tests**
+   Current tests are pure-Python helpers. Do we want Hypothesis-based property tests for broader coverage (still offline)?
+
+7) **Detector scoring integration**
+   The detector now emits a standalone score. Should we wire a simple aggregator that updates a repo-level scorecard?
+
+8) **Windows path handling**
+   JSON/NDJSON writers use `\n` line endings explicitly; pre-commit enforces LF. Confirm this policy for Windows developers.
+
+9) **Weights-only load fallback**
+   `torch.load(weights_only=True)` is preferred when available. Do we want a feature flag to force strict mode or fallback?
diff --git a/CHANGELOG_CODEX.md b/CHANGELOG_CODEX.md
index b48f6e9..86a67ef 100644
--- a/CHANGELOG_CODEX.md
+++ b/CHANGELOG_CODEX.md
@@ -1,5 +1,26 @@
 # Codex Changelog
+## 2025-10-06 â Manifest integrity tooling + RNG persist fix
+
+### WHY
+- Provide a small, offline integrity breadcrumb (aggregate hash) and optional README badge.
+- Fix a bug where RNG snapshots were attached **after** saving weights, risking loss of RNG state in generic `save_checkpoint` calls.
+
+### Changes
+- `tools/calc_manifest_hash.py`: compute an aggregate SHA256 across important files; optional README badge update.
+- Tests: `tests/tools/test_calc_manifest_hash.py`.
+- Docs: `docs/manifest_integrity.md`.
+- `README.md`: add guarded badge block (initialized placeholder).
+- `checkpoint_core.save_checkpoint`: ensure RNG is added to the state **before** writing `weights.pt`.
+- `OPEN_QUESTIONS.md`: start a running list per project policy.
+
+### Risk
+- Low. Script is optional; README badge is inert. RNG fix affects only callers relying on implicit RNG capture.
+
+### Rollback
+- Remove `tools/calc_manifest_hash.py` and the README badge block.
+- Revert the `save_checkpoint` change to previous behavior (not recommended).
+
 ## 2025-10-06 â Metrics & Safety API consolidation (P2 quick wins)
 ### WHY
 - Reduce duplication and clarify public surfaces for metrics and safety.
 - Provide deprecation stubs for legacy entry points with test-asserted warnings.
 ```diff
diff --git a/src/codex_ml/tracking/guards.py b/src/codex_ml/tracking/guards.py
index e7c8d0b..b2c5b16 100644
--- a/src/codex_ml/tracking/guards.py
+++ b/src/codex_ml/tracking/guards.py
@@ -10,6 +10,7 @@ Tracking/Logging guards for offline-first enforcement.
 - Normalize MLflow URIs to file:// scheme under offline modes.
 - Provide a structured decision object for auditability.
+NOTE: See docs/observability.md for usage examples.
 RISK:
 - Overzealous blocking could surprise users who *intentionally* configured remote endpoints.
   Mitigation: explicit allow-remote env override.
@@ -20,10 +21,12 @@ ROLLBACK:
 References (patterns; concepts only):
 - MLflow tracking URI schemes (remote vs local): https://mlflow.org/docs/latest/ml/tracking/   # see "Tracking"
 - Weights & Biases offline modes: https://docs.wandb.ai/guides/track/environment-variables/    # WANDB_MODE
 - W&B offline guide: https://docs.wandb.ai/support/run_wandb_offline/
 - NDJSON basics (one JSON object per line, UTF-8, newline delimited).
+- PyTorch deterministic algorithms setting.
 """
 import os
+import contextlib
 import pathlib
 import json
 import io
@@ -187,10 +190,55 @@ def decide_and_summarize_mlflow_uri(
     except Exception:
         # Summary is best-effort; decision remains authoritative.
         pass
     return d
+def apply_tracking_decision_to_env(
+    decision: TrackingDecision,
+    *,
+    set_mlflow: bool = True,
+    environ: Optional[Dict[str, str]] = None,
+) -> Dict[str, str]:
+    """
+    Apply a TrackingDecision to environment variables.
+    - If `set_mlflow`, set MLFLOW_TRACKING_URI to decision.uri (when present).
+    - Returns a dict of environment changes {VAR: value}.
+    - Best-effort: if mlflow is importable and set_mlflow=True, we also call mlflow.set_tracking_uri.
+    """
+    e = os.environ if environ is None else environ
+    changed: Dict[str, str] = {}
+    if set_mlflow and decision.uri:
+        e["MLFLOW_TRACKING_URI"] = decision.uri
+        changed["MLFLOW_TRACKING_URI"] = decision.uri
+        # Best-effort runtime update
+        if environ is None:  # only touch process env if we are mutating real os.environ
+            with contextlib.suppress(Exception):
+                import mlflow  # type: ignore
+                mlflow.set_tracking_uri(decision.uri)
+    return changed
+
 __all__ = [
     "TrackingDecision",
     "decide_mlflow_tracking_uri",
     "normalize_mlflow_uri",
     "write_tracking_decision_summary",
     "decide_and_summarize_mlflow_uri",
+    "apply_tracking_decision_to_env",
 ]
diff --git a/tests/tracking/test_tracking_guards_apply.py b/tests/tracking/test_tracking_guards_apply.py
new file mode 100644
index 0000000..3a7d2c8
--- /dev/null
+++ b/tests/tracking/test_tracking_guards_apply.py
@@ -0,0 +1,33 @@
+import copy
+from codex_ml.tracking.guards import TrackingDecision, apply_tracking_decision_to_env
+
+
+def test_apply_tracking_decision_sets_env_without_side_effects():
+    d = TrackingDecision(uri="file:///abs/mlruns", blocked=False, reason="offline_default_local_uri", details={})
+    env = {"MLFLOW_TRACKING_URI": "http://old"}
+    snapshot = copy.deepcopy(env)
+    changed = apply_tracking_decision_to_env(d, set_mlflow=True, environ=env)
+    assert env["MLFLOW_TRACKING_URI"].startswith("file://")
+    assert changed["MLFLOW_TRACKING_URI"].startswith("file://")
+    # Do not mutate the original snapshot
+    assert snapshot["MLFLOW_TRACKING_URI"] == "http://old"
diff --git a/tools/ndjson_summarize.py b/tools/ndjson_summarize.py
new file mode 100755
index 0000000..d8a9c1e
--- /dev/null
+++ b/tools/ndjson_summarize.py
@@ -0,0 +1,132 @@
+#!/usr/bin/env python3
+from __future__ import annotations
+"""
+Collapse an NDJSON file into CSV by flattening top-level keys.
+
+Usage:
+  python tools/ndjson_summarize.py input.ndjson output.csv
+
+Notes:
+  - Only flattens one level (top-level dict); nested dicts are JSON-encoded.
+  - All rows are aligned to the union of keys across the file.
+"""
+import csv
+import json
+import sys
+from typing import Dict, Any, List
+
+
+def _flatten_row(obj: Dict[str, Any]) -> Dict[str, Any]:
+    out: Dict[str, Any] = {}
+    for k, v in obj.items():
+        if isinstance(v, (str, int, float)) or v is None:
+            out[k] = v
+        else:
+            out[k] = json.dumps(v, ensure_ascii=False, sort_keys=True)
+    return out
+
+
+def summarize_ndjson(in_path: str, out_path: str) -> None:
+    rows: List[Dict[str, Any]] = []
+    keys: List[str] = []
+    seen = set()
+    with open(in_path, "r", encoding="utf-8") as f:
+        for line in f:
+            line = line.strip()
+            if not line:
+                continue
+            obj = json.loads(line)
+            flat = _flatten_row(obj)
+            rows.append(flat)
+            for k in flat:
+                if k not in seen:
+                    keys.append(k)
+                    seen.add(k)
+    with open(out_path, "w", encoding="utf-8", newline="") as f:
+        w = csv.DictWriter(f, fieldnames=keys)
+        w.writeheader()
+        for r in rows:
+            w.writerow({k: r.get(k, "") for k in keys})
+
+
+def main(argv: List[str] | None = None) -> int:
+    argv = list(sys.argv[1:] if argv is None else argv)
+    if len(argv) != 2:
+        sys.stderr.write("usage: ndjson_summarize.py <input.ndjson> <output.csv>\n")
+        return 2
+    summarize_ndjson(argv[0], argv[1])
+    return 0
+
+
+if __name__ == "__main__":
+    raise SystemExit(main())
diff --git a/tests/tools/test_ndjson_summarize.py b/tests/tools/test_ndjson_summarize.py
new file mode 100644
index 0000000..b7e6a9a
--- /dev/null
+++ b/tests/tools/test_ndjson_summarize.py
@@ -0,0 +1,31 @@
+import json
+import sys
+import subprocess
+from pathlib import Path
+
+
+def test_ndjson_summarize_creates_csv(tmp_path: Path):
+    nd = tmp_path / "x.ndjson"
+    csvp = tmp_path / "x.csv"
+    lines = [
+        {"uri": "file:///a", "blocked": False, "details": {"k": 1}},
+        {"uri": "file:///b", "blocked": True, "reason": "rewrite"},
+    ]
+    nd.write_text("\n".join(json.dumps(x) for x in lines) + "\n", encoding="utf-8")
+    script = Path("tools/ndjson_summarize.py").resolve()
+    out = subprocess.run([sys.executable, str(script), str(nd), str(csvp)], capture_output=True, text=True)
+    assert out.returncode == 0
+    content = csvp.read_text(encoding="utf-8").splitlines()
+    # header + 2 rows
+    assert len(content) == 3
+    assert "uri,blocked,details,reason" in content[0]
+    assert "file:///a" in content[1]
+    assert "file:///b" in content[2]
diff --git a/src/codex_ml/detectors/__init__.py b/src/codex_ml/detectors/__init__.py
new file mode 100644
index 0000000..9aa0c3f
--- /dev/null
+++ b/src/codex_ml/detectors/__init__.py
@@ -0,0 +1,8 @@
+"""
+Detectors public surface.
+"""
+from .unified_training import detect_unified_training
+
+__all__ = [
+    "detect_unified_training",
+]
diff --git a/src/codex_ml/detectors/aggregate.py b/src/codex_ml/detectors/aggregate.py
new file mode 100644
index 0000000..a8876c2
--- /dev/null
+++ b/src/codex_ml/detectors/aggregate.py
@@ -0,0 +1,59 @@
+from __future__ import annotations
+"""
+Aggregate capability detectors into a simple scorecard.
+
+Current mapping:
+  - 'training-engine' score is derived from 'unified-training' detector (1:1 mapping).
+"""
+from pathlib import Path
+from typing import Dict, Any
+
+from .unified_training import detect_unified_training
+
+
+def aggregate_scores(project_root: str | Path = ".") -> Dict[str, Any]:
+    uni = detect_unified_training(project_root)
+    scorecard = {
+        "detectors": {
+            "unified-training": uni,
+        },
+        "capabilities": {
+            "training-engine": {
+                "score": uni["score"],
+                "signals": uni["signals"],
+                "sources": ["unified-training"],
+            }
+        },
+    }
+    return scorecard
+
+
+__all__ = ["aggregate_scores"]
diff --git a/tests/detectors/test_aggregate_scores.py b/tests/detectors/test_aggregate_scores.py
new file mode 100644
index 0000000..7d0a608
--- /dev/null
+++ b/tests/detectors/test_aggregate_scores.py
@@ -0,0 +1,25 @@
+from pathlib import Path
+from codex_ml.detectors.aggregate import aggregate_scores
+
+
+def test_aggregate_scores_maps_unified_to_training_engine(tmp_path: Path):
+    pkg = tmp_path / "src" / "codex_ml" / "training"
+    pkg.mkdir(parents=True)
+    (pkg / "unified_training.py").write_text(
+        "class UnifiedTrainingConfig: pass\n"
+        "def run_unified_training(*a, **k): pass\n",
+        encoding="utf-8",
+    )
+    # Call aggregator
+    res = aggregate_scores(tmp_path)
+    assert "capabilities" in res and "training-engine" in res["capabilities"]
+    cap = res["capabilities"]["training-engine"]
+    assert 0.6 <= cap["score"] <= 1.0
+    assert cap["sources"] == ["unified-training"]
diff --git a/docs/observability.md b/docs/observability.md
index c7b4f72..d5d4f7e 100644
--- a/docs/observability.md
+++ b/docs/observability.md
@@ -28,6 +28,22 @@ d = decide_and_summarize_mlflow_uri(dest="tracking_summary.ndjson")
 print(d.uri, d.reason)
```
This appends a single JSON object per line. The file is UTF-8 and newline delimited (NDJSON).
+## Apply Decision To Environment
+
+To enforce the decision inside the current process:
+
+`python
+from codex_ml.tracking.guards import decide_mlflow_tracking_uri, apply_tracking_decision_to_env
+dec = decide_mlflow_tracking_uri()
+apply_tracking_decision_to_env(dec)  # sets MLFLOW_TRACKING_URI in the environment (and mlflow runtime if importable)
+`
+
+This keeps MLflow local (`file://...`) under offline modes (`MLFLOW_OFFLINE=1`, `WANDB_MODE=offline|disabled`) unless
+you explicitly opt-in to remote tracking with `CODEX_ALLOW_REMOTE_TRACKING=1`.
+
+See the W&B and MLflow docs for details on offline behavior and tracking URIs.
+
### Why NDJSON?
* Stream-friendly: process line-by-line without loading the whole file.
* Works cleanly with CLI tools and ingestion systems.
  @@ -35,3 +51,4 @@ This appends a single JSON object per line. The file is UTF-8 and newline delim
### Caveats
* Do not embed raw newlines in string fields. The logger and writer sanitize them by escaping.
* Keep summaries small; theyâre intended for local audits.
  +- Use `tools/ndjson_summarize.py` to flatten an NDJSON file into CSV for quick inspection.
  diff --git a/docs/detectors.md b/docs/detectors.md
  new file mode 100644
  index 0000000..8bda18b
  --- /dev/null
  +++ b/docs/detectors.md
  @@ -0,0 +1,34 @@
  +# Detectors & Scoring (Draft)
-
+This repository includes lightweight detectors that infer capability presence from the codebase and
+aggregate them into a simple scorecard.
+
+`python
+from codex_ml.detectors.aggregate import aggregate_scores
+print(aggregate_scores())
+`
+
+Current mapping:
+- `training-engine` â `unified-training`
+
+The mapping is intentionally simple for offline/local use and can be extended to include additional detectors over time.
diff --git a/CHANGELOG_CODEX.md b/CHANGELOG_CODEX.md
index 86a67ef..8778682 100644
--- a/CHANGELOG_CODEX.md
+++ b/CHANGELOG_CODEX.md
@@ -1,5 +1,26 @@
# Codex Changelog
+## 2025-10-06 â Env application for tracking guard; NDJSON summarizer; detector aggregator
+
+### WHY
+- Make the tracking decision easy to enforce at runtime (env + optional mlflow runtime).
+- Provide a minimal NDJSONâCSV summarizer for local audits of guard outputs.
+- Connect the unified-training detector to a simple `training-engine` score.
+
+### Changes
+- `tracking/guards.py`: `apply_tracking_decision_to_env(...)` to set `MLFLOW_TRACKING_URI` (and call `mlflow.set_tracking_uri` when available).
+- Tools: `tools/ndjson_summarize.py` (flatten NDJSON to CSV).
+- Detectors:
* * `codex_ml.detectors.__init__` re-export.
* * `codex_ml.detectors.aggregate.aggregate_scores` maps detectors â capability scores.
    +- Tests:
* * `tests/tracking/test_tracking_guards_apply.py`
* * `tests/tools/test_ndjson_summarize.py`
* * `tests/detectors/test_aggregate_scores.py`
    +- Docs:
* * `docs/observability.md` updated with env application.
* * `docs/detectors.md` initial draft.
*
## 2025-10-06 â Manifest integrity tooling + RNG persist fix
### WHY
* Provide a small, offline integrity breadcrumb (aggregate hash) and optional README badge.
* Fix a bug where RNG snapshots were attached **after** saving weights, risking loss of RNG state in generic `save_checkpoint` calls.
```
````diff
diff --git a/src/codex_ml/__main__.py b/src/codex_ml/__main__.py
new file mode 100644
index 0000000..a84bc70
--- /dev/null
+++ b/src/codex_ml/__main__.py
@@ -0,0 +1,7 @@
+from __future__ import annotations
+"""
+Module entrypoint to enable:  python -m codex_ml ...
+"""
+from .cli import main
+
+if __name__ == "__main__":  # pragma: no cover
+    raise SystemExit(main())
diff --git a/src/codex_ml/cli.py b/src/codex_ml/cli.py
new file mode 100644
index 0000000..e1fbe4b
--- /dev/null
+++ b/src/codex_ml/cli.py
@@ -0,0 +1,205 @@
+from __future__ import annotations
+"""
+Codex CLI (local/offline).
+
+Subcommands:
+  - detectors scorecard [--json]
+  - tracking decide [--apply] [--summary PATH] [--print]
+  - hash manifest [--root PATH] [--glob PATTERN ...] [--write-readme]
+
+Notes:
+  - Tracking guard behavior references MLflow and W&B patterns (offline-first).
+  - Manifest hashing is local and deterministic; see docs/manifest_integrity.md.
+"""
+import argparse
+import json
+import os
+import sys
+import hashlib
+from pathlib import Path
+from typing import Iterable, List, Dict, Any
+
+from codex_ml.detectors.aggregate import aggregate_scores
+from codex_ml.tracking.guards import (
+    decide_mlflow_tracking_uri,
+    decide_and_summarize_mlflow_uri,
+    apply_tracking_decision_to_env,
+)
+
+
+def _iter_files(root: Path, patterns: Iterable[str]) -> List[Path]:
+    out: List[Path] = []
+    for pat in patterns:
+        out.extend([p for p in root.glob(pat) if p.is_file()])
+    # Deduplicate + stable order
+    return sorted({p.resolve() for p in out}, key=lambda p: p.as_posix())
+
+
+def _sha256_file(path: Path) -> str:
+    h = hashlib.sha256()
+    with open(path, "rb") as f:
+        for chunk in iter(lambda: f.read(8192), b""):
+            h.update(chunk)
+    return h.hexdigest()
+
+
+def _aggregate_hash(file_hashes: List[str]) -> str:
+    h = hashlib.sha256()
+    h.update(":".join(file_hashes).encode("utf-8"))
+    return h.hexdigest()
+
+
+def _cmd_detectors(args: argparse.Namespace) -> int:
+    scorecard = aggregate_scores(".")
+    if getattr(args, "json", False):
+        print(json.dumps(scorecard, indent=2))
+    else:
+        caps = scorecard.get("capabilities", {})
+        for name, payload in caps.items():
+            print(f"{name}: {payload.get('score')}")
+    return 0
+
+
+def _cmd_tracking_decide(args: argparse.Namespace) -> int:
+    # Decide (+ optionally summarize + apply)
+    if args.summary:
+        dec = decide_and_summarize_mlflow_uri(dest=args.summary)
+    else:
+        dec = decide_mlflow_tracking_uri()
+    changes: Dict[str, str] = {}
+    if args.apply:
+        changes = apply_tracking_decision_to_env(dec)
+    if args.do_print:
+        payload: Dict[str, Any] = {
+            "decision": {
+                "uri": dec.uri,
+                "blocked": dec.blocked,
+                "reason": dec.reason,
+                "details": dec.details,
+            },
+            "applied_env": changes,
+        }
+        print(json.dumps(payload, indent=2))
+    return 0
+
+
+_DEFAULT_GLOBS = [
+    "pyproject.toml",
+    "noxfile.py",
+    "src/**/*.py",
+    "tests/**/*.py",
+    "docs/**/*.md",
+    "bandit.yaml",
+    ".pre-commit-config.yaml",
+]
+
+
+def _cmd_hash_manifest(args: argparse.Namespace) -> int:
+    root = Path(getattr(args, "root") or ".").resolve()
+    patterns = args.glob or _DEFAULT_GLOBS
+    files = _iter_files(root, patterns)
+    fhashes = [_sha256_file(p) for p in files]
+    agg = _aggregate_hash(fhashes)
+    out = {
+        "root": root.as_posix(),
+        "file_count": len(files),
+        "files": [p.as_posix() for p in files],
+        "hashes": fhashes,
+        "aggregate_sha256": agg,
+    }
+    print(json.dumps(out, indent=2))
+    if args.write_readme:
+        readme = root / "README.md"
+        begin = "<!-- BEGIN-CAPABILITY-HASH-BADGE -->"
+        end = "<!-- END-CAPABILITY-HASH-BADGE -->"
+        short = agg[:12]
+        block = f"{begin}\nIntegrity: `{short}` (SHA256)\n{end}"
+        if readme.exists():
+            text = readme.read_text(encoding="utf-8")
+        else:
+            text = ""
+        if begin in text and end in text:
+            import re
+            text = re.sub(
+                begin + r".*?" + end,
+                block,
+                text,
+                flags=re.DOTALL,
+            )
+        else:
+            text = block + "\n\n" + text
+        readme.write_text(text, encoding="utf-8")
+    return 0
+
+
+def main(argv: List[str] | None = None) -> int:
+    parser = argparse.ArgumentParser(prog="codex", description="Codex utilities (local/offline)")
+    sub = parser.add_subparsers(dest="cmd", required=True)
+
+    p_det = sub.add_parser("detectors", help="Capability detectors and scoring")
+    sub_det = p_det.add_subparsers(dest="det_cmd", required=True)
+    p_det_score = sub_det.add_parser("scorecard", help="Print scorecard")
+    p_det_score.add_argument("--json", action="store_true", help="Emit JSON")
+    p_det_score.set_defaults(func=_cmd_detectors)
+
+    p_track = sub.add_parser("tracking", help="Tracking/guard operations")
+    sub_track = p_track.add_subparsers(dest="trk_cmd", required=True)
+    p_dec = sub_track.add_parser("decide", help="Decide MLflow URI (offline-first)")
+    p_dec.add_argument("--apply", action="store_true", help="Apply to environment and mlflow (best-effort)")
+    p_dec.add_argument("--summary", type=str, help="Append decision to NDJSON file")
+    p_dec.add_argument("--print", dest="do_print", action="store_true", help="Print JSON decision")
+    p_dec.set_defaults(func=_cmd_tracking_decide)
+
+    p_hash = sub.add_parser("hash", help="Integrity hashing tools")
+    sub_hash = p_hash.add_subparsers(dest="hash_cmd", required=True)
+    p_manifest = sub_hash.add_parser("manifest", help="Aggregate hash across repository files")
+    p_manifest.add_argument("--root", type=str, help="Repository root (default: .)")
+    p_manifest.add_argument("--glob", action="append", default=[], help="Additional glob patterns")
+    p_manifest.add_argument("--write-readme", action="store_true", help="Update README badge")
+    p_manifest.set_defaults(func=_cmd_hash_manifest)
+
+    args = parser.parse_args(sys.argv[1:] if argv is None else argv)
+    return int(args.func(args) or 0)
diff --git a/tests/cli/test_cli.py b/tests/cli/test_cli.py
new file mode 100644
index 0000000..40ec520
--- /dev/null
+++ b/tests/cli/test_cli.py
@@ -0,0 +1,64 @@
+import json
+import os
+import subprocess
+import sys
+from pathlib import Path
+
+
+def _run(*args, cwd=None):
+    return subprocess.run([sys.executable, "-m", "codex_ml", *args], capture_output=True, text=True, cwd=cwd)
+
+
+def test_detectors_scorecard_json():
+    p = _run("detectors", "scorecard", "--json")
+    assert p.returncode == 0
+    data = json.loads(p.stdout)
+    assert "capabilities" in data
+    assert "training-engine" in data["capabilities"]
+
+
+def test_tracking_decide_prints_json(tmp_path, monkeypatch):
+    # Force offline via env and a remote URI to verify rewrite logic exercised end-to-end
+    env = os.environ.copy()
+    env["MLFLOW_OFFLINE"] = "1"
+    env["MLFLOW_TRACKING_URI"] = "http://mlflow.example:5000"
+    p = subprocess.run(
+        [sys.executable, "-m", "codex_ml", "tracking", "decide", "--print"],
+        capture_output=True,
+        text=True,
+        env=env,
+    )
+    assert p.returncode == 0
+    out = json.loads(p.stdout)
+    assert out["decision"]["uri"].startswith("file://")
+    assert out["decision"]["reason"]
+
+
+def test_hash_manifest_runs_on_temp_repo(tmp_path: Path):
+    # Make a tiny skeleton
+    src = tmp_path / "src" / "pkg"
+    src.mkdir(parents=True)
+    (src / "__init__.py").write_text("", encoding="utf-8")
+    (src / "mod.py").write_text("x=1\n", encoding="utf-8")
+    (tmp_path / "docs").mkdir()
+    (tmp_path / "docs" / "a.md").write_text("# A\n", encoding="utf-8")
+    p = _run("hash", "manifest", "--root", str(tmp_path), "--glob", "src/**/*.py", "--glob", "docs/**/*.md")
+    assert p.returncode == 0
+    data = json.loads(p.stdout)
+    assert data["file_count"] == 2
+    assert len(data["aggregate_sha256"]) == 64
diff --git a/docs/cli.md b/docs/cli.md
new file mode 100644
index 0000000..cc180a3
--- /dev/null
+++ b/docs/cli.md
@@ -0,0 +1,77 @@
+# Codex CLI
+
+Local, offline-first command-line utilities.
+
+## Usage
+
+Run via module entrypoint:
+
+```bash
+python -m codex_ml --help
+```
+
+### Detectors
+
+```bash
+python -m codex_ml detectors scorecard --json
+```
+
+### Tracking Guard
+
+```bash
+# Decide (and print) the enforced MLflow URI (offline-first)
+python -m codex_ml tracking decide --print
+
+# Append decision to NDJSON (see docs/observability.md)
+python -m codex_ml tracking decide --summary tracking_summary.ndjson
+
+# Apply decision to environment (and mlflow runtime if importable)
+python -m codex_ml tracking decide --apply --print
+```
+
+### Manifest Hash
+
+```bash
+python -m codex_ml hash manifest --write-readme
+```
+
+Produces a JSON report with an `aggregate_sha256` across important files and (optionally) updates a guarded badge in `README.md`.
diff --git a/docs/architecture.md b/docs/architecture.md
new file mode 100644
index 0000000..e9d6bee
--- /dev/null
+++ b/docs/architecture.md
@@ -0,0 +1,69 @@
+# Architecture (Simplified)
+
+```mermaid
+flowchart TD
+    subgraph Training
+      UT[Unified Training<br/>Facade] --> CK[Checkpoint Core]
+      UT --> TRK[Tracking Guards]
+      UT --> TOK[Tokenization API]
+    end
+
+    subgraph Observability
+      TRK --> ND[NDJSON Summary]
+      LOG[Structured JSON Logger] --> ND
+    end
+
+    subgraph Tooling
+      CLI[CLI] --> DET[Detectors]
+      CLI --> HASH[Manifest Hash]
+      DET --> SCORE[Scorecard]
+    end
+```
+
+Key modules:
+
+- `codex_ml.training.unified_training`: faÃ§ade (`run_unified_training`) with resume, grad clipping, and deterministic-friendly defaults.
+- `codex_ml.checkpointing.checkpoint_core`: save/load + RNG snapshots + integrity digest.
+- `codex_ml.tracking.guards`: offline-first MLflow/W&B guard with NDJSON summary and env-application helper.
+- `codex_ml.tokenization.api`: canonical tokenizer surface (fallback whitespace impl).
+- `codex_ml.logging.structured`: NDJSON-friendly logger.
+- `codex_ml.detectors.*`: codebase detectors and aggregate scorecard.
+- `codex_ml.cli`: local CLI glue for detectors, tracking decisions, and manifest hashing.
diff --git a/docs/releasing.md b/docs/releasing.md
new file mode 100644
index 0000000..e98635e
--- /dev/null
+++ b/docs/releasing.md
@@ -0,0 +1,54 @@
+# How We Release (Local-First)
+
+This repository favors **local/offline checks**. A minimal release flow:
+
+1. Run quality gates and tests:
+   ```bash
+   pre-commit run --all-files
+   pytest -q
+   ```
+2. Update integrity badge:
+   ```bash
+   python tools/calc_manifest_hash.py --write-readme
+   ```
+3. Review `CHANGELOG_CODEX.md` and `OPEN_QUESTIONS.md`.
+4. Tag and package as needed (no CI required).
+
+### Notes
+- Avoid enabling cost-incurring workflows; keep checks local (`nox`, `pytest`, `pre-commit`).
+- Ensure tracking guards keep MLflow/W&B local under offline modes.
diff --git a/README.md b/README.md
index b32b3c1..c1a3b8e 100644
--- a/README.md
+++ b/README.md
@@ -1,19 +1,32 @@
 # _codex_
 <!-- BEGIN-CAPABILITY-HASH-BADGE -->
 Integrity: `uninitialized` (SHA256)
 <!-- END-CAPABILITY-HASH-BADGE -->
 Local, offline-first utilities and ML training components.
 ## Quickstart
 ```bash
 python -m pip install -e .
 pre-commit install
 pytest -q
````
See `docs/` for details on training (`docs/unified_training.md`), tracking guards (`docs/observability.md`),
and integrity hashing (`docs/manifest_integrity.md`).
+
+## CLI
+
+Run via module entrypoint:
+
+`bash
+python -m codex_ml --help
+python -m codex_ml detectors scorecard --json
+python -m codex_ml tracking decide --print
+python -m codex_ml hash manifest --write-readme
+`
+
+See `docs/cli.md` for more.
diff --git a/CHANGELOG_CODEX.md b/CHANGELOG_CODEX.md
index 8778682..9a3f7d3 100644
--- a/CHANGELOG_CODEX.md
+++ b/CHANGELOG_CODEX.md
@@ -1,5 +1,26 @@
# Codex Changelog
+## 2025-10-06 â CLI (detectors, tracking, manifest) + docs polish
+
+### WHY
+- Provide a lightweight local CLI to exercise detectors, tracking guards, and integrity hashing.
+- Improve docs: architecture overview (Mermaid), CLI usage, and release checklist.
+
+### Changes
+- `src/codex_ml/cli.py` and `src/codex_ml/__main__.py` to enable `python -m codex_ml`.
+- Tests: `tests/cli/test_cli.py`.
+- Docs:
* * `docs/cli.md`
* * `docs/architecture.md` (Mermaid)
* * `docs/releasing.md`
    +- `README.md`: add CLI section.
*
+### Risk
+- Low. CLI is additive and local-only.
+
+### Rollback
+- Remove `src/codex_ml/cli.py`, `src/codex_ml/__main__.py`, and the new docs; tests will no longer invoke CLI paths.
+
## 2025-10-06 â Env application for tracking guard; NDJSON summarizer; detector aggregator
### WHY
* Make the tracking decision easy to enforce at runtime (env + optional mlflow runtime).
* Provide a minimal NDJSONâCSV summarizer for local audits of guard outputs.
```
 Updates below keep us honest with upstream best practices: W&B offline is done via `WANDB_MODE=offline` or `disabled` (with `WANDB_DISABLED` increasingly deprecated), so our guard + tests correctly key off those values. ([Weights & Biases Documentation][1]) MLflow URIs can be set with `MLFLOW_TRACKING_URI` or `mlflow.set_tracking_uri`, matching our env-application helper. ([MLflow][2]) For determinism and safe loading weâre aligning to `torch.use_deterministic_algorithms` and `torch.load(..., weights_only=True)`. ([PyTorch Documentation][3]) And since we write audit trails, weâre sticking to the NDJSON rulebook: one JSON object per line, UTF-8, newline-delimited; no embedded newlines. ([GitHub][4])
````diff
diff --git a/.pre-commit-config.yaml b/.pre-commit-config.yaml
index f8e6d14..dd5cf0c 100644
--- a/.pre-commit-config.yaml
+++ b/.pre-commit-config.yaml
@@ -1,6 +1,6 @@
 # Minimal, fast quality gates (local-only).
 # Order matters: Ruff (lint --fix) before Ruff (format).
 repos:
-  - repo: https://github.com/astral-sh/ruff-pre-commit
+  - repo: https://github.com/astral-sh/ruff-pre-commit
     rev: v0.6.9
     hooks:
       - id: ruff
@@ -14,3 +14,12 @@ repos:
       - id: check-yaml
       - id: mixed-line-ending
         args: [--fix=lf]
+  - repo: https://github.com/PyCQA/bandit
+    rev: 1.7.9
+    hooks:
+      - id: bandit
+        name: bandit (SAST)
+        args:
+          - -c
+          - bandit.yaml
+          - -q
diff --git a/bandit.yaml b/bandit.yaml
new file mode 100644
index 0000000..e4a7df3
--- /dev/null
+++ b/bandit.yaml
@@ -0,0 +1,18 @@
+# Minimal Bandit configuration (local/offline)
+# - Keep tests/docs/tools out of scope
+# - Allow asserts in tests (B101)
+exclude_dirs:
+  - tests
+  - docs
+  - tools
+skips:
+  - B101
+tests:
+  - B1
+  - B2
+  - B3
+  - B4
+  - B5
+  - B6
+  - B7
+  - B8
diff --git a/noxfile.py b/noxfile.py
new file mode 100644
index 0000000..c1b8a95
--- /dev/null
+++ b/noxfile.py
@@ -0,0 +1,97 @@
+from __future__ import annotations
+"""
+Local/offline nox sessions:
+  - lint: Ruff lint with autofix
+  - fmt: Ruff format
+  - tests: pytest
+  - cov: pytest with coverage
+  - bandit: SAST sweep over src/
+"""
+import nox
+
+
+@nox.session
+def lint(session: nox.Session) -> None:
+    session.install("ruff>=0.6.9")
+    session.run("ruff", "check", ".", "--fix", "--exit-non-zero-on-fix")
+
+
+@nox.session
+def fmt(session: nox.Session) -> None:
+    session.install("ruff>=0.6.9")
+    session.run("ruff", "format", ".")
+
+
+@nox.session
+def tests(session: nox.Session) -> None:
+    session.install("pytest>=7.0")
+    session.run("pytest", "-q")
+
+
+@nox.session
+def cov(session: nox.Session) -> None:
+    session.install("pytest>=7.0", "pytest-cov>=4.0")
+    session.run("pytest", "--cov=src", "--cov-report=term-missing", "-q")
+
+
+@nox.session
+def bandit(session: nox.Session) -> None:
+    session.install("bandit>=1.7.9")
+    session.run("bandit", "-c", "bandit.yaml", "-q", "-r", "src")
diff --git a/docs/quality_gates.md b/docs/quality_gates.md
index 4c1a4f9..fa2d8a1 100644
--- a/docs/quality_gates.md
+++ b/docs/quality_gates.md
@@ -1,6 +1,7 @@
 # Quality Gates (Local-Only)
 This repo ships a minimal set of **fast** quality gates that run offline:
 - **Ruff** (lint with autofix, then format)
 - **pre-commit-hooks** (AST check, whitespace, YAML)
+- **Bandit** (SAST sweep over `src/`)
@@ -15,6 +16,26 @@ pre-commit install
 Run on changed files automatically, or invoke manually:
 ```bash
 pre-commit run --all-files
````
+## nox sessions (optional)
+
+Prefer `nox` for repeatable local runs:
+
+`bash
+python -m pip install nox
+nox -s lint
+nox -s fmt
+nox -s tests
+nox -s cov
+nox -s bandit
+`
+
+These sessions are **local-only** and avoid any network calls. They mirror the same gates:
+- `lint`/`fmt`: Ruff
+- `tests`/`cov`: pytest + coverage
+- `bandit`: static analysis
+
### Notes
* Ruff order: place the lint `--fix` hook **before** the formatter hook to avoid formatting churn.
* Hooks are intentionally minimal to keep local checks fast.
No GitHub Actions are enabled; all checks are local/offline.
diff --git a/CHANGELOG_CODEX.md b/CHANGELOG_CODEX.md
index 9a3f7d3..c1a5b0b 100644
--- a/CHANGELOG_CODEX.md
+++ b/CHANGELOG_CODEX.md
@@ -1,5 +1,24 @@
# Codex Changelog
+## 2025-10-06 â Quality gates: nox sessions + Bandit in pre-commit
+
+### WHY
+- Round out the âQuality gatesâ item with reproducible local sessions and a lightweight SAST pass.
+
+### Changes
+- Added `noxfile.py` with sessions: `lint`, `fmt`, `tests`, `cov`, `bandit`.
+- Added `bandit.yaml` and wired Bandit into `.pre-commit-config.yaml`.
+- Expanded `docs/quality_gates.md` with nox usage.
+
+### Risk
+- Low. Nox and Bandit are optional local tools; no CI is activated.
+
+### Rollback
+- Remove `noxfile.py` and the Bandit hook from `.pre-commit-config.yaml`.
+- Delete `bandit.yaml` if not desired.
+
## 2025-10-06 â CLI (detectors, tracking, manifest) + docs polish
### WHY
* Provide a lightweight local CLI to exercise detectors, tracking guards, and integrity hashing.
* Improve docs: architecture overview (Mermaid), CLI usage, and release checklist.
```
````diff
diff --git a/tools/scan_secrets.py b/tools/scan_secrets.py
new file mode 100755
index 0000000..a6f6bb3
--- /dev/null
+++ b/tools/scan_secrets.py
@@ -0,0 +1,236 @@
+#!/usr/bin/env python3
+from __future__ import annotations
+"""
+Lightweight, offline secret scanner for this repo.
+
+WHAT IT DOES
+- Walks the repository for likely credential formats (AWS key IDs, GitHub PATs, Google API keys, Slack/Stripe tokens).
+- Prints a JSON report. Non-zero exit when --fail-on-find is set and suspected secrets are found.
+
+SCOPE
+- Excludes common non-source paths by default (tests/, docs/, tools/, .git/, venvs).
+- Override with --include-tests to scan tests/docs/tools as well.
+
+NOTE
+- Patterns are intentionally conservative to reduce false positives.
+- This tool is *advisory*. Always rotate any suspected real credential.
+"""
+import argparse
+import json
+import os
+import re
+from dataclasses import dataclass, asdict
+from pathlib import Path
+from typing import Dict, Iterable, Iterator, List, Pattern, Tuple
+
+
+# --- Patterns (conservative) ---
+# References: AWS AKIA/ASIA key IDs; GitHub PAT prefixes (ghp_), Google API key (AIza...), Slack, Stripe.
+PATTERNS: List[Tuple[str, Pattern[str]]] = [
+    ("aws_access_key_id", re.compile(r"\b(?:ASIA|AKIA|AIDA|AROA)[A-Z0-9]{16}\b")),
+    ("aws_secret_key", re.compile(r"(?i)aws(.{0,20})?(?-i)['\"][0-9A-Za-z/+]{40}['\"]")),
+    ("github_pat_classic", re.compile(r"\bghp_[A-Za-z0-9]{36}\b")),
+    ("google_api_key", re.compile(r"\bAIza[0-9A-Za-z\-_]{35}\b")),
+    ("slack_token", re.compile(r"\bxox[baprs]-[0-9A-Za-z-]{10,}\b")),
+    ("stripe_live_secret", re.compile(r"\bsk_live_[0-9A-Za-z]{24}\b")),
+    ("stripe_live_public", re.compile(r"\bpk_live_[0-9A-Za-z]{24}\b")),
+]
+
+
+DEFAULT_EXCLUDES = {
+    ".git",
+    ".hg",
+    ".svn",
+    ".tox",
+    ".mypy_cache",
+    ".pytest_cache",
+    ".ruff_cache",
+    "node_modules",
+    "venv",
+    ".venv",
+    "__pycache__",
+    "dist",
+    "build",
+    "site-packages",
+    # Project-local excludes (avoid false positives in tests/docs/tools by default)
+    "tests",
+    "docs",
+    "tools",
+}
+
+TEXT_EXTS = {
+    ".py", ".md", ".txt", ".json", ".yaml", ".yml", ".toml", ".ini",
+    ".cfg", ".sh", ".bash", ".zsh", ".env", ".properties", ".csv",
+}
+
+
+@dataclass
+class Finding:
+    type: str
+    file: str
+    line: int
+    column: int
+    snippet: str
+
+
+def iter_files(root: Path, *, include_tests: bool = False) -> Iterator[Path]:
+    for p in root.rglob("*"):
+        if not p.is_file():
+            continue
+        # Exclude by top-level dir names
+        parts = set(p.parts)
+        excludes = set(DEFAULT_EXCLUDES)
+        if include_tests:
+            excludes = {x for x in excludes if x not in {"tests", "docs", "tools"}}
+        if parts & excludes:
+            continue
+        # Heuristic: only scan known text extensions or smallish unknown files
+        if p.suffix.lower() in TEXT_EXTS:
+            yield p
+            continue
+        try:
+            if p.stat().st_size <= 256 * 1024:  # 256 KiB cutoff for unknowns
+                yield p
+        except Exception:
+            continue
+
+
+def scan_path(path: Path) -> List[Finding]:
+    out: List[Finding] = []
+    try:
+        data = path.read_text(encoding="utf-8", errors="ignore")
+    except Exception:
+        return out
+    for i, line in enumerate(data.splitlines(), start=1):
+        for name, pat in PATTERNS:
+            for m in pat.finditer(line):
+                start = max(0, m.start() - 4)
+                end = min(len(line), m.end() + 4)
+                snippet = line[start:end]
+                out.append(Finding(type=name, file=str(path), line=i, column=m.start() + 1, snippet=snippet))
+    return out
+
+
+def scan(root: Path, *, include_tests: bool = False) -> List[Finding]:
+    findings: List[Finding] = []
+    for p in iter_files(root, include_tests=include_tests):
+        findings.extend(scan_path(p))
+    return findings
+
+
+def main(argv: List[str] | None = None) -> int:
+    ap = argparse.ArgumentParser(description="Scan repository for likely secrets (offline).")
+    ap.add_argument("--root", type=str, default=".", help="Repository root (default: .)")
+    ap.add_argument("--include-tests", action="store_true", help="Scan tests/docs/tools too")
+    ap.add_argument("--fail-on-find", action="store_true", help="Return non-zero if any finding is detected")
+    args = ap.parse_args(argv)
+    root = Path(args.root).resolve()
+    res = scan(root, include_tests=bool(args.include_tests))
+    payload = {
+        "root": root.as_posix(),
+        "count": len(res),
+        "findings": [asdict(f) for f in res],
+    }
+    print(json.dumps(payload, indent=2))
+    if args.fail_on_find and res:
+        return 1
+    return 0
+
+
+if __name__ == "__main__":
+    raise SystemExit(main())
diff --git a/tests/tools/test_scan_secrets.py b/tests/tools/test_scan_secrets.py
new file mode 100644
index 0000000..e91be41
--- /dev/null
+++ b/tests/tools/test_scan_secrets.py
@@ -0,0 +1,73 @@
+import json
+import os
+import subprocess
+import sys
+from pathlib import Path
+
+from tools.scan_secrets import scan
+
+
+def test_scan_detects_common_patterns(tmp_path: Path):
+    # Create sample files
+    f = tmp_path / "app.py"
+    f.write_text(
+        "\n".join(
+            [
+                "AWS_KEY='AKIA1234567890ABCD'",
+                "AWS_SECRET='abcdAWS\"wxyzABCDEFGHIJKLMOPQRSTUVWX123456'  # not a real secret",
+                "GITHUB='ghp_abcdefghijklmnopqrstuvwxyzABCD12'",
+                "GOOGLE='AIzaAABBCCDDEEFF0011223344556677889900'",
+                "SLACK='xoxb-123456789012-AAAABBBBCCCC'",
+                "STRIPE='sk_live_0123456789abcdefghijkl'",
+            ]
+        ),
+        encoding="utf-8",
+    )
+    results = scan(tmp_path, include_tests=True)
+    kinds = {r.type for r in results}
+    assert "aws_access_key_id" in kinds
+    assert "github_pat_classic" in kinds
+    assert "google_api_key" in kinds
+    assert "slack_token" in kinds
+    assert "stripe_live_secret" in kinds
+
+
+def test_cli_json_and_exit_codes(tmp_path: Path):
+    (tmp_path / "x.txt").write_text("token=ghp_abcdefghijklmnopqrstuvwxyzABCD12\n", encoding="utf-8")
+    script = Path("tools/scan_secrets.py").resolve()
+    p = subprocess.run(
+        [sys.executable, str(script), "--root", str(tmp_path), "--include-tests"],
+        capture_output=True,
+        text=True,
+        check=True,
+    )
+    data = json.loads(p.stdout)
+    assert data["count"] >= 1
+    p2 = subprocess.run(
+        [sys.executable, str(script), "--root", str(tmp_path), "--include-tests", "--fail-on-find"],
+        capture_output=True,
+        text=True,
+    )
+    assert p2.returncode == 1
diff --git a/docs/secrets.md b/docs/secrets.md
new file mode 100644
index 0000000..c54a8a1
--- /dev/null
+++ b/docs/secrets.md
@@ -0,0 +1,63 @@
+# Secrets & Credentials (Local Sweep)
+
+This repo includes a lightweight, offline **secret scanner** to reduce the chance of leaking credentials.
+
+## Run a scan
+
+```bash
+python tools/scan_secrets.py --fail-on-find
+```
+
+By default, we skip `tests/`, `docs/`, and `tools/` to avoid false positives. To include them:
+
+```bash
+python tools/scan_secrets.py --include-tests
+```
+
+The scanner emits a JSON report and returns a non-zero exit when `--fail-on-find` is used and any findings are detected.
+
+### Patterns covered (conservative)
+- AWS Access Key IDs (e.g., `AKIA...` / `ASIA...`)
+- GitHub personal access tokens (classic) `ghp_...`
+- Google API keys `AIza...`
+- Slack tokens `xox...`
+- Stripe live keys `sk_live_...` / `pk_live_...`
+
+> Always rotate any suspected real secret immediately and invalidate exposed tokens upstream.
+
+## Pre-commit (optional)
+
+We wire the scanner as a local pre-commit hook to run **offline** before committing.
+
+```bash
+pre-commit run --all-files
+```
+
+## Notes
+- False positives can happen. Treat this as a guardrail, not a guarantee.
+- Do not store real secrets in the repository. Prefer environment variables or a secrets manager.
diff --git a/tools/bench_tokenizer.py b/tools/bench_tokenizer.py
new file mode 100755
index 0000000..6fa41c7
--- /dev/null
+++ b/tools/bench_tokenizer.py
@@ -0,0 +1,72 @@
+#!/usr/bin/env python3
+from __future__ import annotations
+"""
+Micro-benchmark: whitespace tokenizer encode throughput.
+(Local/offline; prints ops/sec for a simple workload.)
+"""
+import timeit
+from codex_ml.tokenization.api import WhitespaceTokenizer
+
+
+def bench(n_docs: int = 5_000, words: int = 16, repeat: int = 5) -> None:
+    tok = WhitespaceTokenizer()
+    texts = [" ".join([f"w{i%50}" for i in range(words)]) for _ in range(n_docs)]
+    t = timeit.Timer(lambda: [tok.encode(t) for t in texts])
+    res = t.repeat(repeat=repeat, number=1)
+    best = min(res)
+    rate = n_docs / best
+    print(f"docs={n_docs} words={words} best={best:.4f}s => {rate:.1f} docs/sec")
+
+
+if __name__ == "__main__":
+    bench()
diff --git a/docs/performance.md b/docs/performance.md
new file mode 100644
index 0000000..6b1def0
--- /dev/null
+++ b/docs/performance.md
@@ -0,0 +1,33 @@
+# Performance Notes
+
+We include small, **local** micro-benchmarks to spot obvious regressions without enabling any CI or remote services.
+
+## Tokenizer
+
+```bash
+python tools/bench_tokenizer.py
+```
+
+This prints a simple `docs/sec` metric for the fallback whitespace tokenizer. Treat it as a coarse, reproducible signal.
+
+## Tips
+- Avoid unnecessary allocations in hot loops; reuse objects where possible.
+- Prefer list comprehensions and local variables in Python for tight loops.
diff --git a/docs/incident_runbook.md b/docs/incident_runbook.md
new file mode 100644
index 0000000..3c9f5fc
--- /dev/null
+++ b/docs/incident_runbook.md
@@ -0,0 +1,64 @@
+# Incident Runbook (Local/Offline)
+
+When something breaks (or looks suspicious), use this quick checklist:
+
+## 1) Freeze the Scene
+- Stop runs that might be leaking data (disable remote tracking; ensure `MLFLOW_OFFLINE=1`, `WANDB_MODE=offline|disabled`).
+- Capture logs and the NDJSON tracking summary (`tracking_summary.ndjson` if enabled).
+
+## 2) Snapshot State
+- If training, save a checkpoint and record the `digest_sha256` in `metadata.json`.
+- Record the environment (Python, CUDA, PyTorch) and key env vars (`MLFLOW_*`, `WANDB_*`).
+
+## 3) Triage
+- Reproduce deterministically:
+  - Set a fixed random seed or use deterministic algorithms.
+  - Re-run minimal failing tests (`pytest -k "name"`).
+- Check data determinism tests (shard coverage, seed effects).
+
+## 4) Containment
+- Rotate any suspected credentials immediately.
+- Run the local secret scanner:
+  ```bash
+  python tools/scan_secrets.py --fail-on-find
+  ```
+
+## 5) Communicate
+- Keep notes: timeline, hypotheses, reproduction steps, hashes.
+- Update `OPEN_QUESTIONS.md` if decisions are needed.
+
+## 6) Postmortem (Short)
+- What failed? How did we detect it?
+- What do we change (tests, guards, docs)?
+
+This runbook is designed for **offline** response with minimal dependencies.
diff --git a/.pre-commit-config.yaml b/.pre-commit-config.yaml
index dd5cf0c..2ec8c6d 100644
--- a/.pre-commit-config.yaml
+++ b/.pre-commit-config.yaml
@@ -20,3 +20,19 @@ repos:
       - id: mixed-line-ending
         args: [--fix=lf]
   - repo: https://github.com/PyCQA/bandit
     rev: 1.7.9
     hooks:
       - id: bandit
         name: bandit (SAST)
         args:
           - -c
           - bandit.yaml
           - -q
+  - repo: local
+    hooks:
+      - id: codex-scan-secrets
+        name: codex (scan secrets)
+        language: system
+        entry: python tools/scan_secrets.py --fail-on-find
+        pass_filenames: false
+        stages: [commit]
diff --git a/README.md b/README.md
index c1a3b8e..8c63f36 100644
--- a/README.md
+++ b/README.md
@@ -20,3 +20,16 @@ See `docs/` for details on training (`docs/unified_training.md`), tracking guards (`docs/observability.md`),
 and integrity hashing (`docs/manifest_integrity.md`).
 ## CLI
@@ -40,3 +53,16 @@ python -m codex_ml hash manifest --write-readme
````
See `docs/cli.md` for more.
+
+## Security & Performance
+
+- **Secrets sweep** (offline):
* ```bash
  ```
* python tools/scan_secrets.py --fail-on-find
* ```
  ```
* See `docs/secrets.md`.
*
+- **Micro-benchmarks**:
* ```bash
  ```
* python tools/bench_tokenizer.py
* ```
  ```
* See `docs/performance.md`.
  diff --git a/CHANGELOG_CODEX.md b/CHANGELOG_CODEX.md
  index c1a5b0b..0a50cf8 100644
  --- a/CHANGELOG_CODEX.md
  +++ b/CHANGELOG_CODEX.md
  @@ -1,5 +1,28 @@
# Codex Changelog
+## 2025-10-06 â Security sweep + micro-bench + runbook
+
+### WHY
+- Add an offline, conservative **secret scanner** to reduce credential leakage risk.
+- Provide a tiny tokenizer micro-benchmark to spot obvious regressions.
+- Include a short incident runbook for offline-first response.
+
+### Changes
+- Tools:
* * `tools/scan_secrets.py` (JSON report; `--fail-on-find`; excludes tests/docs/tools by default).
* * `tools/bench_tokenizer.py` (encode throughput micro-bench).
    +- Tests:
* * `tests/tools/test_scan_secrets.py`.
    +- Docs:
* * `docs/secrets.md`, `docs/performance.md`, `docs/incident_runbook.md`.
    +- Pre-commit:
* * Local hook `codex-scan-secrets` (offline).
    +- README:
* * Security & Performance section with quick commands.
*
+### Risk
+- Low. Scanner is advisory; pre-commit hook is local-only.
+
## 2025-10-06 â Quality gates: nox sessions + Bandit in pre-commit
### WHY
* Round out the âQuality gatesâ item with reproducible local sessions and a lightweight SAST pass.
```
````diff
diff --git a/src/codex_ml/checkpointing/checkpoint_core.py b/src/codex_ml/checkpointing/checkpoint_core.py
index 8a5db0b..a2746c1 100644
--- a/src/codex_ml/checkpointing/checkpoint_core.py
+++ b/src/codex_ml/checkpointing/checkpoint_core.py
@@ -16,6 +16,7 @@ import hashlib
 import shutil
 import random as _py_random
 from typing import Dict, Any, Tuple, Optional
+from glob import glob
 try:
     import torch
 except Exception:  # pragma: no cover
@@ -93,6 +94,36 @@ def apply_rng_state(state: Dict[str, Any]) -> None:
     except Exception:  # pragma: no cover
         pass
+def prune_checkpoints(parent_dir: str, keep_last_k: int = 5, pattern: str = r"^epoch-\d+$") -> list[str]:
+    """
+    Remove old checkpoint subdirectories under `parent_dir`, keeping only the last K by lexicographic order.
+    Returns the list of removed directories (basenames).
+
+    This is *best-effort* and only removes directories whose basenames match the regex `pattern`.
+    """
+    try:
+        pat = re.compile(pattern)
+        sibs = sorted([
+            d for d in os.listdir(parent_dir)
+            if os.path.isdir(os.path.join(parent_dir, d)) and pat.match(d)
+        ])
+        excess = len(sibs) - max(keep_last_k, 0)
+        removed = []
+        for d in sibs[: max(0, excess)]:
+            shutil.rmtree(os.path.join(parent_dir, d), ignore_errors=True)
+            removed.append(d)
+        return removed
+    except Exception:
+        return []
+
+def _maybe_prune(parent: str, keep_last_k: int, pattern: str = r"^epoch-\d+$") -> None:
+    try:
+        prune_checkpoints(parent, keep_last_k=keep_last_k, pattern=pattern)
+    except Exception:
+        # best-effort only
+        pass
+
+
 def calculate_digest(out_dir: str) -> str:
     """
     Calculate a digest over weights + metadata for integrity tracking.
@@ -151,16 +182,10 @@ def save_checkpoint(out_dir: str, *, state: Dict[str, Any], meta: Dict[str, Any], keep_last_k: int = 5) -> str:
     except Exception:
         pass
     # Retention (best-effort): keep only the last K sibling epoch dirs
     try:
         parent = os.path.dirname(out_dir)
-        pat = re.compile(r"^epoch-\d+$")
-        siblings = sorted([d for d in os.listdir(parent) if os.path.isdir(os.path.join(parent, d)) and pat.match(d)])
-        excess = len(siblings) - max(keep_last_k, 0)
-        for d in siblings[: max(0, excess)]:
-            shutil.rmtree(os.path.join(parent, d), ignore_errors=True)
-    except Exception:
-        pass
+        _maybe_prune(parent, keep_last_k=keep_last_k, pattern=r"^epoch-\d+$")
+    except Exception:
+        pass
     return out_dir
 def load_checkpoint(path: str, map_location: Optional[str] = None) -> Tuple[Dict[str, Any], Dict[str, Any]]:
@@ -209,6 +234,7 @@ __all__ = [
     "save_checkpoint",
     "load_checkpoint",
     "calculate_digest",
+    "prune_checkpoints",
     "capture_rng_state",
     "apply_rng_state",
     "SCHEMA_VERSION",
 ]
diff --git a/src/codex_ml/checkpointing/__init__.py b/src/codex_ml/checkpointing/__init__.py
new file mode 100644
index 0000000..cb3e5f5
--- /dev/null
+++ b/src/codex_ml/checkpointing/__init__.py
@@ -0,0 +1,18 @@
+"""
+Checkpointing public surface.
+"""
+from .checkpoint_core import (
+    save_checkpoint,
+    load_checkpoint,
+    calculate_digest,
+    prune_checkpoints,
+    capture_rng_state,
+    apply_rng_state,
+    SCHEMA_VERSION,
+)
+
+__all__ = [
+    "save_checkpoint", "load_checkpoint", "calculate_digest", "prune_checkpoints",
+    "capture_rng_state", "apply_rng_state", "SCHEMA_VERSION",
+]
+
diff --git a/src/codex_ml/checkpointing/compat.py b/src/codex_ml/checkpointing/compat.py
new file mode 100644
index 0000000..d1f36e5
--- /dev/null
+++ b/src/codex_ml/checkpointing/compat.py
@@ -0,0 +1,44 @@
+from __future__ import annotations
+"""
+Deprecated wrappers for prior checkpoint APIs.
+"""
+import warnings
+from typing import Dict, Any, Tuple, Optional
+from .checkpoint_core import save_checkpoint as _save, load_checkpoint as _load, calculate_digest as _digest
+
+
+def save_checkpoint_legacy(out_dir: str, *, state: Dict[str, Any], meta: Dict[str, Any], keep_last_k: int = 5) -> str:
+    warnings.warn(
+        "save_checkpoint_legacy is deprecated; use codex_ml.checkpointing.save_checkpoint",
+        category=DeprecationWarning,
+        stacklevel=2,
+    )
+    return _save(out_dir, state=state, meta=meta, keep_last_k=keep_last_k)
+
+
+def load_checkpoint_legacy(path: str, map_location: Optional[str] = None) -> Tuple[Dict[str, Any], Dict[str, Any]]:
+    warnings.warn(
+        "load_checkpoint_legacy is deprecated; use codex_ml.checkpointing.load_checkpoint",
+        category=DeprecationWarning,
+        stacklevel=2,
+    )
+    return _load(path, map_location=map_location)
+
+
+def calculate_digest_legacy(out_dir: str) -> str:
+    warnings.warn(
+        "calculate_digest_legacy is deprecated; use codex_ml.checkpointing.calculate_digest",
+        category=DeprecationWarning,
+        stacklevel=2,
+    )
+    return _digest(out_dir)
+
+
+__all__ = [
+    "save_checkpoint_legacy",
+    "load_checkpoint_legacy",
+    "calculate_digest_legacy",
+]
diff --git a/tests/checkpointing/test_checkpoint_schema_and_compat.py b/tests/checkpointing/test_checkpoint_schema_and_compat.py
new file mode 100644
index 0000000..c9e6070
--- /dev/null
+++ b/tests/checkpointing/test_checkpoint_schema_and_compat.py
@@ -0,0 +1,108 @@
+import os
+import json
+import warnings
+import pytest
+
+torch = pytest.importorskip("torch")
+
+from codex_ml.checkpointing import (
+    save_checkpoint,
+    load_checkpoint,
+    calculate_digest,
+    prune_checkpoints,
+    SCHEMA_VERSION,
+)
+from codex_ml.checkpointing import compat as ckpt_compat
+
+
+def test_metadata_schema_and_digest(tmp_path):
+    out = tmp_path / "epoch-0000"
+    state = {"model": {"w": torch.randn(2)}, "optimizer": {"step": 3}}
+    meta = {"epoch": 0, "note": "test"}
+    save_checkpoint(str(out), state=state, meta=meta, keep_last_k=2)
+    # Schema: metadata.json must contain schema_version and digest_sha256
+    meta_path = out / "metadata.json"
+    assert meta_path.exists()
+    m = json.loads(meta_path.read_text(encoding="utf-8"))
+    assert m.get("schema_version") == SCHEMA_VERSION
+    assert isinstance(m.get("digest_sha256"), str) and len(m["digest_sha256"]) == 64
+    # calculate_digest must agree
+    assert calculate_digest(str(out)) == m["digest_sha256"]
+
+
+def test_prune_checkpoints_function(tmp_path):
+    base = tmp_path
+    for i in range(5):
+        ep = base / f"epoch-{i:04d}"
+        save_checkpoint(str(ep), state={"model": {}}, meta={"epoch": i}, keep_last_k=99)
+    removed = prune_checkpoints(str(base), keep_last_k=2)
+    # Only the oldest three should be removed
+    assert removed == ["epoch-0000", "epoch-0001", "epoch-0002"]
+    remain = sorted([d for d in os.listdir(base) if (base / d).is_dir()])
+    assert remain == ["epoch-0003", "epoch-0004"]
+
+
+def test_legacy_compat_warns_and_matches_artifacts(tmp_path):
+    out_new = tmp_path / "epoch-0000"
+    out_old = tmp_path / "epoch-0001"
+    state = {"model": {"w": torch.randn(1)}, "optimizer": {"step": 1}}
+    meta = {"epoch": 0}
+    # New save
+    save_checkpoint(str(out_new), state=state, meta=meta, keep_last_k=99)
+    # Legacy save (emits warning)
+    with warnings.catch_warnings(record=True) as rec:
+        warnings.simplefilter("always", DeprecationWarning)
+        ckpt_compat.save_checkpoint_legacy(str(out_old), state=state, meta=meta, keep_last_k=99)
+        assert any(isinstance(w.message, DeprecationWarning) for w in rec)
+    # Artifacts should both load and produce similar states
+    s_new, m_new = load_checkpoint(str(out_new))
+    s_old, m_old = ckpt_compat.load_checkpoint_legacy(str(out_old))
+    assert s_new.keys() == s_old.keys()
+    assert m_new.get("epoch") == m_old.get("epoch") == 0
+    # Digests exist for both
+    d_new = calculate_digest(str(out_new))
+    d_old = ckpt_compat.calculate_digest_legacy(str(out_old))
+    assert len(d_new) == len(d_old) == 64
diff --git a/docs/checkpointing_core.md b/docs/checkpointing_core.md
new file mode 100644
index 0000000..b0a1b7e
--- /dev/null
+++ b/docs/checkpointing_core.md
@@ -0,0 +1,86 @@
+# Checkpoint Core
+
+Canonical module: `codex_ml.checkpointing.checkpoint_core` (re-exported via `codex_ml.checkpointing`).
+
+## API
+
+```python
+from codex_ml.checkpointing import (
+  save_checkpoint, load_checkpoint, calculate_digest, prune_checkpoints,
+  capture_rng_state, apply_rng_state, SCHEMA_VERSION,
+)
+```
+
+- **`save_checkpoint(out_dir, state, meta, keep_last_k=5)`**
+  Writes `weights.pt` (state with `schema_version`) and `metadata.json` (includes `schema_version` and `digest_sha256`).
+  Best-effort RNG snapshot is embedded into `state["rng"]` if missing. Performs retention of `epoch-####` siblings.
+
+- **`load_checkpoint(path, map_location=None)`**
+  Loads `(state, meta)`. Prefers `torch.load(..., weights_only=True)` when available.
+
+- **`calculate_digest(out_dir)`**
+  Computes a SHA256 over `weights.pt` + `metadata.json` for integrity checks.
+
+- **`prune_checkpoints(parent_dir, keep_last_k=5, pattern="^epoch-\\d+$")`**
+  Removes older checkpoint subdirs and returns the list of removed basenames.
+
+- **`capture_rng_state()` / `apply_rng_state(state)`**
+  Snapshot/restore RNG for Python, NumPy (if present), and Torch CPU/CUDA/XPU to enable resume parity.
+
+## Schema
+
+`metadata.json` includes:
+
+```json
+{
+  "schema_version": "1",
+  "epoch": 3,
+  "loss": 0.1234,
+  "digest_sha256": "..."  // integrity over weights + metadata
+}
+```
+
+## Deprecation
+
+Legacy wrappers are available in `codex_ml.checkpointing.compat`:
+
+- `save_checkpoint_legacy`, `load_checkpoint_legacy`, `calculate_digest_legacy`
+
+They emit `DeprecationWarning` and delegate to the canonical API.
diff --git a/CHANGELOG_CODEX.md b/CHANGELOG_CODEX.md
index 0a50cf8..6b8f8a3 100644
--- a/CHANGELOG_CODEX.md
+++ b/CHANGELOG_CODEX.md
@@ -1,5 +1,24 @@
 # Codex Changelog
+## 2025-10-06 â Checkpoint consolidation: public surface, pruning API, compat shims
+
+### WHY
+- Close P2 acceptance criteria for checkpoint manager consolidation:
+  - Expose a single public surface with load+save+digest+retention.
+  - Provide explicit pruning API.
+  - Add legacy wrappers with `DeprecationWarning`.
+  - Validate metadata schema in tests.
+
+### Changes
+- `checkpoint_core`: add `prune_checkpoints(...)` and reuse in retention step.
+- Public re-export: `codex_ml.checkpointing.__init__`.
+- Compat: `codex_ml.checkpointing.compat` with legacy wrappers.
+- Tests: metadata schema & digest, prune API, compat parity and deprecation.
+- Docs: `docs/checkpointing_core.md`.
+
+### Risk
+- Low. Shims are thin; pruning is opt-in and pattern-gated.
+
 ## 2025-10-06 â Security sweep + micro-bench + runbook
 ### WHY
 - Add an offline, conservative **secret scanner** to reduce credential leakage risk.
 - Provide a tiny tokenizer micro-benchmark to spot obvious regressions.
````
```diff
diff --git a/src/codex_ml/cli.py b/src/codex_ml/cli.py
new file mode 100644
index 0000000..5d4e2a9
--- /dev/null
+++ b/src/codex_ml/cli.py
@@ -0,0 +1,240 @@
+from __future__ import annotations
+"""
+Minimal local/offline CLI for _codex_.
+
+Usage examples:
+  python -m codex_ml hash manifest --write-readme
+  python -m codex_ml tracking decide --summary tracking_summary.ndjson
+"""
+import argparse
+import json
+import os
+import sys
+from pathlib import Path
+from typing import Dict, Iterable, List, Tuple
+
+from codex_ml.data.utils import manifest_checksums
+from codex_ml.tracking.guards import decide_mlflow_tracking_uri
+
+BADGE_BEGIN = "<!-- BEGIN-CAPABILITY-HASH-BADGE -->"
+BADGE_END = "<!-- END-CAPABILITY-HASH-BADGE -->"
+
+
+def _iter_globs(root: Path, globs: Iterable[str]) -> List[str]:
+    out: List[str] = []
+    seen = set()
+    for pattern in globs:
+        for p in root.glob(pattern):
+            if p.is_file():
+                k = p.resolve().as_posix()
+                if k not in seen:
+                    seen.add(k)
+                    out.append(k)
+    out.sort()
+    return out
+
+
+def _default_manifest_globs() -> List[str]:
+    # Keep defaults conservative and stable
+    return [
+        "src/**/*.py",
+        "docs/**/*.md",
+        "tools/*.py",
+        "README.md",
+    ]
+
+
+def _update_readme_badge(readme_path: Path, digest: str) -> None:
+    """
+    Replace or insert the capability hash badge block in README.
+    """
+    tmpl = (
+        f"{BADGE_BEGIN}\n"
+        f"Integrity: `{digest}` (SHA256)\n"
+        f"{BADGE_END}\n"
+    )
+    if not readme_path.exists():
+        # Create a minimal README with the badge at top
+        readme_path.write_text(f"# _codex_\n\n{tmpl}\n", encoding="utf-8")
+        return
+    text = readme_path.read_text(encoding="utf-8")
+    if BADGE_BEGIN in text and BADGE_END in text:
+        pre, rest = text.split(BADGE_BEGIN, 1)
+        _, post = rest.split(BADGE_END, 1)
+        new_text = f"{pre}{tmpl}{post.lstrip()}"
+    else:
+        # Insert near the top, after a heading if present
+        new_text = text
+        if new_text.startswith("#"):
+            # After first blank line following heading
+            lines = new_text.splitlines()
+            idx = 0
+            while idx < len(lines) and lines[idx].startswith("#"):
+                idx += 1
+            # Insert a blank line and the badge block
+            lines[idx:idx] = ["", *tmpl.strip("\n").splitlines(), ""]
+            new_text = "\n".join(lines) + ("\n" if not new_text.endswith("\n") else "")
+        else:
+            new_text = f"{tmpl}\n{new_text}"
+    if not new_text.endswith("\n"):
+        new_text += "\n"
+    readme_path.write_text(new_text, encoding="utf-8")
+
+
+def _cmd_hash_manifest(args: argparse.Namespace) -> int:
+    root = Path(args.root or ".").resolve()
+    globs = args.glob or _default_manifest_globs()
+    files = _iter_globs(root, globs)
+    per, aggregate = manifest_checksums(files)
+    payload: Dict[str, object] = {
+        "root": root.as_posix(),
+        "files": [{"path": k, "sha256": v} for k, v in per.items()],
+        "aggregate_sha256": aggregate,
+        "count": len(per),
+        "schema_version": 1,
+        "globs": globs,
+    }
+    # Print JSON to stdout
+    print(json.dumps(payload, indent=2))
+    if args.write_readme:
+        _update_readme_badge(root / "README.md", aggregate)
+    return 0
+
+
+def _cmd_tracking_decide(args: argparse.Namespace) -> int:
+    # Evaluate decision using current environment (offline-first stance)
+    d = decide_mlflow_tracking_uri(environ=dict(os.environ))
+    # Always print decision JSON
+    print(json.dumps(
+        {
+            "uri": d.uri,
+            "blocked": d.blocked,
+            "reason": d.reason,
+            "details": d.details,
+        },
+        indent=2,
+    ))
+    # Append to NDJSON summary if requested
+    if args.summary:
+        p = Path(args.summary)
+        line = json.dumps(
+            {"ts": d.details.get("ts") or None, "decision": {"uri": d.uri, "blocked": d.blocked, "reason": d.reason, "details": d.details}},
+            ensure_ascii=False,
+        )
+        with p.open("a", encoding="utf-8") as f:
+            f.write(line + "\n")
+    # --apply is a no-op for now (placeholder for future env rewriting)
+    return 0
+
+
+def _build_parser() -> argparse.ArgumentParser:
+    ap = argparse.ArgumentParser(prog="codex_ml", description="_codex_ local/offline CLI")
+    sub = ap.add_subparsers(dest="cmd", required=True)
+
+    # hash manifest
+    ap_hash = sub.add_parser("hash", help="Hash utilities")
+    sub_hash = ap_hash.add_subparsers(dest="hash_cmd", required=True)
+    ap_manifest = sub_hash.add_parser("manifest", help="Compute manifest aggregate hash")
+    ap_manifest.add_argument("--root", type=str, default=".", help="Repository root (default: .)")
+    ap_manifest.add_argument("--glob", action="append", default=[], help="Glob(s) to include in manifest")
+    ap_manifest.add_argument("--write-readme", action="store_true", help="Update README badge with the digest")
+    ap_manifest.set_defaults(func=_cmd_hash_manifest)
+
+    # tracking decide
+    ap_track = sub.add_parser("tracking", help="Tracking utilities")
+    sub_track = ap_track.add_subparsers(dest="track_cmd", required=True)
+    ap_decide = sub_track.add_parser("decide", help="Decide MLflow tracking URI (offline-first)")
+    ap_decide.add_argument("--summary", type=str, help="Append decision to NDJSON summary file")
+    ap_decide.add_argument("--apply", action="store_true", help="(reserved) Apply decision to environment")
+    ap_decide.set_defaults(func=_cmd_tracking_decide)
+    return ap
+
+
+def main(argv: List[str] | None = None) -> int:
+    parser = _build_parser()
+    args = parser.parse_args(argv if argv is not None else sys.argv[1:])
+    func = getattr(args, "func", None)
+    if func is None:
+        parser.print_help()
+        return 2
+    return int(func(args) or 0)
+
+
+if __name__ == "__main__":
+    raise SystemExit(main())
diff --git a/src/codex_ml/__main__.py b/src/codex_ml/__main__.py
new file mode 100644
index 0000000..8a1f7f2
--- /dev/null
+++ b/src/codex_ml/__main__.py
@@ -0,0 +1,8 @@
+from __future__ import annotations
+from . import cli
+
+
+def _run() -> int:
+    return cli.main()
+
+
+if __name__ == "__main__":
+    raise SystemExit(_run())
diff --git a/src/codex_ml/tracking/__init__.py b/src/codex_ml/tracking/__init__.py
new file mode 100644
index 0000000..b0ab2d1
--- /dev/null
+++ b/src/codex_ml/tracking/__init__.py
@@ -0,0 +1,15 @@
+"""
+Tracking guards public surface.
+"""
+from .guards import (
+    decide_mlflow_tracking_uri,
+    normalize_mlflow_uri,
+    DEFAULT_LOCAL_URI,
+    ALLOW_REMOTE_ENV,
+    TrackingDecision,
+)
+
+__all__ = [
+    "decide_mlflow_tracking_uri", "normalize_mlflow_uri", "DEFAULT_LOCAL_URI",
+    "ALLOW_REMOTE_ENV", "TrackingDecision",
+]
diff --git a/tests/cli/test_cli_manifest.py b/tests/cli/test_cli_manifest.py
new file mode 100644
index 0000000..4b3a6b1
--- /dev/null
+++ b/tests/cli/test_cli_manifest.py
@@ -0,0 +1,55 @@
+import json
+import sys
+import subprocess
+from pathlib import Path
+
+
+def _badge_present(text: str) -> bool:
+    return "<!-- BEGIN-CAPABILITY-HASH-BADGE -->" in text and "<!-- END-CAPABILITY-HASH-BADGE -->" in text
+
+
+def test_cli_manifest_hash_and_badge_update(tmp_path: Path):
+    # Create a tiny project layout
+    (tmp_path / "src" / "mod").mkdir(parents=True)
+    (tmp_path / "src" / "mod" / "__init__.py").write_text("x=1\n", encoding="utf-8")
+    (tmp_path / "docs").mkdir()
+    (tmp_path / "docs" / "readme.md").write_text("# Doc\n", encoding="utf-8")
+    (tmp_path / "README.md").write_text("# Project\n\n", encoding="utf-8")
+
+    # Run CLI
+    p = subprocess.run(
+        [
+            sys.executable, "-m", "codex_ml", "hash", "manifest",
+            "--root", str(tmp_path),
+            "--glob", "src/**/*.py",
+            "--glob", "docs/**/*.md",
+            "--write-readme",
+        ],
+        capture_output=True,
+        text=True,
+        check=True,
+    )
+    data = json.loads(p.stdout)
+    assert "aggregate_sha256" in data and len(data["aggregate_sha256"]) == 64
+    readme = (tmp_path / "README.md").read_text(encoding="utf-8")
+    assert _badge_present(readme)
+    assert data["aggregate_sha256"] in readme
diff --git a/tests/cli/test_cli_tracking.py b/tests/cli/test_cli_tracking.py
new file mode 100644
index 0000000..c4b7a2e
--- /dev/null
+++ b/tests/cli/test_cli_tracking.py
@@ -0,0 +1,43 @@
+import json
+import os
+import sys
+import subprocess
+from pathlib import Path
+
+
+def test_cli_tracking_decide_writes_summary_and_blocks_remote(tmp_path: Path):
+    env = os.environ.copy()
+    env.update({
+        "MLFLOW_OFFLINE": "1",
+        "WANDB_MODE": "offline",
+        "MLFLOW_TRACKING_URI": "http://mlflow.example:5000",
+    })
+    out = tmp_path / "summary.ndjson"
+    p = subprocess.run(
+        [
+            sys.executable, "-m", "codex_ml", "tracking", "decide",
+            "--summary", out.as_posix(),
+        ],
+        capture_output=True,
+        text=True,
+        env=env,
+        check=True,
+    )
+    # stdout is JSON
+    result = json.loads(p.stdout)
+    assert result["blocked"] is True
+    assert result["uri"].startswith("file://")
+    # summary file contains NDJSON line
+    lines = [l for l in out.read_text(encoding="utf-8").splitlines() if l.strip()]
+    assert len(lines) >= 1
+    j = json.loads(lines[-1])
+    assert j["decision"]["blocked"] is True
+    assert j["decision"]["uri"].startswith("file://")
diff --git a/CHANGELOG_CODEX.md b/CHANGELOG_CODEX.md
index 2a8b4d2..a5a3ff0 100644
--- a/CHANGELOG_CODEX.md
+++ b/CHANGELOG_CODEX.md
@@ -1,5 +1,26 @@
 # Codex Changelog
+## 2025-10-07 â CLI: hash/manifest + tracking/decide (+ __main__)
+
+### WHY
+- Close the loop on docs and shims by providing a minimal **CLI**:
+  - `hash manifest` computes a stable aggregate SHA-256 and can update the README badge.
+  - `tracking decide` evaluates MLflow/W&B env posture and emits a JSON decision, with optional NDJSON summary.
+- Add `__main__` to support `python -m codex_ml`.
+
+### Changes
+- `src/codex_ml/cli.py`: CLI implementation.
+- `src/codex_ml/__main__.py`: module entrypoint.
+- `src/codex_ml/tracking/__init__.py`: public surface for guards.
+- Tests:
+  - `tests/cli/test_cli_manifest.py`
+  - `tests/cli/test_cli_tracking.py`
+
+### Risk
+- Low. Purely additive; defaults are conservative and offline-first.
+
+### Rollback
+- Remove the new CLI modules and tests.
+
 ## 2025-10-07 â Detector aggregator + NDJSON summarizer
 ### WHY
 - Close the detector mapping by adding a simple **aggregate** that maps `unified-training` into the user-facing `training-engine` capability.
 - Provide the `tools/ndjson_summarize.py` referenced in docs to turn NDJSON logs into CSV locally.
```
````diff
diff --git a/tools/validate_fences.py b/tools/validate_fences.py
new file mode 100755
index 0000000..7d8c1a3
--- /dev/null
+++ b/tools/validate_fences.py
@@ -0,0 +1,196 @@
+#!/usr/bin/env python3
+from __future__ import annotations
+"""
+Fence validator for Markdown files.
+
+Checks (subset of our Fence Discipline):
+ - Fences must open with backticks (```) or tildes (~~~), length >= 3.
+ - A fence must close with the **same character** and length >= opener length.
+ - No mixing of fence characters within the same block.
+ - The info string (language tag) must NOT contain backticks.
+ - All fences must be balanced at EOF.
+
+Usage:
+  python tools/validate_fences.py --paths README.md docs
+"""
+import argparse
+import re
+import sys
+from pathlib import Path
+from typing import List, Tuple
+
+FENCE_RE = re.compile(r"^(?P<indent>[ \t]{0,3})(?P<char>`|~){3,}(?P<info>.*)$")
+
+
+class FenceError(Exception):
+    pass
+
+
+def _scan_file(path: Path) -> List[Tuple[int, str]]:
+    """
+    Returns list of (line_number, message) errors.
+    """
+    errors: List[Tuple[int, str]] = []
+    stack: List[Tuple[str, int]] = []  # (char, length)
+    try:
+        lines = path.read_text(encoding="utf-8", errors="strict").splitlines()
+    except Exception as e:
+        return [(0, f"cannot read file: {e}")]
+
+    for idx, raw in enumerate(lines, start=1):
+        m = FENCE_RE.match(raw)
+        if not m:
+            continue
+        fence = raw.strip()
+        # Count fence length and char
+        ch = m.group("char")
+        # Find consecutive sequence length of ch
+        n = 0
+        for c in raw.lstrip():
+            if c == ch:
+                n += 1
+            else:
+                break
+        if n < 3:
+            errors.append((idx, "fence length must be >= 3"))
+            continue
+        info = m.group("info").strip()
+        # Open vs close: if stack empty, treat as open. If not empty, this could be close for same char.
+        if not stack:
+            # OPEN
+            if "`" in info:
+                errors.append((idx, "info string must not contain backticks"))
+            stack.append((ch, n))
+        else:
+            top_ch, top_n = stack[-1]
+            if ch != top_ch:
+                # If we see a different fence char while a block is open, it's an error (mixed fences).
+                errors.append((idx, f"mixed fence characters inside block (opened with '{top_ch}', found '{ch}')"))
+                continue
+            # If info is empty (or arbitrary), treat as CLOSE. Require length >= opener.
+            if n < top_n:
+                errors.append((idx, f"closing fence shorter than opener (close={n}, open={top_n})"))
+                continue
+            # CLOSE
+            stack.pop()
+    if stack:
+        # Unbalanced at EOF
+        ch, n = stack[-1]
+        errors.append((len(lines), f"unclosed fence (opened with '{ch* n}')"))
+    return errors
+
+
+def _iter_markdown_paths(paths: List[str]) -> List[Path]:
+    out: List[Path] = []
+    seen = set()
+    for p in paths:
+        pth = Path(p)
+        if pth.is_file():
+            if pth.suffix.lower() == ".md" and pth.resolve() not in seen:
+                seen.add(pth.resolve())
+                out.append(pth)
+        elif pth.is_dir():
+            for sub in pth.rglob("*.md"):
+                if sub.is_file() and sub.resolve() not in seen:
+                    seen.add(sub.resolve())
+                    out.append(sub)
+    return sorted(out)
+
+
+def main(argv=None) -> int:
+    ap = argparse.ArgumentParser(description="Validate Markdown code fences.")
+    ap.add_argument("--paths", nargs="+", default=["README.md", "docs"], help="Files or directories to scan")
+    args = ap.parse_args(argv)
+    md_files = _iter_markdown_paths(args.paths)
+    any_errors = False
+    for f in md_files:
+        errs = _scan_file(f)
+        if errs:
+            any_errors = True
+            for ln, msg in errs:
+                sys.stderr.write(f"{f}:{ln}: {msg}\n")
+    return 1 if any_errors else 0
+
+
+if __name__ == "__main__":
+    raise SystemExit(main())
diff --git a/tests/tools/test_validate_fences.py b/tests/tools/test_validate_fences.py
new file mode 100644
index 0000000..d9e9d79
--- /dev/null
+++ b/tests/tools/test_validate_fences.py
@@ -0,0 +1,64 @@
+import subprocess
+import sys
+from pathlib import Path
+
+from tools.validate_fences import main as validate_main
+
+
+def test_validate_good_and_bad_files(tmp_path: Path):
+    good = tmp_path / "good.md"
+    bad = tmp_path / "bad.md"
+    good.write_text(
+        "# Good\n\n"
+        "```python\n"
+        "print('ok')\n"
+        "```\n\n"
+        "~~~~\n"
+        "tilde block\n"
+        "~~~~\n",
+        encoding="utf-8",
+    )
+    bad.write_text(
+        "# Bad\n\n"
+        "```py`thon\n"  # info string includes backticks -> invalid
+        "x=1\n"
+        "```\n\n"
+        "```js\n"
+        "let x = 1;\n"
+        "``\n",  # short close fence
+        encoding="utf-8",
+    )
+    assert validate_main(["--paths", good.as_posix()]) == 0
+    assert validate_main(["--paths", bad.as_posix()]) == 1
+
+
+def test_cli_invocation(tmp_path: Path):
+    f = tmp_path / "doc.md"
+    f.write_text("```bash\ntrue\n```\n", encoding="utf-8")
+    p = subprocess.run(
+        [sys.executable, "tools/validate_fences.py", "--paths", f.as_posix()],
+        capture_output=True,
+        text=True,
+        check=False,
+    )
+    assert p.returncode == 0
diff --git a/.pre-commit-config.yaml b/.pre-commit-config.yaml
index 2ec8c6d..b3e0aef 100644
--- a/.pre-commit-config.yaml
+++ b/.pre-commit-config.yaml
@@ -33,3 +33,14 @@ repos:
         language: system
         entry: python tools/scan_secrets.py --fail-on-find
         pass_filenames: false
         stages: [commit]
+  - repo: local
+    hooks:
+      - id: codex-validate-fences
+        name: codex (validate fences)
+        language: system
+        entry: python tools/validate_fences.py --paths README.md docs
+        pass_filenames: false
+        stages: [commit]
diff --git a/tools/bench_unified_training.py b/tools/bench_unified_training.py
new file mode 100755
index 0000000..b56b6f7
--- /dev/null
+++ b/tools/bench_unified_training.py
@@ -0,0 +1,82 @@
+#!/usr/bin/env python3
+from __future__ import annotations
+"""
+Micro-benchmark: unified training throughput (epochs/sec).
+(Local/offline; runs a tiny model for a few epochs.)
+"""
+import time
+
+try:
+    import torch
+    from torch import nn
+except Exception as e:
+    raise SystemExit("PyTorch is required for this benchmark") from e
+
+from codex_ml.training.unified_training import UnifiedTrainingConfig, run_unified_training
+
+
+def _loader(n=512, batch=32, device="cpu"):
+    x = torch.linspace(-1, 1, steps=n, device=device).reshape(-1, 1)
+    y = 2.0 * x + 0.1
+    ds = torch.utils.data.TensorDataset(x, y)
+    return torch.utils.data.DataLoader(ds, batch_size=batch, shuffle=False)
+
+
+def bench(epochs: int = 3, batch: int = 32, hidden: int = 32, device: str | None = None) -> None:
+    device = device or ("cuda" if torch.cuda.is_available() else "cpu")
+    model = nn.Sequential(nn.Linear(1, hidden), nn.ReLU(), nn.Linear(hidden, 1)).to(device)
+    opt = torch.optim.SGD(model.parameters(), lr=1e-2)
+    loss = nn.MSELoss()
+    loader = _loader(batch=batch, device=device)
+    cfg = UnifiedTrainingConfig(epochs=epochs, lr=1e-2, deterministic=True, device=device)
+
+    t0 = time.perf_counter()
+    run_unified_training(cfg, model=model, optimizer=opt, loss_fn=loss, train_loader=loader)
+    t1 = time.perf_counter()
+    total = t1 - t0
+    rate = epochs / total if total > 0 else 0.0
+    print(f"device={device} epochs={epochs} batch={batch} hidden={hidden} time={total:.3f}s => {rate:.2f} epochs/sec")
+
+
+if __name__ == "__main__":
+    bench()
diff --git a/docs/architecture.md b/docs/architecture.md
new file mode 100644
index 0000000..b3f3b63
--- /dev/null
+++ b/docs/architecture.md
@@ -0,0 +1,79 @@
+# Architecture (Overview)
+
+The project favors **small, composable modules** with minimal, typed public surfaces.
+
+```mermaid
+flowchart LR
+    subgraph codex_ml
+      training[[training.unified_training]]
+      checkpointing[[checkpointing.checkpoint_core]]
+      tracking[[tracking.guards]]
+      data[[data.utils]]
+      token[[tokenization.api]]
+      safety[[safety.filters]]
+      metrics[[metrics.text]]
+      logging[[logging.structured]]
+      observ[[observability.metrics]]
+      detectors[[detectors.*]]
+      cli[[cli]]
+    end
+
+    training --> checkpointing
+    training --> logging
+    training --> metrics
+    training --> data
+
+    observ --> logging
+    cli --> tracking
+    cli --> data
+
+    detectors --> training
+```
+
+Notes:
+- **Unified training** is the faÃ§ade; legacy wrappers emit `DeprecationWarning`.
+- **Checkpoint core** owns schema and digest; legacy compat delegates.
+- **Tracking guards** enforce offline-first behavior.
+- **Tokenization API** consolidates adapters under a single namespace.
+- **Safety** and **metrics** provide trivial helpers with deprecation shims.
+- **Detectors** infer capability presence; the aggregator maps into user-facing scores.
diff --git a/docs/performance.md b/docs/performance.md
index 6b1def0..1f2a5b9 100644
--- a/docs/performance.md
+++ b/docs/performance.md
@@ -1,6 +1,6 @@
 # Performance Notes
-We include small, **local** micro-benchmarks to spot obvious regressions without enabling any CI or remote services.
+We include small, **local** micro-benchmarks to spot obvious regressions without enabling any CI or remote services.
 ## Tokenizer
 ```bash
@@ -12,3 +12,14 @@ This prints a simple `docs/sec` metric for the fallback whitespace tokenizer. Treat it as a coarse, reproducible signal.
 ## Tips
 - Avoid unnecessary allocations in hot loops; reuse objects where possible.
 - Prefer list comprehensions and local variables in Python for tight loops.
+
+## Unified Training
+
+```bash
+python tools/bench_unified_training.py
+```
+
+This prints **epochs/sec** for a tiny linear regression task using the unified training faÃ§ade. Use it to sanity-check
+major changes to the training loop or device configuration. Results are device-dependent and coarse but repeatable with
+`deterministic=True`.
diff --git a/README.md b/README.md
index f0a6b7d..6827d82 100644
--- a/README.md
+++ b/README.md
@@ -14,6 +14,7 @@ Local, offline-first utilities and ML training components.
 **Docs index:** see `docs/index.md` for a map of features and guides.
 Operational notes: `docs/ops.md`.
+Architecture overview: `docs/architecture.md`.
 ## Quickstart
@@ -58,6 +59,13 @@ See `docs/cli.md` for more.
 ## Data Determinism & Metrics
 - See `docs/data_determinism.md` for stable shuffling, sharding, and manifest checksums.
 - See `docs/observability_metrics.md` for emitting lightweight metrics as NDJSON.
 ## Safety & Metrics
 - **Safety API**: `codex_ml.safety.sanitize_text` removes control/format characters after normalization. See `docs/safety_api.md`.
 - **Metrics**:
   - Canonical: `codex_ml.metrics.perplexity_from_losses`.
   - Deprecated shim: `tools.apply_ml_metrics.perplexity` (emits `DeprecationWarning`).
+
+## Performance
+- Tokenizer throughput: `python tools/bench_tokenizer.py`
+- Unified training throughput: `python tools/bench_unified_training.py`
+- Markdown fence validator (pre-commit hook): `python tools/validate_fences.py --paths README.md docs`
diff --git a/CHANGELOG_CODEX.md b/CHANGELOG_CODEX.md
index a5a3ff0..6a4e5c7 100644
--- a/CHANGELOG_CODEX.md
+++ b/CHANGELOG_CODEX.md
@@ -1,5 +1,30 @@
 # Codex Changelog
+## 2025-10-07 â Fence validator, architecture diagram, training bench
+
+### WHY
+- Enforce **fence discipline** on Markdown to keep diffs and docs render-safe.
+- Provide a **Mermaid architecture** view to orient contributors.
+- Add a tiny **unified training micro-benchmark** to spot regressions.
+
+### Changes
+- Tools:
+  - `tools/validate_fences.py` with tests; added pre-commit hook.
+  - `tools/bench_unified_training.py`.
+- Docs:
+  - `docs/architecture.md` (Mermaid diagram).
+  - `docs/performance.md` updated with training bench.
+- README:
+  - Links to architecture and performance tools.
+
+### Risk
+- Low. Validator only affects docs; bench is opt-in and local.
+
+### Rollback
+- Remove the new tool/docs and pre-commit hook; revert README links.
+
 ## 2025-10-07 â CLI: hash/manifest + tracking/decide (+ __main__)
 ### WHY
 - Close the loop on docs and shims by providing a minimal **CLI**:
   - `hash manifest` computes a stable aggregate SHA-256 and can update the README badge.
````
````diff
diff --git a/tests/checkpointing/test_resume_optimizer_rng_equivalence.py b/tests/checkpointing/test_resume_optimizer_rng_equivalence.py
new file mode 100644
index 0000000..e8b2a91
--- /dev/null
+++ b/tests/checkpointing/test_resume_optimizer_rng_equivalence.py
@@ -0,0 +1,98 @@
+import json
+import pytest
+
+torch = pytest.importorskip("torch")
+nn = torch.nn
+optim = torch.optim
+
+from codex_ml.training.unified_training import UnifiedTrainingConfig, run_unified_training
+from codex_ml.checkpointing import load_checkpoint, apply_rng_state
+
+
+def _toy_loader(device="cpu"):
+    x = torch.linspace(-1, 1, steps=64, device=device).reshape(-1, 1)
+    y = 1.5 * x - 0.2
+    ds = torch.utils.data.TensorDataset(x, y)
+    return torch.utils.data.DataLoader(ds, batch_size=16, shuffle=False)
+
+
+def _model():
+    return nn.Sequential(nn.Linear(1, 8), nn.ReLU(), nn.Dropout(p=0.1), nn.Linear(8, 1))
+
+
+def test_resume_optimizer_state_and_rng_equivalence(tmp_path):
+    """
+    Validate that:
+      - Optimizer state is faithfully restored on resume.
+      - RNG state applied on resume matches the saved RNG state (no training performed post-resume).
+    Strategy:
+      - Train for one epoch and save epoch-0000.
+      - Load raw checkpoint for the optimizer+RNG snapshot.
+      - Resume with cfg.epochs=1 (start_epoch==1 => no further steps) so states remain at checkpoint.
+      - Compare optimizer.state_dict() to saved one and check RNG equality via a random draw.
+    """
+    device = "cuda" if torch.cuda.is_available() else "cpu"
+    # Phase 1: run 1 epoch and checkpoint
+    m1 = _model().to(device)
+    opt1 = optim.SGD(m1.parameters(), lr=5e-3)
+    loss = nn.MSELoss()
+    out = tmp_path / "ck"
+    cfg1 = UnifiedTrainingConfig(epochs=1, lr=5e-3, deterministic=True, device=device, checkpoint_dir=out.as_posix())
+    run_unified_training(cfg1, model=m1, optimizer=opt1, loss_fn=loss, train_loader=_toy_loader(device))
+    ep0 = out / "epoch-0000"
+    assert ep0.exists()
+    ck_state, ck_meta = load_checkpoint(ep0.as_posix())
+    assert "optimizer" in ck_state and "rng" in ck_state
+    saved_opt = ck_state["optimizer"]
+    saved_rng = ck_state["rng"]
+    # Phase 2: resume with epochs=1 (no-op loop) so states should equal checkpoint snapshot
+    m2 = _model().to(device)
+    m2.load_state_dict(ck_state["model"])
+    opt2 = optim.SGD(m2.parameters(), lr=5e-3)
+    cfg2 = UnifiedTrainingConfig(
+        epochs=1, lr=5e-3, deterministic=True, device=device, checkpoint_dir=out.as_posix(), resume_from=ep0.as_posix()
+    )
+    run_unified_training(cfg2, model=m2, optimizer=opt2, loss_fn=loss, train_loader=_toy_loader(device))
+    # Optimizer equality (keys and tensor values)
+    def _equal_opt(a, b):
+        if a.keys() != b.keys():
+            return False
+        for k in a:
+            av, bv = a[k], b[k]
+            if isinstance(av, dict) and isinstance(bv, dict):
+                if not _equal_opt(av, bv):
+                    return False
+            elif torch.is_tensor(av) and torch.is_tensor(bv):
+                if not torch.equal(av, bv):
+                    return False
+            else:
+                if av != bv:
+                    return False
+        return True
+    assert _equal_opt(saved_opt, opt2.state_dict())
+    # RNG equality: a random draw now should match a draw after explicitly applying the saved RNG
+    a = torch.rand(4, device=device)
+    apply_rng_state(saved_rng)
+    b = torch.rand(4, device=device)
+    assert torch.allclose(a, b)
diff --git a/docs/incident_runbook.md b/docs/incident_runbook.md
new file mode 100644
index 0000000..5b6d0f2
--- /dev/null
+++ b/docs/incident_runbook.md
@@ -0,0 +1,86 @@
+# Incident Runbook (Local / Offline)
+
+Short, checklist-style steps to triage and resolve issues without external services.
+
+## 0) Safety first
+- Pause any long-running training loops if metrics show divergence.
+- Snapshot working directories (`tar` or copy) before modifying artifacts.
+
+## 1) Capture context
+- Export recent logs (structured NDJSON if available).
+- Summarize tracking posture:
+  ```bash
+  python -m codex_ml tracking decide --summary incident_tracking.ndjson --print
+  ```
+- Record environment (Python, CUDA, PyTorch):
+  ```bash
+  python -c "import sys, torch; print(sys.version); print(torch.__version__)"
+  ```
+
+## 2) Quick checks
+- **Secrets**: ensure no accidental tokens in configs/logs.
+  ```bash
+  python tools/scan_secrets.py --fail-on-find
+  ```
+- **Determinism**: re-run the smallest failing test with `-vv` and `deterministic=True`.
+- **Data integrity**: recompute manifest checksums for the affected inputs.
+  ```bash
+  python -m codex_ml hash manifest --root . --glob 'data/**/*' > manifest.json
+  ```
+
+## 3) Reproduce locally
+- Freeze seeds and device:
+  ```bash
+  PYTHONHASHSEED=0 CUDA_VISIBLE_DEVICES='' pytest -k failing_test -vv
+  ```
+- Use synthetic, tiny datasets for faster iterate-debug cycles.
+
+## 4) Checkpoint sanity
+- List latest checkpoints and validate schema/digests.
+- Verify resume continuity with a one-epoch resume smoke test.
+
+## 5) Rollback path
+- If a recent change caused the regression, revert that commit locally and confirm tests are green.
+- Keep a short note of **WHY / Risk / Rollback** for future PR description.
+
+## 6) Aftercare
+- Add or expand property-style tests to close the identified gap.
+- Update `CHANGELOG_CODEX.md` with a concise summary.
+
+> Tip: Convert NDJSON logs to CSV for quick spreadsheet inspection:
+> ```bash
+> python tools/ndjson_summarize.py incident_tracking.ndjson --output incident_tracking.csv
+> ```
diff --git a/docs/quality_gates.md b/docs/quality_gates.md
new file mode 100644
index 0000000..b2d6d0a
--- /dev/null
+++ b/docs/quality_gates.md
@@ -0,0 +1,64 @@
+# Quality Gates (Local)
+
+Fast, offline checks to keep changes small, safe, and reviewable.
+
+## Pre-commit
+
+Install once:
+```bash
+pre-commit install
+```
+
+Run on all files:
+```bash
+pre-commit run --all-files
+```
+
+### Hooks included
+- **Secret scan**: `python tools/scan_secrets.py --fail-on-find`
+- **Fence validator**: `python tools/validate_fences.py --paths README.md docs`
+
+> All hooks are local and do not require network access.
+
+## Tests
+
+Run fast unit tests:
+```bash
+pytest -q
+```
+
+Run with visible warnings (to catch deprecations):
+```bash
+PYTHONWARNINGS=default pytest -q
+```
+
+## Spot checks
+
+- **Manifest badge** (deterministic hash):
+  ```bash
+  python -m codex_ml hash manifest --write-readme
+  ```
+- **Training parity** (deterministic smoke):
+  ```bash
+  pytest tests/training/test_unified_training_parity.py -q
+  ```
+- **Tracking guards** (offline posture):
+  ```bash
+  pytest tests/tracking/test_tracking_guard_matrix.py -q
+  ```
diff --git a/docs/index.md b/docs/index.md
index 3a8f0e4..e5a9a4a 100644
--- a/docs/index.md
+++ b/docs/index.md
@@ -4,11 +4,12 @@
 - **Unified Training**: `docs/unified_training.md`
 - **Checkpoint Core**: `docs/checkpointing_core.md`
 - **Tracking & Observability**: `docs/observability.md`, `docs/observability_metrics.md`
 - **Tokenization API**: `docs/tokenization_api.md`
 - **Data Determinism**: `docs/data_determinism.md`
+- **Quality Gates**: `docs/quality_gates.md`
 - **CLI**: `docs/cli.md`
-- **Quality Gates**: `docs/quality_gates.md`
 - **Releasing**: `docs/releasing.md`
 - **Detectors**: `docs/detectors.md`
 - **Security / Secrets**: `docs/secrets.md`
 - **Incident Runbook**: `docs/incident_runbook.md`
 - **Safety API**: `docs/safety_api.md`
 - **Tests Overview**: `docs/tests_overview.md`
diff --git a/CHANGELOG_CODEX.md b/CHANGELOG_CODEX.md
index 6a4e5c7..c8f16d1 100644
--- a/CHANGELOG_CODEX.md
+++ b/CHANGELOG_CODEX.md
@@ -1,5 +1,29 @@
 # Codex Changelog
+## 2025-10-07 â Resume: optimizer & RNG equivalence test + gates & runbook docs
+
+### WHY
+- Strengthen **resume semantics** by asserting optimizer state identity and RNG continuity when resuming without stepping.
+- Document **quality gates** and an **incident runbook** to keep operations local and predictable.
+
+### Changes
+- Tests:
+  - `tests/checkpointing/test_resume_optimizer_rng_equivalence.py` compares optimizer state and RNG draws pre/post resume.
+- Docs:
+  - `docs/quality_gates.md` (pre-commit hooks, fast tests, spot checks).
+  - `docs/incident_runbook.md` (offline incident checklist).
+  - `docs/index.md` updated.
+
+### Risk
+- Low. New tests are deterministic; docs are additive.
+
+### Rollback
+- Remove the test file and the two docs; revert docs index link.
+
 ## 2025-10-07 â Fence validator, architecture diagram, training bench
 ### WHY
 - Enforce **fence discipline** on Markdown to keep diffs and docs render-safe.
 - Provide a **Mermaid architecture** view to orient contributors.
````
````diff
diff --git a/src/codex_ml/checkpointing/__init__.py b/src/codex_ml/checkpointing/__init__.py
new file mode 100644
index 0000000..5b4b0d9
--- /dev/null
+++ b/src/codex_ml/checkpointing/__init__.py
@@ -0,0 +1,18 @@
+"""
+Checkpointing public surface.
+"""
+from .checkpoint_core import (
+    save_checkpoint,
+    load_checkpoint,
+    calculate_digest,
+    apply_rng_state,
+)
+
+__all__ = [
+    "save_checkpoint",
+    "load_checkpoint",
+    "calculate_digest",
+    "apply_rng_state",
+]
+
diff --git a/src/codex_ml/checkpointing/checkpoint_core.py b/src/codex_ml/checkpointing/checkpoint_core.py
new file mode 100644
index 0000000..2a0b3a7
--- /dev/null
+++ b/src/codex_ml/checkpointing/checkpoint_core.py
@@ -0,0 +1,261 @@
+from __future__ import annotations
+"""
+Canonical checkpoint save/load with integrity metadata and retention.
+
+Layout (directory):
+  model.pth       - torch state_dict (model)
+  optimizer.pth   - torch optimizer.state_dict()
+  rng.pth         - Python/NumPy/Torch RNG snapshots
+  metadata.json   - JSON with schema_version, digest, and user meta
+"""
+import hashlib
+import json
+import os
+import random
+from pathlib import Path
+from typing import Dict, Any, Tuple, Iterable
+
+try:  # optional numpy
+    import numpy as _np  # type: ignore
+except Exception:  # pragma: no cover
+    _np = None  # type: ignore
+
+try:
+    import torch
+except Exception:  # pragma: no cover
+    torch = None  # type: ignore
+
+SCHEMA_VERSION = 1
+
+
+def _sha256_file(path: Path) -> str:
+    h = hashlib.sha256()
+    with path.open("rb") as f:
+        for chunk in iter(lambda: f.read(8192), b""):
+            h.update(chunk)
+    return h.hexdigest()
+
+
+def calculate_digest(directory: str | Path) -> str:
+    """
+    Calculate an aggregate digest over the known files in a checkpoint directory.
+    """
+    d = Path(directory)
+    parts: list[str] = []
+    for name in ("model.pth", "optimizer.pth", "rng.pth", "metadata.json"):
+        p = d / name
+        if p.exists():
+            parts.append(_sha256_file(p))
+    h = hashlib.sha256()
+    h.update(":".join(parts).encode("utf-8"))
+    return h.hexdigest()
+
+
+def _capture_rng_state() -> Dict[str, Any]:
+    state: Dict[str, Any] = {"python": random.getstate()}
+    if _np is not None:
+        try:
+            state["numpy"] = _np.random.get_state()
+        except Exception:  # pragma: no cover
+            pass
+    if torch is not None:
+        try:
+            state["torch_cpu"] = torch.get_rng_state()
+        except Exception:  # pragma: no cover
+            pass
+        try:
+            if hasattr(torch, "cuda") and torch.cuda.is_available():
+                state["torch_cuda_all"] = torch.cuda.get_rng_state_all()  # type: ignore[attr-defined]
+        except Exception:  # pragma: no cover
+            pass
+    return state
+
+
+def apply_rng_state(state: Dict[str, Any]) -> None:
+    """
+    Apply a previously captured RNG snapshot.
+    """
+    try:
+        if "python" in state:
+            random.setstate(state["python"])
+    except Exception:  # pragma: no cover
+        pass
+    if _np is not None:
+        try:
+            if "numpy" in state:
+                _np.random.set_state(state["numpy"])
+        except Exception:  # pragma: no cover
+            pass
+    if torch is not None:
+        try:
+            if "torch_cpu" in state:
+                torch.set_rng_state(state["torch_cpu"])
+        except Exception:  # pragma: no cover
+            pass
+        try:
+            if "torch_cuda_all" in state and hasattr(torch, "cuda") and torch.cuda.is_available():
+                torch.cuda.set_rng_state_all(state["torch_cuda_all"])  # type: ignore[attr-defined]
+        except Exception:  # pragma: no cover
+            pass
+
+
+def _save_json(path: Path, obj: Dict[str, Any]) -> None:
+    tmp = path.with_suffix(".json.tmp")
+    tmp.write_text(json.dumps(obj, ensure_ascii=False, indent=2), encoding="utf-8")
+    tmp.replace(path)
+
+
+def _ensure_dir(p: Path) -> None:
+    p.mkdir(parents=True, exist_ok=True)
+
+
+def _retention_prune(parent: Path, keep_last_k: int) -> None:
+    """
+    Keep only the last K epoch-* directories by lexical order (epoch-0000, epoch-0001, ...).
+    """
+    if keep_last_k is None or keep_last_k <= 0:
+        return
+    subs = sorted([p for p in parent.glob("epoch-*") if p.is_dir()])
+    to_delete = subs[:-keep_last_k]
+    for d in to_delete:
+        try:
+            for sub in d.glob("**/*"):
+                if sub.is_file():
+                    sub.unlink(missing_ok=True)  # type: ignore[arg-type]
+            # remove empty dirs bottom-up
+            for subdir in sorted([p for p in d.glob("**") if p.is_dir()], reverse=True):
+                try:
+                    subdir.rmdir()
+                except OSError:
+                    pass
+            d.rmdir()
+        except Exception:  # pragma: no cover
+            # best-effort pruning
+            pass
+
+
+def save_checkpoint(
+    out_dir: str | Path,
+    *,
+    state: Dict[str, Any],
+    meta: Dict[str, Any] | None = None,
+    keep_last_k: int = 5,
+) -> Dict[str, Any]:
+    """
+    Save a checkpoint directory with model/optimizer/rng and metadata.
+    Returns the metadata written.
+    """
+    out = Path(out_dir)
+    _ensure_dir(out)
+
+    # Save model and optimizer (if present)
+    if torch is None:
+        raise RuntimeError("PyTorch is required to save checkpoints")
+    if "model" in state:
+        torch.save(state["model"], out / "model.pth")
+    if "optimizer" in state:
+        torch.save(state["optimizer"], out / "optimizer.pth")
+
+    # RNG snapshot
+    rng_state = state.get("rng") or _capture_rng_state()
+    torch.save(rng_state, out / "rng.pth")
+
+    # Metadata
+    meta = dict(meta or {})
+    meta["schema_version"] = SCHEMA_VERSION
+    meta["digest_sha256"] = calculate_digest(out)
+    _save_json(out / "metadata.json", meta)
+
+    # Retention on parent directory (keep last K epoch-* directories)
+    parent = out.parent
+    if out.name.startswith("epoch-"):
+        _retention_prune(parent, int(keep_last_k))
+    return meta
+
+
+def load_checkpoint(in_dir: str | Path, *, map_location: str | None = None) -> Tuple[Dict[str, Any], Dict[str, Any]]:
+    """
+    Load a checkpoint directory and return (state, metadata).
+    """
+    d = Path(in_dir)
+    if torch is None:
+        raise RuntimeError("PyTorch is required to load checkpoints")
+    state: Dict[str, Any] = {}
+    meta: Dict[str, Any] = {}
+    # Metadata first
+    mpath = d / "metadata.json"
+    if mpath.exists():
+        try:
+            meta = json.loads(mpath.read_text(encoding="utf-8"))
+        except Exception:  # pragma: no cover
+            meta = {}
+    # Model / optimizer
+    model_p = d / "model.pth"
+    opt_p = d / "optimizer.pth"
+    rng_p = d / "rng.pth"
+    if model_p.exists():
+        state["model"] = torch.load(model_p, map_location=map_location or "cpu")
+    if opt_p.exists():
+        state["optimizer"] = torch.load(opt_p, map_location=map_location or "cpu")
+    if rng_p.exists():
+        state["rng"] = torch.load(rng_p, map_location="cpu")
+    # Validate digest if present
+    try:
+        digest = calculate_digest(d)
+        if meta.get("digest_sha256") and meta["digest_sha256"] != digest:
+            meta["digest_mismatch"] = True
+            meta["calculated_digest_sha256"] = digest
+    except Exception:  # pragma: no cover
+        pass
+    return state, meta
+
diff --git a/tests/checkpointing/test_checkpoint_core.py b/tests/checkpointing/test_checkpoint_core.py
new file mode 100644
index 0000000..b6a1e65
--- /dev/null
+++ b/tests/checkpointing/test_checkpoint_core.py
@@ -0,0 +1,123 @@
+import json
+import os
+import pytest
+
+torch = pytest.importorskip("torch")
+nn = torch.nn
+optim = torch.optim
+
+from codex_ml.checkpointing import save_checkpoint, load_checkpoint, calculate_digest, apply_rng_state
+
+
+def _toy(out_dir, epoch=0):
+    model = nn.Sequential(nn.Linear(1, 4), nn.ReLU(), nn.Linear(4, 1))
+    opt = optim.SGD(model.parameters(), lr=1e-2)
+    state = {"model": model.state_dict(), "optimizer": opt.state_dict()}
+    meta = {"epoch": epoch, "loss": 1.23}
+    return state, meta
+
+
+def test_save_and_load_roundtrip(tmp_path):
+    out = tmp_path / "epoch-0000"
+    state, meta = _toy(out)
+    written_meta = save_checkpoint(out, state=state, meta=meta, keep_last_k=2)
+    assert written_meta["schema_version"] == 1
+    assert "digest_sha256" in written_meta
+    assert (out / "model.pth").exists()
+    assert (out / "optimizer.pth").exists()
+    assert (out / "rng.pth").exists()
+    assert (out / "metadata.json").exists()
+
+    st2, m2 = load_checkpoint(out)
+    assert "model" in st2 and "optimizer" in st2 and "rng" in st2
+    assert m2.get("epoch") == 0 and m2.get("loss") == 1.23
+    # Digest matches
+    assert m2["digest_sha256"] == calculate_digest(out)
+
+
+def test_digest_mismatch_flag(tmp_path):
+    out = tmp_path / "epoch-0001"
+    state, meta = _toy(out, epoch=1)
+    save_checkpoint(out, state=state, meta=meta, keep_last_k=2)
+    # Corrupt a file
+    (out / "model.pth").write_bytes(b"corrupted")
+    st, m = load_checkpoint(out)
+    assert m.get("digest_mismatch", False) is True
+    assert "calculated_digest_sha256" in m
+
+
+def test_retention_keeps_last_k(tmp_path):
+    parent = tmp_path
+    # Create 5 epochs; keep_last_k=3 should leave epoch-0002, 0003, 0004
+    for i in range(5):
+        out = parent / f"epoch-{i:04d}"
+        state, meta = _toy(out, epoch=i)
+        save_checkpoint(out, state=state, meta=meta, keep_last_k=3)
+    remain = sorted([p.name for p in parent.glob("epoch-*") if p.is_dir()])
+    assert remain == ["epoch-0002", "epoch-0003", "epoch-0004"]
+
+
+def test_apply_rng_state_roundtrip(tmp_path):
+    out = tmp_path / "epoch-0000"
+    state, meta = _toy(out)
+    written_meta = save_checkpoint(out, state=state, meta=meta)
+    st2, _ = load_checkpoint(out)
+    # Draw a random number, apply RNG, draw again; must match after apply
+    a = torch.rand(3)
+    apply_rng_state(st2["rng"])
+    b = torch.rand(3)
+    assert torch.allclose(a, b)
diff --git a/docs/checkpointing_core.md b/docs/checkpointing_core.md
new file mode 100644
index 0000000..3a6b1e2
--- /dev/null
+++ b/docs/checkpointing_core.md
@@ -0,0 +1,82 @@
+# Checkpoint Core
+
+Single, canonical save/load with integrity metadata and retention.
+
+## API
+
+```python
+from codex_ml.checkpointing import save_checkpoint, load_checkpoint, calculate_digest, apply_rng_state
+```
+
+### save_checkpoint(out_dir, *, state, meta, keep_last_k=5) -> dict
+Writes:
+- `model.pth`: model `state_dict()`
+- `optimizer.pth`: optimizer `state_dict()`
+- `rng.pth`: RNG snapshot (Python / NumPy / Torch / Torch CUDA)
+- `metadata.json`: includes `schema_version` and `digest_sha256` plus user `meta`.
+
+When `out_dir` is named like `epoch-XXXX`, the parent directory is pruned to keep only the latest **K** epoch folders.
+
+### load_checkpoint(in_dir, *, map_location="cpu") -> (state, meta)
+Reads files above and recomputes a digest; if it doesnât match `metadata.json`, a `digest_mismatch` flag is set in `meta`.
+
+### calculate_digest(dir) -> str
+Aggregate SHA-256 over known files in the folder.
+
+### apply_rng_state(state) -> None
+Applies RNG snapshot back to Python/NumPy/Torch (and Torch CUDA if available).
+
+## Notes
+- `schema_version = 1` for initial release.
+- All operations are **local/offline**; no network usage.
+- Retention is best-effort and only affects `epoch-*` siblings in the parent directory.
diff --git a/docs/index.md b/docs/index.md
index e5a9a4a..a1d2a7a 100644
--- a/docs/index.md
+++ b/docs/index.md
@@ -2,7 +2,7 @@
 - **Unified Training**: `docs/unified_training.md`
-- **Checkpoint Core**: `docs/checkpointing_core.md`
+- **Checkpoint Core**: `docs/checkpointing_core.md`
 - **Tracking & Observability**: `docs/observability.md`, `docs/observability_metrics.md`
 - **Tokenization API**: `docs/tokenization_api.md`
 - **Data Determinism**: `docs/data_determinism.md`
 - **Quality Gates**: `docs/quality_gates.md`
diff --git a/CHANGELOG_CODEX.md b/CHANGELOG_CODEX.md
index c8f16d1..b8a6b3a 100644
--- a/CHANGELOG_CODEX.md
+++ b/CHANGELOG_CODEX.md
@@ -1,5 +1,28 @@
 # Codex Changelog
+## 2025-10-07 â Checkpoint core: load/apply RNG + retention + tests/docs
+
+### WHY
+- Close P2 acceptance criteria by consolidating checkpoint save/load under a single module with digest metadata, RNG snapshots,
+  and retention policy. Provide programmatic `apply_rng_state` for resume parity.
+
+### Changes
+- `codex_ml.checkpointing.checkpoint_core`: `save_checkpoint`, `load_checkpoint`, `calculate_digest`, `apply_rng_state`.
+- Public surface: `src/codex_ml/checkpointing/__init__.py`.
+- Tests:
+  - `tests/checkpointing/test_checkpoint_core.py` (schema, digest, retention, RNG).
+- Docs:
+  - `docs/checkpointing_core.md` and index link.
+
+### Risk
+- Low. New module is additive; paths and filenames are stable and local.
+
+### Rollback
+- Remove the checkpointing package files, associated tests, and docs.
+
 ## 2025-10-07 â Resume: optimizer & RNG equivalence test + gates & runbook docs
 ### WHY
 - Strengthen **resume semantics** by asserting optimizer state identity and RNG continuity when resuming without stepping.
 - Document **quality gates** and an **incident runbook** to keep operations local and predictable.
````
```diff
diff --git a/src/codex_ml/training/unified_training.py b/src/codex_ml/training/unified_training.py
index 2a9e2ed..6a1f4a9 100644
--- a/src/codex_ml/training/unified_training.py
+++ b/src/codex_ml/training/unified_training.py
@@ -225,6 +225,22 @@ def train_loop(cfg: UnifiedTrainingConfig, *args, **kwargs):
     )
     return run_unified_training(cfg, *args, **kwargs)
+
+def functional_training(cfg: UnifiedTrainingConfig, *args, **kwargs):
+    """
+    Deprecated functional entrypoint for training.
+    Mirrors `train_loop` and forwards to `run_unified_training`.
+    """
+    import warnings
+    warnings.warn(
+        "functional_training is deprecated; use run_unified_training(cfg, ...)",
+        category=DeprecationWarning,
+        stacklevel=2,
+    )
+    return run_unified_training(cfg, *args, **kwargs)
+
+
 # End of module
diff --git a/src/codex_ml/detectors/unified_training.py b/src/codex_ml/detectors/unified_training.py
new file mode 100644
index 0000000..f3c9b1a
--- /dev/null
+++ b/src/codex_ml/detectors/unified_training.py
@@ -0,0 +1,131 @@
+from __future__ import annotations
+"""
+Detector: unified-training faÃ§ade presence and richness.
+
+Signals:
+  - config: presence of `class UnifiedTrainingConfig`
+  - runner: presence of `def run_unified_training`
+  - wrappers: legacy wrappers (`train_loop`, `functional_training`) emitting DeprecationWarning
+  - resume_hooks: tokens suggesting resume/checkpoint hooks (`resume_from`, `checkpoint_dir`, `save_checkpoint`)
+
+Scoring (heuristic):
+  - If both config & runner: base 0.60
+  - +0.10 if wrappers present (both)
+  - +0.10 if resume/checkpoint hooks detected
+  - If only one of {config, runner}: base 0.30
+  - Clamp to [0.0, 1.0]
+"""
+from pathlib import Path
+from typing import Dict, Any, Tuple
+
+
+def _read_text(p: Path) -> str:
+    try:
+        return p.read_text(encoding="utf-8")
+    except Exception:
+        return ""
+
+
+def _find_unified_training_path(root: Path) -> Path | None:
+    # Prefer canonical path; otherwise search under src/
+    candidates = [
+        root / "src" / "codex_ml" / "training" / "unified_training.py",
+        root / "codex_ml" / "training" / "unified_training.py",
+    ]
+    for c in candidates:
+        if c.exists():
+            return c
+    # Fallback: crude glob
+    for c in (root / "src").rglob("unified_training.py"):
+        if c.is_file():
+            return c
+    return None
+
+
+def _score(signals: Dict[str, bool]) -> float:
+    config = signals.get("config", False)
+    runner = signals.get("runner", False)
+    wrappers = signals.get("wrappers", False)
+    resume = signals.get("resume_hooks", False)
+    if config and runner:
+        s = 0.60
+        if wrappers:
+            s += 0.10
+        if resume:
+            s += 0.10
+    elif config or runner:
+        s = 0.30
+    else:
+        s = 0.0
+    return float(max(0.0, min(1.0, s)))
+
+
+def detect_unified_training(project_root: str | Path = ".") -> Dict[str, Any]:
+    root = Path(project_root)
+    path = _find_unified_training_path(root)
+    if path is None:
+        return {"score": 0.0, "signals": {"config": False, "runner": False, "wrappers": False, "resume_hooks": False}}
+    txt = _read_text(path)
+    signals = {
+        "config": "class UnifiedTrainingConfig" in txt,
+        "runner": "def run_unified_training" in txt,
+        "wrappers": ("def train_loop" in txt) and ("def functional_training" in txt),
+        "resume_hooks": ("resume_from" in txt) or ("checkpoint_dir" in txt) or ("save_checkpoint" in txt),
+    }
+    return {
+        "path": path.as_posix(),
+        "signals": signals,
+        "score": _score(signals),
+    }
+
+
+__all__ = ["detect_unified_training"]
diff --git a/tests/detectors/test_unified_training_detector.py b/tests/detectors/test_unified_training_detector.py
new file mode 100644
index 0000000..b1a4a9c
--- /dev/null
+++ b/tests/detectors/test_unified_training_detector.py
@@ -0,0 +1,66 @@
+from pathlib import Path
+from codex_ml.detectors.unified_training import detect_unified_training
+
+
+def _write(p: Path, text: str):
+    p.parent.mkdir(parents=True, exist_ok=True)
+    p.write_text(text, encoding="utf-8")
+
+
+def test_detector_scores_minimal_and_full(tmp_path: Path):
+    # Minimal: only runner present
+    p = tmp_path / "src" / "codex_ml" / "training" / "unified_training.py"
+    _write(
+        p,
+        "def run_unified_training(*a, **k):\n    pass\n",
+    )
+    res_min = detect_unified_training(tmp_path)
+    assert 0.29 <= res_min["score"] <= 0.31
+    # Full: config + runner + wrappers + resume hooks
+    _write(
+        p,
+        "class UnifiedTrainingConfig:\n    pass\n"
+        "def run_unified_training(*a, **k):\n    pass\n"
+        "def train_loop(*a, **k):\n    pass\n"
+        "def functional_training(*a, **k):\n    pass\n"
+        "resume_from = None\n"
+        "checkpoint_dir = 'ck'\n",
+    )
+    res_full = detect_unified_training(tmp_path)
+    assert 0.79 <= res_full["score"] <= 0.81
+    sig = res_full["signals"]
+    assert sig["config"] and sig["runner"] and sig["wrappers"] and sig["resume_hooks"]
+
+
+def test_detector_no_file(tmp_path: Path):
+    res = detect_unified_training(tmp_path)
+    assert res["score"] == 0.0
+    assert res["signals"]["config"] is False
diff --git a/tokenization/__init__.py b/tokenization/__init__.py
new file mode 100644
index 0000000..4b1b1e2
--- /dev/null
+++ b/tokenization/__init__.py
@@ -0,0 +1,54 @@
+"""
+Deprecated namespace: `tokenization`
+
+Use `codex_ml.tokenization.api` instead.
+This shim emits a DeprecationWarning and provides minimal convenience
+functions (`encode`, `decode`) backed by the canonical API.
+"""
+from __future__ import annotations
+import warnings
+
+warnings.warn(
+    "The 'tokenization' namespace is deprecated; use 'codex_ml.tokenization.api' instead.",
+    category=DeprecationWarning,
+    stacklevel=2,
+)
+
+try:
+    from codex_ml.tokenization import api as _api  # type: ignore
+except Exception as _e:  # pragma: no cover
+    _api = None  # type: ignore
+
+
+def encode(text: str) -> list[int]:
+    if _api is None:  # pragma: no cover
+        raise RuntimeError("codex_ml.tokenization.api is unavailable")
+    return _api.WhitespaceTokenizer().encode(text)
+
+
+def decode(ids: list[int]) -> str:
+    if _api is None:  # pragma: no cover
+        raise RuntimeError("codex_ml.tokenization.api is unavailable")
+    return _api.WhitespaceTokenizer().decode(ids)
+
+
+# Re-export commonly used symbols for transition
+if _api is not None:  # pragma: no cover
+    WhitespaceTokenizer = _api.WhitespaceTokenizer  # type: ignore
+
+__all__ = ["encode", "decode", "WhitespaceTokenizer"]
diff --git a/tests/config/test_unified_training_config_negative.py b/tests/config/test_unified_training_config_negative.py
new file mode 100644
index 0000000..f2c3d3b
--- /dev/null
+++ b/tests/config/test_unified_training_config_negative.py
@@ -0,0 +1,49 @@
+import pytest
+from codex_ml.training import UnifiedTrainingConfig
+
+
+def test_epochs_must_be_positive_int():
+    with pytest.raises(ValueError):
+        UnifiedTrainingConfig(epochs=0)
+    with pytest.raises(ValueError):
+        UnifiedTrainingConfig(epochs=-1)
+    with pytest.raises(ValueError):
+        UnifiedTrainingConfig(epochs=1.5)  # type: ignore[arg-type]
+
+
+def test_lr_must_be_positive():
+    with pytest.raises(ValueError):
+        UnifiedTrainingConfig(lr=0.0)
+    with pytest.raises(ValueError):
+        UnifiedTrainingConfig(lr=-1e-3)
+
+
+def test_device_must_be_valid():
+    with pytest.raises(ValueError):
+        UnifiedTrainingConfig(device="tpu")  # type: ignore[arg-type]
+
+
+def test_clip_grad_validation_and_modes():
+    # Negative threshold disallowed
+    with pytest.raises(ValueError):
+        UnifiedTrainingConfig(clip_grad_norm=-0.1)
+    # Unknown type disallowed
+    with pytest.raises(ValueError):
+        UnifiedTrainingConfig(clip_grad_norm=0.1, clip_grad_type="mystery")  # type: ignore[arg-type]
+
+
+def test_norm_type_must_be_numeric():
+    with pytest.raises(ValueError):
+        UnifiedTrainingConfig(clip_grad_norm=0.1, norm_type="two")  # type: ignore[arg-type]
diff --git a/CHANGELOG_CODEX.md b/CHANGELOG_CODEX.md
index b8a6b3a..f4a7b9e 100644
--- a/CHANGELOG_CODEX.md
+++ b/CHANGELOG_CODEX.md
@@ -1,5 +1,32 @@
 # Codex Changelog
+## 2025-10-07 â Detector (unified-training) + tokenization shim + config negatives
+
+### WHY
+- Close acceptance criterion (c): capability **detector** recognizes unified-training and maps into training-engine.
+- Provide a **deprecated `tokenization` namespace** shim that warns and forwards to the canonical API to avoid import breakage.
+- Add **negative tests** for `UnifiedTrainingConfig` validations.
+- Restore missing legacy wrapper `functional_training` to satisfy deprecation tests.
+
+### Changes
+- Training:
+  - `unified_training.functional_training` wrapper (deprecated) forwarding to faÃ§ade.
+- Detectors:
+  - `codex_ml.detectors.unified_training` with heuristic scoring and signals.
+  - Tests for minimal/full detection cases.
+- Tokenization:
+  - Root-level `tokenization/__init__.py` shim emitting `DeprecationWarning` and exposing `encode`/`decode`.
+  - Existing deprecation snapshot test now passes.
+- Config:
+  - `tests/config/test_unified_training_config_negative.py` asserts dataclass validations.
+
+### Risk
+- Low. Additive modules; shim only warns and redirects.
+
+### Rollback
+- Remove the detector and shim, delete the tests, and drop the extra wrapper.
+
 ## 2025-10-07 â Checkpoint core: load/apply RNG + retention + tests/docs
 ### WHY
 - Close P2 acceptance criteria by consolidating checkpoint save/load under a single module with digest metadata, RNG snapshots,
   and retention policy. Provide programmatic `apply_rng_state` for resume parity.
```
Why these data tests? Deterministic seeds & sharding are standard practice (PyTorch points to `use_deterministic_algorithms`; DDP uses rank/world partitioning, often via `DistributedSampler`-style index splits). Manifest hashing as a stable aggregate of per-file hashes is also common in integrity work. ([PyTorch Documentation][1])
Below is the **next diff set** adding the missing **data determinism & integrity** utilities, tests, and docs (plus a tiny import surface to satisfy the CLI):
````diff
diff --git a/src/codex_ml/data/__init__.py b/src/codex_ml/data/__init__.py
new file mode 100644
index 0000000..f2c0b0a
--- /dev/null
+++ b/src/codex_ml/data/__init__.py
@@ -0,0 +1,18 @@
+"""
+Data utilities public surface.
+"""
+from .utils import (
+    compute_file_sha256,
+    manifest_checksums,
+    shuffle_indices,
+    shard_range,
+    iter_lines_normalized,
+)
+
+__all__ = [
+    "compute_file_sha256",
+    "manifest_checksums",
+    "shuffle_indices",
+    "shard_range",
+    "iter_lines_normalized",
+]
diff --git a/src/codex_ml/data/utils.py b/src/codex_ml/data/utils.py
new file mode 100644
index 0000000..a37a8f2
--- /dev/null
+++ b/src/codex_ml/data/utils.py
@@ -0,0 +1,162 @@
+from __future__ import annotations
+import hashlib
+import io
+import os
+import unicodedata
+from pathlib import Path
+from typing import Dict, Iterable, Iterator, List, Tuple
+import random
+
+
+def compute_file_sha256(path: str | Path) -> str:
+    """
+    Compute SHA-256 of a file path.
+    """
+    p = Path(path)
+    h = hashlib.sha256()
+    with p.open("rb") as f:
+        for chunk in iter(lambda: f.read(8192), b""):
+            h.update(chunk)
+    return h.hexdigest()
+
+
+def manifest_checksums(files: Iterable[str | Path]) -> Tuple[Dict[str, str], str]:
+    """
+    Given an iterable of file paths, return (per_file_hashes, aggregate_hash).
+    The aggregate hash is SHA-256 over the colon-joined list of each file's SHA-256
+    in *sorted* path order for stability.
+    """
+    items: List[Tuple[str, str]] = []
+    for fp in files:
+        p = Path(fp).resolve()
+        if p.is_file():
+            items.append((p.as_posix(), compute_file_sha256(p)))
+    items.sort(key=lambda x: x[0])
+    per: Dict[str, str] = {k: v for k, v in items}
+    h = hashlib.sha256()
+    h.update(":".join(v for _, v in items).encode("utf-8"))
+    return per, h.hexdigest()
+
+
+def shuffle_indices(n: int, seed: int) -> List[int]:
+    """
+    Deterministic index permutation of range(n) using the given seed.
+    """
+    idx = list(range(n))
+    rng = random.Random(int(seed))
+    rng.shuffle(idx)
+    return idx
+
+
+def shard_range(n: int, rank: int, world: int) -> Tuple[int, int]:
+    """
+    Compute [start, end) index range for a shard out of `world` shards given `rank`.
+    Partitions are contiguous and cover [0, n) without overlap.
+    """
+    if world <= 0:
+        raise ValueError("world must be positive")
+    if rank < 0 or rank >= world:
+        raise ValueError("rank out of range")
+    base = n // world
+    rem = n % world
+    # First `rem` shards take one extra element
+    start = rank * base + min(rank, rem)
+    end = start + base + (1 if rank < rem else 0)
+    return start, end
+
+
+def _normalize_text(s: str) -> str:
+    # Normalize to NFKC for stability across Unicode variants
+    return unicodedata.normalize("NFKC", s.replace("\r\n", "\n").replace("\r", "\n"))
+
+
+def iter_lines_normalized(path: str | Path, *, encoding_candidates: Iterable[str] = ("utf-8", "latin-1")) -> Iterator[str]:
+    """
+    Iterate lines from a file, attempting encodings in order; replace errors if needed.
+    Lines are normalized to NFKC and LF newlines.
+    """
+    p = Path(path)
+    data: bytes = p.read_bytes()
+    text: str | None = None
+    for enc in encoding_candidates:
+        try:
+            text = data.decode(enc, errors="strict")
+            break
+        except Exception:
+            continue
+    if text is None:
+        # Fallback with replacement to ensure progress (deterministic)
+        text = data.decode("utf-8", errors="replace")
+    buf = io.StringIO(_normalize_text(text))
+    for line in buf:
+        yield line.rstrip("\n")
diff --git a/tests/data/test_data_determinism.py b/tests/data/test_data_determinism.py
new file mode 100644
index 0000000..f1b2b78
--- /dev/null
+++ b/tests/data/test_data_determinism.py
@@ -0,0 +1,130 @@
+from __future__ import annotations
+from pathlib import Path
+from codex_ml.data.utils import manifest_checksums, shuffle_indices, shard_range, iter_lines_normalized
+
+
+def test_manifest_hash_stability_and_change(tmp_path: Path):
+    a = tmp_path / "a.txt"
+    b = tmp_path / "b.txt"
+    a.write_text("alpha\n", encoding="utf-8")
+    b.write_text("beta\n", encoding="utf-8")
+    per1, agg1 = manifest_checksums([a, b])
+    per2, agg2 = manifest_checksums([a, b])
+    assert per1 == per2
+    assert agg1 == agg2
+    # Change content -> aggregate must change
+    b.write_text("beta2\n", encoding="utf-8")
+    _, agg3 = manifest_checksums([a, b])
+    assert agg3 != agg2
+
+
+def test_shuffle_indices_seed_repro_and_diff():
+    n = 100
+    s1 = shuffle_indices(n, seed=123)
+    s2 = shuffle_indices(n, seed=123)
+    s3 = shuffle_indices(n, seed=124)
+    assert s1 == s2
+    assert s1 != s3
+    # All indices covered, no duplicates
+    assert sorted(s1) == list(range(n))
+
+
+def test_shard_range_covers_disjoint_union():
+    n = 101
+    world = 8
+    seen = []
+    for rank in range(world):
+        lo, hi = shard_range(n, rank=rank, world=world)
+        assert 0 <= lo <= hi <= n
+        seen.extend(range(lo, hi))
+    # Full coverage with no duplicates
+    assert len(seen) == n
+    assert sorted(seen) == list(range(n))
+
+
+def test_iter_lines_normalized_handles_mixed_encodings(tmp_path: Path):
+    p = tmp_path / "mixed.txt"
+    # Compose a byte sequence that's invalid UTF-8 but valid latin-1
+    data = "cafÃ©\n".encode("latin-1") + b"\xff\xfe" + "A\u00A0B\r\nC\u200B".encode("utf-8")
+    p.write_bytes(data)
+    lines = list(iter_lines_normalized(p))
+    # Basic expectations: newlines normalized, zero-width space removed by NFKC no-op (kept as it's not removed here),
+    # non-breaking space normalized to regular space under NFKC.
+    assert lines[0] == "cafÃ©"
+    assert lines[1].endswith("B")  # NBSP collapsed to space then rstrip LF
+    assert "C" in lines[-1]
diff --git a/docs/data_determinism.md b/docs/data_determinism.md
new file mode 100644
index 0000000..7f6e5de
--- /dev/null
+++ b/docs/data_determinism.md
@@ -0,0 +1,82 @@
+# Data Determinism & Integrity
+
+This module provides small, local primitives for reproducible data handling:
+
+## Manifest checksums
+
+```python
+from codex_ml.data import manifest_checksums
+per, agg = manifest_checksums(["data/a.txt", "data/b.txt"])
+print(agg)  # stable SHA-256 over per-file hashes in sorted path order
+```
+
+Use a **stable aggregate hash of per-file hashes** to detect changes to any input without relying on tool-specific archives. See general checksum/integrity practices for background. (No network required.)
+
+## Deterministic shuffling
+
+```python
+from codex_ml.data import shuffle_indices
+idx = shuffle_indices(n=1_000, seed=42)
+```
+
+Seeds yield identical permutations across runs, which aligns with PyTorchâs reproducibility guidance when paired with deterministic algorithms during training.
+
+## Shard partitioning
+
+```python
+from codex_ml.data import shard_range
+start, end = shard_range(n=dataset_len, rank=my_rank, world=world_size)
+```
+
+This yields contiguous, non-overlapping shards whose union covers the datasetâcompatible with the common DDP mental model of **rank** and **world size**.
+
+## Normalized line iteration
+
+```python
+from codex_ml.data import iter_lines_normalized
+for line in iter_lines_normalized("mixed-encoding.txt"):
+    ...
+```
+
+Tries common encodings in order and normalizes to **NFKC** with LF newlines, so mixed sources are readable deterministically.
+
+---
+
+### Notes & Rationale
+- Reproducible training combines deterministic data handling with deterministic kernels. PyTorch provides `torch.use_deterministic_algorithms` and related settings for the latter.
+- For distributed workloads, DDP-style sharding partitions by **rank/world** and guarantees coverage without overlap.
+- Manifest hashing with per-file SHA-256 yields a small, stable fingerprint suitable for badges or cache keys.
+
+All utilities are local/offline and safe to run in air-gapped environments.
diff --git a/CHANGELOG_CODEX.md b/CHANGELOG_CODEX.md
index f4a7b9e..c3f4f98 100644
--- a/CHANGELOG_CODEX.md
+++ b/CHANGELOG_CODEX.md
@@ -1,5 +1,26 @@
 # Codex Changelog
+## 2025-10-07 â Data determinism & integrity utilities + tests & docs
+
+### WHY
+- Close the remaining P1 acceptance criteria for **data-pipeline determinism** with property-style tests:
+  stable manifest hash, seed reproducibility, shard coverage, and mixed-encoding reads.
+
+### Changes
+- Package:
+  - `src/codex_ml/data/utils.py` and `src/codex_ml/data/__init__.py`.
+- Tests:
+  - `tests/data/test_data_determinism.py`.
+- Docs:
+  - `docs/data_determinism.md`.
+
+### Risk
+- Low. Self-contained utilities; pure local file/CPU usage.
+
+### Rollback
+- Remove the new data module, tests, and docs.
+
 ## 2025-10-07 â Detector (unified-training) + tokenization shim + config negatives
 ### WHY
 - Close acceptance criterion (c): capability **detector** recognizes unified-training and maps into training-engine.
 - Provide a **deprecated `tokenization` namespace** shim that warns and forwards to the canonical API to avoid import breakage.
````
````diff
diff --git a/src/codex_ml/tracking/guards.py b/src/codex_ml/tracking/guards.py
new file mode 100644
index 0000000..f7a2c8e
--- /dev/null
+++ b/src/codex_ml/tracking/guards.py
@@ -0,0 +1,203 @@
+from __future__ import annotations
+"""
+Offline-first tracking guards for MLflow and W&B.
+
+Policy:
+- If offline posture is detected, any remote MLflow URI is blocked and rewritten to a local `file://` URI.
+- W&B is gated by `WANDB_MODE=offline` (or `disabled`) to avoid network egress.
+
+Background:
+- PyTorch reproducibility: `torch.use_deterministic_algorithms(True, warn_only=True)` (training loop handles this).  # See docs: https://docs.pytorch.org/docs/stable/notes/randomness.html
+- MLflow remote URIs are enabled by setting `MLFLOW_TRACKING_URI` (e.g., http://host:5000).  # mlflow docs
+- W&B offline posture is controlled by `WANDB_MODE=offline` or `wandb offline`.  # wandb docs
+"""
+import os
+import re
+import time
+from dataclasses import dataclass, asdict
+from pathlib import Path
+from typing import Dict, Any, Mapping, Optional
+
+DEFAULT_LOCAL_URI = "file://" + (Path.cwd() / "mlruns").as_posix()
+ALLOW_REMOTE_ENV = "CODEX_ALLOW_REMOTE_TRACKING"
+
+_REMOTE_SCHEMES = ("http://", "https://", "databricks://", "postgresql://", "mysql://", "sqlite://", "file:///", "wasbs://", "s3://", "gs://")
+
+
+def _is_remote_uri(uri: str) -> bool:
+    u = uri.strip().lower()
+    return u.startswith(_REMOTE_SCHEMES) and not u.startswith("file://")
+
+
+def normalize_mlflow_uri(uri: Optional[str], *, default_local: str = DEFAULT_LOCAL_URI) -> str:
+    """
+    Normalize an MLflow URI. If absent, return local `file://...`.
+    """
+    if not uri:
+        return default_local
+    u = uri.strip()
+    # Expand bare paths into file://
+    if "://" not in u:
+        return "file://" + Path(u).resolve().as_posix()
+    return u
+
+
+@dataclass
+class TrackingDecision:
+    uri: str
+    blocked: bool
+    reason: str
+    details: Dict[str, Any]
+
+
+def _offline_posture(env: Mapping[str, str]) -> bool:
+    """
+    Offline posture if:
+      - MLFLOW_OFFLINE=1, or
+      - WANDB_MODE in {"offline", "disabled"}
+    """
+    if env.get("MLFLOW_OFFLINE", "").strip() in {"1", "true", "True"}:
+        return True
+    mode = (env.get("WANDB_MODE", "") or "").strip().lower()
+    if mode in {"offline", "disabled"}:
+        return True
+    return False
+
+
+def decide_mlflow_tracking_uri(*, environ: Optional[Mapping[str, str]] = None) -> TrackingDecision:
+    """
+    Decide the effective MLflow tracking URI under an offline-first policy.
+    - If offline and URI is remote, rewrite to local and mark as blocked.
+    - If remote and explicit allow flag is set, pass through.
+    - Always return a normalized URI.
+    """
+    env = dict(environ or os.environ)
+    raw = env.get("MLFLOW_TRACKING_URI")
+    uri = normalize_mlflow_uri(raw)
+    offline = _offline_posture(env)
+    allow_remote = env.get(ALLOW_REMOTE_ENV, "").strip() in {"1", "true", "True"}
+    blocked = False
+    reason = "ok"
+
+    if offline and _is_remote_uri(uri):
+        blocked = True
+        reason = "offline-posture: remote mlflow uri blocked -> file"
+        uri = DEFAULT_LOCAL_URI
+    elif _is_remote_uri(uri) and not allow_remote:
+        # Even if not offline, remain conservative without explicit allow.
+        blocked = True
+        reason = "no-explicit-allow: remote mlflow uri blocked -> file"
+        uri = DEFAULT_LOCAL_URI
+
+    details: Dict[str, Any] = {
+        "ts": time.time(),
+        "raw_uri": raw,
+        "offline": offline,
+        "allow_remote_flag": allow_remote,
+        "env_wandb_mode": env.get("WANDB_MODE"),
+    }
+    return TrackingDecision(uri=uri, blocked=blocked, reason=reason, details=details)
diff --git a/tests/tracking/test_tracking_guard_matrix.py b/tests/tracking/test_tracking_guard_matrix.py
new file mode 100644
index 0000000..e74e9e2
--- /dev/null
+++ b/tests/tracking/test_tracking_guard_matrix.py
@@ -0,0 +1,130 @@
+import os
+import itertools
+import pytest
+from codex_ml.tracking.guards import decide_mlflow_tracking_uri, normalize_mlflow_uri, DEFAULT_LOCAL_URI, ALLOW_REMOTE_ENV
+
+
+LOCAL_PATH = "mlruns_local"
+REMOTE_HTTP = "http://mlflow.example:5000"
+REMOTE_S3 = "s3://bucket/mlruns"
+
+
+@pytest.mark.parametrize(
+    "uri,expected_prefix",
+    [
+        (None, "file://"),
+        ("./mlruns_here", "file://"),
+        ("file:///tmp/mlruns", "file://"),
+        (REMOTE_HTTP, "http://"),
+    ],
+)
+def test_normalize_mlflow_uri(uri, expected_prefix, tmp_path):
+    os.chdir(tmp_path)
+    out = normalize_mlflow_uri(uri)
+    assert out.startswith(expected_prefix)
+
+
+@pytest.mark.parametrize("remote_uri", [REMOTE_HTTP, REMOTE_S3])
+@pytest.mark.parametrize("mlflow_offline", ["", "0", "1"])
+@pytest.mark.parametrize("wandb_mode", ["", "offline", "disabled"])
+@pytest.mark.parametrize("allow_remote", ["", "1"])
+def test_decide_matrix(remote_uri, mlflow_offline, wandb_mode, allow_remote):
+    env = {
+        "MLFLOW_TRACKING_URI": remote_uri,
+        "MLFLOW_OFFLINE": mlflow_offline,
+        "WANDB_MODE": wandb_mode,
+        ALLOW_REMOTE_ENV: allow_remote,
+    }
+    d = decide_mlflow_tracking_uri(environ=env)
+    offline = (mlflow_offline in {"1"}) or (wandb_mode in {"offline", "disabled"})
+    allowed = allow_remote == "1"
+    if offline:
+        # Always blocked under offline posture
+        assert d.blocked is True
+        assert d.uri.startswith("file://")
+    else:
+        if allowed:
+            assert d.blocked is False
+            assert d.uri == remote_uri
+        else:
+            assert d.blocked is True
+            assert d.uri.startswith("file://")
+
+
+def test_local_uri_passes(tmp_path):
+    env = {
+        "MLFLOW_TRACKING_URI": f"file://{tmp_path.as_posix()}",
+    }
+    d = decide_mlflow_tracking_uri(environ=env)
+    assert d.blocked is False
+    assert d.uri.startswith("file://")
diff --git b/docs/observability.md a/docs/observability.md
new file mode 100644
index 0000000..9a7a0e2
--- /dev/null
+++ b/docs/observability.md
@@ -0,0 +1,64 @@
+# Observability (Offline-First)
+
+We gate outbound tracking by default and emit **structured NDJSON** logs locally.
+
+## Tracking guards
+
+- **MLflow**: We treat remote URIs (e.g., `http://host:5000`, `s3://bucket`) as **remote**. In offline posture or without an explicit allow flag, remote URIs are blocked and rewritten to a local `file://...` run store.
+  - MLflow remote behavior is configured via `MLFLOW_TRACKING_URI`. See docs.
+  - Our allow-flag is `CODEX_ALLOW_REMOTE_TRACKING=1`.
+- **W&B**: Gate via `WANDB_MODE` set to `offline` (or `disabled`), which prevents runs from syncing to the hosted service.
+
+CLI helper:
+
+```bash
+python -m codex_ml tracking decide --summary tracking_summary.ndjson --print
+```
+
+## Structured logs
+
+Use the JSON logger to write NDJSON:
+
+```python
+from codex_ml.logging import json_logger
+log = json_logger("codex")
+log.info("step", extra={"extra": {"loss": 0.123}})
+```
+
+Convert NDJSON to CSV:
+
+```bash
+python tools/ndjson_summarize.py my.ndjson --output my.csv
+```
diff --git a/src/codex_ml/tokenization/__init__.py b/src/codex_ml/tokenization/__init__.py
new file mode 100644
index 0000000..b5a6a43
--- /dev/null
+++ b/src/codex_ml/tokenization/__init__.py
@@ -0,0 +1,5 @@
+"""
+Tokenization namespace.
+"""
+from .api import WhitespaceTokenizer
+__all__ = ["WhitespaceTokenizer"]
diff --git a/src/codex_ml/tokenization/api.py b/src/codex_ml/tokenization/api.py
new file mode 100644
index 0000000..b2c9d5e
--- /dev/null
+++ b/src/codex_ml/tokenization/api.py
@@ -0,0 +1,167 @@
+from __future__ import annotations
+"""
+Canonical tokenization API.
+
+Contract:
+- encode(text) -> List[int]
+- batch_encode(texts: List[str]) -> List[List[int]]
+- decode(ids) -> str
+- vocab_size -> int
+- save(path) / load(path)
+
+This module provides a minimal, deterministic **WhitespaceTokenizer** as the default fallback.
+"""
+from dataclasses import dataclass, field
+from pathlib import Path
+from typing import Dict, List
+import json
+import unicodedata
+
+
+def _normalize(s: str) -> str:
+    # Normalize whitespace variants and Unicode to NFKC; collapse spaces
+    s = unicodedata.normalize("NFKC", s.replace("\r\n", "\n").replace("\r", "\n"))
+    return " ".join(s.split())
+
+
+@dataclass
+class WhitespaceTokenizer:
+    unk_token: str = "<unk>"
+    _token2id: Dict[str, int] = field(default_factory=dict)
+    _id2token: Dict[int, str] = field(default_factory=dict)
+
+    def __post_init__(self) -> None:
+        if self.unk_token not in self._token2id:
+            self._register(self.unk_token)
+
+    @property
+    def vocab_size(self) -> int:
+        return len(self._token2id)
+
+    def _register(self, tok: str) -> int:
+        if tok not in self._token2id:
+            idx = len(self._token2id)
+            self._token2id[tok] = idx
+            self._id2token[idx] = tok
+        return self._token2id[tok]
+
+    def encode(self, text: str) -> List[int]:
+        ids: List[int] = []
+        for tok in _normalize(text).split(" "):
+            if tok == "":
+                continue
+            idx = self._token2id.get(tok)
+            if idx is None:
+                idx = self._register(tok)
+            ids.append(idx)
+        return ids or [self._token2id[self.unk_token]]
+
+    def batch_encode(self, texts: List[str]) -> List[List[int]]:
+        return [self.encode(t) for t in texts]
+
+    def decode(self, ids: List[int]) -> str:
+        toks = [self._id2token.get(int(i), self.unk_token) for i in ids]
+        return " ".join(toks)
+
+    def save(self, path: str | Path) -> None:
+        p = Path(path)
+        p.parent.mkdir(parents=True, exist_ok=True)
+        obj = {"unk_token": self.unk_token, "token2id": self._token2id}
+        p.write_text(json.dumps(obj, ensure_ascii=False, indent=2), encoding="utf-8")
+
+    @classmethod
+    def load(cls, path: str | Path) -> "WhitespaceTokenizer":
+        p = Path(path)
+        obj = json.loads(p.read_text(encoding="utf-8"))
+        tok = cls(unk_token=obj.get("unk_token", "<unk>"))
+        # Reconstruct maps in stable order by id
+        t2i = obj.get("token2id", {})
+        tok._token2id = dict(t2i)
+        tok._id2token = {int(v): k for k, v in t2i.items()}
+        return tok
diff --git a/tests/tokenization/test_tokenization_api.py b/tests/tokenization/test_tokenization_api.py
new file mode 100644
index 0000000..2bd8f21
--- /dev/null
+++ b/tests/tokenization/test_tokenization_api.py
@@ -0,0 +1,62 @@
+from pathlib import Path
+from codex_ml.tokenization.api import WhitespaceTokenizer
+
+
+def test_roundtrip_and_vocab_growth(tmp_path: Path):
+    tok = WhitespaceTokenizer()
+    ids = tok.encode("a b a")
+    assert tok.decode(ids) == "a b a"
+    # Vocab grows only on unseen tokens
+    n0 = tok.vocab_size
+    tok.encode("a b c")
+    assert tok.vocab_size == n0 + 1
+
+
+def test_batch_and_save_load(tmp_path: Path):
+    tok = WhitespaceTokenizer()
+    batch = tok.batch_encode(["x  y", "z"])
+    assert batch and isinstance(batch[0], list)
+    p = tmp_path / "tok.json"
+    tok.save(p)
+    tok2 = WhitespaceTokenizer.load(p)
+    assert tok2.vocab_size == tok.vocab_size
+    assert tok2.decode(tok2.encode("x y")) == "x y"
diff --git a/docs/tokenization_api.md b/docs/tokenization_api.md
new file mode 100644
index 0000000..f5ab8f1
--- /dev/null
+++ b/docs/tokenization_api.md
@@ -0,0 +1,64 @@
+# Tokenization API
+
+Canonical surface:
+
+```python
+from codex_ml.tokenization.api import WhitespaceTokenizer
+```
+
+## Contract
+- `encode(text) -> List[int>`
+- `batch_encode(texts) -> List[List[int]]`
+- `decode(ids) -> str`
+- `vocab_size -> int`
+- `save(path) / load(path)`
+
+### Fallback tokenizer
+`WhitespaceTokenizer` is deterministic and normalizes to **NFKC** with collapsed whitespace, suitable for tests and small utilities.
+
+### Deprecation shims
+A legacy module `tokenization` exists and emits `DeprecationWarning`, forwarding simple helpers to the canonical API.
+
+> For advanced tokenization (e.g., subword/BPE), this faÃ§ade can wrap external libraries while preserving the contract above.
diff --git a/README.md b/README.md
index 6827d82..64a0f6f 100644
--- a/README.md
+++ b/README.md
@@ -15,6 +15,7 @@ Operational notes: `docs/ops.md`.
 Architecture overview: `docs/architecture.md`.
 **Docs index:** see `docs/index.md` for a map of features and guides.
+Observability & tracking guards: `docs/observability.md`.
 ## Quickstart
diff --git a/CHANGELOG_CODEX.md b/CHANGELOG_CODEX.md
index c3f4f98..b27c6fe 100644
--- a/CHANGELOG_CODEX.md
+++ b/CHANGELOG_CODEX.md
@@ -1,5 +1,35 @@
 # Codex Changelog
+## 2025-10-07 â Tracking guard matrix + tokenization API (canonical)
+
+### WHY
+- Close P1 logging/tracking **matrix coverage** with explicit MLflow/W&B permutations and local rewrite semantics.
+  - MLflow remote tracking URIs are commonly configured via `MLFLOW_TRACKING_URI`; we ensure offline-first posture by default. :contentReference[oaicite:0]{index=0}
+  - W&B offline mode is enforced when `WANDB_MODE=offline`/`disabled`, preventing sync; our decision object reflects posture. :contentReference[oaicite:1]{index=1}
+- Finalize a **canonical tokenization API** with a deterministic `WhitespaceTokenizer` and roundtrip/save/load tests.
+
+### Changes
+- Tracking:
+  - `codex_ml.tracking.guards` with `TrackingDecision`, `decide_mlflow_tracking_uri`, `normalize_mlflow_uri`, allow-flag, and defaults.
+  - Tests: `tests/tracking/test_tracking_guard_matrix.py`.
+  - Docs: `docs/observability.md` (offline-first gating + NDJSON).
+- Tokenization:
+  - `src/codex_ml/tokenization/api.py` and export in `__init__.py`.
+  - Tests: `tests/tokenization/test_tokenization_api.py`.
+  - Docs: `docs/tokenization_api.md`.
+- README: link to Observability docs.
+
+### Risk
+- Low. Additive utilities and tests; defaults bias to local filesystem URIs.
+
+### Rollback
+- Remove the tracking/tokenization modules, tests, and docs additions.
+
 ## 2025-10-07 â Data determinism & integrity utilities + tests & docs
 ### WHY
 - Close the remaining P1 acceptance criteria for **data-pipeline determinism** with property-style tests:
 - stable manifest hash, seed reproducibility, shard coverage, and mixed-encoding reads.
````
````diff
diff --git a/src/codex_ml/checkpointing/compat.py b/src/codex_ml/checkpointing/compat.py
new file mode 100644
index 0000000..8f2d3a1
--- /dev/null
+++ b/src/codex_ml/checkpointing/compat.py
@@ -0,0 +1,74 @@
+from __future__ import annotations
+"""
+Deprecated compatibility wrappers for legacy checkpoint helpers.
+
+Re-exports map to the canonical core and emit DeprecationWarning once.
+"""
+import warnings
+from .checkpoint_core import save_checkpoint, load_checkpoint, calculate_digest as _calculate_digest
+
+_WARNED = False
+
+
+def _warn_once():
+    global _WARNED
+    if not _WARNED:
+        warnings.warn(
+            "codex_ml.checkpointing.compat is deprecated; use codex_ml.checkpointing.{save_checkpoint,load_checkpoint,calculate_digest}",
+            category=DeprecationWarning,
+            stacklevel=2,
+        )
+        _WARNED = True
+
+
+def save_ckpt(out_dir, *, state, meta=None, keep_last_k: int = 5):
+    _warn_once()
+    return save_checkpoint(out_dir, state=state, meta=meta, keep_last_k=keep_last_k)
+
+
+def load_ckpt(in_dir, *, map_location: str | None = None):
+    _warn_once()
+    return load_checkpoint(in_dir, map_location=map_location)
+
+
+def calc_digest(directory):
+    _warn_once()
+    return _calculate_digest(directory)
diff --git a/tests/checkpointing/test_checkpoint_compat.py b/tests/checkpointing/test_checkpoint_compat.py
new file mode 100644
index 0000000..a1e2c78
--- /dev/null
+++ b/tests/checkpointing/test_checkpoint_compat.py
@@ -0,0 +1,54 @@
+import warnings
+import pytest
+
+torch = pytest.importorskip("torch")
+nn = torch.nn
+optim = torch.optim
+
+from codex_ml.checkpointing.compat import save_ckpt, load_ckpt, calc_digest
+from codex_ml.checkpointing import save_checkpoint, load_checkpoint, calculate_digest
+
+
+def _toy_state():
+    m = nn.Sequential(nn.Linear(1, 2), nn.ReLU(), nn.Linear(2, 1))
+    o = optim.SGD(m.parameters(), lr=1e-2)
+    return {"model": m.state_dict(), "optimizer": o.state_dict()}, {"epoch": 0}
+
+
+def test_compat_wrappers_emit_warning_and_roundtrip(tmp_path):
+    with warnings.catch_warnings(record=True) as rec:
+        warnings.simplefilter("always", DeprecationWarning)
+        state, meta = _toy_state()
+        out = tmp_path / "epoch-0000"
+        save_ckpt(out, state=state, meta=meta)
+        assert any(isinstance(w.message, DeprecationWarning) for w in rec)
+    st, m = load_ckpt(out)
+    assert "model" in st and "optimizer" in st and "rng" in st
+    # digests equal between compat and canonical paths
+    assert calc_digest(out) == calculate_digest(out)
+
+
+def test_mix_compat_and_canonical(tmp_path):
+    # Save with canonical, load with compat (and vice versa)
+    state, meta = _toy_state()
+    out = tmp_path / "epoch-0001"
+    save_checkpoint(out, state=state, meta=meta)
+    st1, m1 = load_ckpt(out)
+    st2, m2 = load_checkpoint(out)
+    assert st1.keys() == st2.keys()
+    assert m1.get("epoch") == m2.get("epoch") == 0
diff --git a/docs/unified_training.md b/docs/unified_training.md
new file mode 100644
index 0000000..f47b1f6
--- /dev/null
+++ b/docs/unified_training.md
@@ -0,0 +1,118 @@
+# Unified Training FaÃ§ade
+
+`codex_ml.training.run_unified_training(cfg, ...)` is the single entrypoint for simple, deterministic-friendly training.
+
+## Config (highlights)
+```python
+from codex_ml.training import UnifiedTrainingConfig
+
+cfg = UnifiedTrainingConfig(
+    epochs=2,
+    lr=1e-2,
+    deterministic=True,        # enable torch deterministic algorithms where available
+    clip_grad_norm=None,       # or a float; set clip_grad_type='norm'|'value'
+    device="cpu",              # or "cuda" if available
+    checkpoint_dir="runs/ck",  # optional: save epoch-XXXX checkpoints
+    resume_from=None,          # optional: path to a previous epoch-XXXX directory
+)
+```
+
+## Usage
+```python
+from torch import nn
+import torch
+from codex_ml.training import run_unified_training, UnifiedTrainingConfig
+
+model = nn.Sequential(nn.Linear(1, 8), nn.ReLU(), nn.Linear(8, 1))
+opt = torch.optim.SGD(model.parameters(), lr=1e-2)
+loss = nn.MSELoss()
+cfg = UnifiedTrainingConfig(epochs=2, lr=1e-2, deterministic=True, device="cpu")
+history = run_unified_training(cfg, model=model, optimizer=opt, loss_fn=loss)
+print(history["loss"])
+```
+
+## Resume semantics
+- If `resume_from` points to an `epoch-XXXX` directory saved by our **checkpoint core**, the loop:
+  - restores **model**, **optimizer**, and **RNG**,
+  - sets the starting epoch to `XXXX + 1`.
+- When `checkpoint_dir` is set, the loop writes checkpoints at each epoch boundary and keeps the last **K** by default.
+
+## Gradient clipping
+- `clip_grad_norm` controls clipping:
+  - `clip_grad_type='norm'` uses `clip_grad_norm_` with `norm_type` (default L2).
+  - `clip_grad_type='value'` uses `clip_grad_value_`.
+- If utilities are unavailable (older torch), clipping is skipped without failing.
+
+## Determinism
+- With `deterministic=True`, the loop enables PyTorch deterministic algorithms (warn-only) and seeds Python/torch (and CUDA if present). See `tests/training/test_unified_training_parity.py` for parity checks.
+
+## Legacy shims
+- `train_loop(...)` and `functional_training(...)` forward to the faÃ§ade and emit `DeprecationWarning`. Tests assert warnings to keep the transition visible.
diff --git a/tools/bench_tokenizer.py b/tools/bench_tokenizer.py
new file mode 100755
index 0000000..7b6d2ce
--- /dev/null
+++ b/tools/bench_tokenizer.py
@@ -0,0 +1,78 @@
+#!/usr/bin/env python3
+from __future__ import annotations
+"""
+Micro-benchmark: whitespace tokenizer throughput (docs/sec, rough).
+"""
+import time
+from pathlib import Path
+from codex_ml.tokenization.api import WhitespaceTokenizer
+
+
+def _corpus(n: int = 100_000) -> list[str]:
+    # Tiny synthetic corpus
+    base = "the quick brown fox jumps over the lazy dog 123 Î±Î²Î³ Î´Î­Î¶Î·"
+    return [base for _ in range(n)]
+
+
+def bench(n_docs: int = 50_000) -> None:
+    tok = WhitespaceTokenizer()
+    docs = _corpus(n_docs)
+    t0 = time.perf_counter()
+    _ = tok.batch_encode(docs)
+    t1 = time.perf_counter()
+    dt = max(1e-12, t1 - t0)
+    rate = n_docs / dt
+    print(f"docs={n_docs} time={dt:.3f}s => {rate:.1f} docs/sec; vocab={tok.vocab_size}")
+
+
+if __name__ == "__main__":
+    bench()
diff --git a/docs/index.md b/docs/index.md
index a1d2a7a..1f3d28e 100644
--- a/docs/index.md
+++ b/docs/index.md
@@ -1,6 +1,7 @@
 # Documentation Index
 - **Unified Training**: `docs/unified_training.md`
+- **Tokenizer Benchmark**: `tools/bench_tokenizer.py`
 - **Checkpoint Core**: `docs/checkpointing_core.md`
 - **Tracking & Observability**: `docs/observability.md`, `docs/observability_metrics.md`
 - **Tokenization API**: `docs/tokenization_api.md`
 - **Data Determinism**: `docs/data_determinism.md`
diff --git a/README.md b/README.md
index 64a0f6f..2e2c6a0 100644
--- a/README.md
+++ b/README.md
@@ -56,6 +56,7 @@ See `docs/cli.md` for more.
 ## Performance
 - Tokenizer throughput: `python tools/bench_tokenizer.py`
 - Unified training throughput: `python tools/bench_unified_training.py`
 - Markdown fence validator (pre-commit hook): `python tools/validate_fences.py --paths README.md docs`
+See `docs/unified_training.md` and `docs/tokenization_api.md` for details.
diff --git a/CHANGELOG_CODEX.md b/CHANGELOG_CODEX.md
index b27c6fe..4e8a0de 100644
--- a/CHANGELOG_CODEX.md
+++ b/CHANGELOG_CODEX.md
@@ -1,5 +1,27 @@
 # Codex Changelog
+## 2025-10-07 â Checkpoint compat shims + unified training docs + tokenizer bench
+
+### WHY
+- Provide **deprecated** checkpointing wrappers to avoid churn while consolidating on the core API.
+- Document the **unified training faÃ§ade**.
+- Add a small **tokenizer benchmark** script referenced in perf docs/README.
+
+### Changes
+- Checkpointing:
+  - `src/codex_ml/checkpointing/compat.py` with deprecation warnings.
+  - Tests: `tests/checkpointing/test_checkpoint_compat.py`.
+- Docs:
+  - `docs/unified_training.md`; link in docs index and README perf section.
+- Tools:
+  - `tools/bench_tokenizer.py`.
+
+### Risk
+- Low. Shims only warn and forward; docs/tools are additive.
+
+### Rollback
+- Remove the compat module/tests/docs/tool and revert links.
+
 ## 2025-10-07 â Tracking guard matrix + tokenization API (canonical)
 ### WHY
 - Close P1 logging/tracking **matrix coverage** with explicit MLflow/W&B permutations and local rewrite semantics.
 - Finalize a **canonical tokenization API** with a deterministic `WhitespaceTokenizer` and roundtrip/save/load tests.
````
````diff
diff --git a/src/codex_ml/tracking/guards.py b/src/codex_ml/tracking/guards.py
index f7a2c8e..9b1f5f4 100644
--- a/src/codex_ml/tracking/guards.py
+++ b/src/codex_ml/tracking/guards.py
@@ -1,12 +1,13 @@
 from __future__ import annotations
 """
 Offline-first tracking guards for MLflow and W&B.
 Policy:
 - If offline posture is detected, any remote MLflow URI is blocked and rewritten to a local `file://` URI.
-- W&B is gated by `WANDB_MODE=offline` (or `disabled`) to avoid network egress.
+- W&B is gated by `WANDB_MODE=offline` (or `disabled`) or `WANDB_DISABLED=true` to avoid network egress.
+- An explicit `WANDB_ENABLE=1` overrides W&B offline posture check.
 Background:
 - PyTorch reproducibility: `torch.use_deterministic_algorithms(True, warn_only=True)` (training loop handles this).  # See docs: https://docs.pytorch.org/docs/stable/notes/randomness.html
 - MLflow remote URIs are enabled by setting `MLFLOW_TRACKING_URI` (e.g., http://host:5000).  # mlflow docs
-- W&B offline posture is controlled by `WANDB_MODE=offline` or `wandb offline`.  # wandb docs
+- W&B offline posture is controlled by `WANDB_MODE=offline` / `WANDB_DISABLED=true` (overridden by `WANDB_ENABLE=1`).  # wandb docs
 """
 import os
 import re
 import time
@@ -23,6 +24,8 @@ DEFAULT_LOCAL_URI = "file://" + (Path.cwd() / "mlruns").as_posix()
 ALLOW_REMOTE_ENV = "CODEX_ALLOW_REMOTE_TRACKING"
 _REMOTE_SCHEMES = ("http://", "https://", "databricks://", "postgresql://", "mysql://", "sqlite://", "file:///", "wasbs://", "s3://", "gs://")
+_TRUTHY = {"1", "true", "yes", "on", "True", "TRUE", "Y", "y"}
+_FALSY = {"0", "false", "no", "off", "False", "FALSE", "N", "n"}
 def _is_remote_uri(uri: str) -> bool:
@@ -53,12 +56,20 @@ class TrackingDecision:
     details: Dict[str, Any]
 def _offline_posture(env: Mapping[str, str]) -> bool:
     """
     Offline posture if:
-      - MLFLOW_OFFLINE=1, or
-      - WANDB_MODE in {"offline", "disabled"}
+      - MLFLOW_OFFLINE=1, or
+      - W&B offline: WANDB_MODE in {"offline", "disabled"} OR WANDB_DISABLED truthy
+      - Overrides: if WANDB_ENABLE=1, W&B is considered enabled (does not force offline posture)
     """
-    if env.get("MLFLOW_OFFLINE", "").strip() in {"1", "true", "True"}:
+    if env.get("MLFLOW_OFFLINE", "").strip() in _TRUTHY:
         return True
-    mode = (env.get("WANDB_MODE", "") or "").strip().lower()
-    if mode in {"offline", "disabled"}:
+    mode = (env.get("WANDB_MODE", "") or "").strip().lower()
+    wandb_disabled = (env.get("WANDB_DISABLED", "") or "").strip()
+    wandb_enable = (env.get("WANDB_ENABLE", "") or "").strip()
+    if wandb_enable in _TRUTHY:
+        return False
+    if mode in {"offline", "disabled"}:
+        return True
+    if wandb_disabled in _TRUTHY and wandb_disabled not in _FALSY:
         return True
     return False
 def decide_mlflow_tracking_uri(*, environ: Optional[Mapping[str, str]] = None) -> TrackingDecision:
@@ -86,6 +97,9 @@ def decide_mlflow_tracking_uri(*, environ: Optional[Mapping[str, str]] = None) ->
     details: Dict[str, Any] = {
         "ts": time.time(),
         "raw_uri": raw,
         "offline": offline,
         "allow_remote_flag": allow_remote,
-        "env_wandb_mode": env.get("WANDB_MODE"),
+        "env_wandb_mode": env.get("WANDB_MODE"),
+        "env_wandb_disabled": env.get("WANDB_DISABLED"),
+        "env_wandb_enable": env.get("WANDB_ENABLE"),
     }
     return TrackingDecision(uri=uri, blocked=blocked, reason=reason, details=details)
diff --git a/tests/tracking/test_tracking_guard_matrix.py b/tests/tracking/test_tracking_guard_matrix.py
index e74e9e2..9a9d2f4 100644
--- a/tests/tracking/test_tracking_guard_matrix.py
+++ b/tests/tracking/test_tracking_guard_matrix.py
@@ -1,15 +1,17 @@
 import os
 import itertools
 import pytest
 from codex_ml.tracking.guards import decide_mlflow_tracking_uri, normalize_mlflow_uri, DEFAULT_LOCAL_URI, ALLOW_REMOTE_ENV
 LOCAL_PATH = "mlruns_local"
 REMOTE_HTTP = "http://mlflow.example:5000"
 REMOTE_S3 = "s3://bucket/mlruns"
@@ -26,13 +28,17 @@ def test_normalize_mlflow_uri(uri, expected_prefix, tmp_path):
     out = normalize_mlflow_uri(uri)
     assert out.startswith(expected_prefix)
-@pytest.mark.parametrize("remote_uri", [REMOTE_HTTP, REMOTE_S3])
-@pytest.mark.parametrize("mlflow_offline", ["", "0", "1"])
-@pytest.mark.parametrize("wandb_mode", ["", "offline", "disabled"])
-@pytest.mark.parametrize("allow_remote", ["", "1"])
-def test_decide_matrix(remote_uri, mlflow_offline, wandb_mode, allow_remote):
+@pytest.mark.parametrize("remote_uri", [REMOTE_HTTP, REMOTE_S3])
+@pytest.mark.parametrize("mlflow_offline", ["", "0", "1"])
+@pytest.mark.parametrize("wandb_mode", ["", "offline", "disabled"])
+@pytest.mark.parametrize("wandb_disabled", ["", "1"])
+@pytest.mark.parametrize("wandb_enable", ["", "1"])
+@pytest.mark.parametrize("allow_remote", ["", "1"])
+def test_decide_matrix(remote_uri, mlflow_offline, wandb_mode, wandb_disabled, wandb_enable, allow_remote):
     env = {
         "MLFLOW_TRACKING_URI": remote_uri,
         "MLFLOW_OFFLINE": mlflow_offline,
         "WANDB_MODE": wandb_mode,
+        "WANDB_DISABLED": wandb_disabled,
+        "WANDB_ENABLE": wandb_enable,
         ALLOW_REMOTE_ENV: allow_remote,
     }
     d = decide_mlflow_tracking_uri(environ=env)
-    offline = (mlflow_offline in {"1"}) or (wandb_mode in {"offline", "disabled"})
+    offline = (mlflow_offline == "1") or ((wandb_mode in {"offline", "disabled"}) or (wandb_disabled == "1")) and (wandb_enable != "1")
     allowed = allow_remote == "1"
     if offline:
         # Always blocked under offline posture
         assert d.blocked is True
         assert d.uri.startswith("file://")
@@ -49,6 +55,7 @@ def test_local_uri_passes(tmp_path):
     env = {
         "MLFLOW_TRACKING_URI": f"file://{tmp_path.as_posix()}",
     }
     d = decide_mlflow_tracking_uri(environ=env)
     assert d.blocked is False
     assert d.uri.startswith("file://")
diff --git a/src/codex_ml/cli.py b/src/codex_ml/cli.py
index 5d4e2a9..2a8d7b1 100644
--- a/src/codex_ml/cli.py
+++ b/src/codex_ml/cli.py
@@ -9,12 +9,13 @@ Minimal local/offline CLI for _codex_.
 Usage examples:
   python -m codex_ml hash manifest --write-readme
   python -m codex_ml tracking decide --summary tracking_summary.ndjson
 """
 import argparse
 import json
+import hashlib
 import os
 import sys
 from pathlib import Path
 from typing import Dict, Iterable, List, Tuple
 from codex_ml.data.utils import manifest_checksums
@@ -27,12 +28,13 @@ BADGE_END = "<!-- END-CAPABILITY-HASH-BADGE -->"
 def _iter_globs(root: Path, globs: Iterable[str]) -> List[str]:
     out: List[str] = []
     seen = set()
     for pattern in globs:
         for p in root.glob(pattern):
             if p.is_file():
                 k = p.resolve().as_posix()
                 if k not in seen:
                     seen.add(k)
                     out.append(k)
     out.sort()
     return out
@@ -54,8 +56,16 @@ def _update_readme_badge(readme_path: Path, digest: str) -> None:
     """
     Replace or insert the capability hash badge block in README.
     """
-    tmpl = (
-        f"{BADGE_BEGIN}\n"
-        f"Integrity: `{digest}` (SHA256)\n"
-        f"{BADGE_END}\n"
-    )
+    def _tmpl(digest: str, cap_digest: str | None) -> str:
+        lines = [
+            BADGE_BEGIN,
+            f"Integrity: `{digest}` (SHA256)",
+        ]
+        if cap_digest:
+            lines.append(f"Capabilities: `{cap_digest}` (SHA256)")
+        lines.append(BADGE_END)
+        return "\n".join(lines) + "\n"
+
+    # patched later with optional capabilities digest
+    tmpl = _tmpl(digest, None)
     if not readme_path.exists():
         # Create a minimal README with the badge at top
         readme_path.write_text(f"# _codex_\n\n{tmpl}\n", encoding="utf-8")
         return
@@ -73,11 +83,11 @@ def _update_readme_badge(readme_path: Path, digest: str) -> None:
         else:
             new_text = f"{tmpl}\n{new_text}"
     if not new_text.endswith("\n"):
         new_text += "\n"
-    readme_path.write_text(new_text, encoding="utf-8")
+    readme_path.write_text(new_text, encoding="utf-8")
-def _cmd_hash_manifest(args: argparse.Namespace) -> int:
+def _cmd_hash_manifest(args: argparse.Namespace) -> int:
     root = Path(args.root or ".").resolve()
     globs = args.glob or _default_manifest_globs()
     files = _iter_globs(root, globs)
     per, aggregate = manifest_checksums(files)
     payload: Dict[str, object] = {
@@ -87,10 +97,31 @@ def _cmd_hash_manifest(args: argparse.Namespace) -> int:
         "aggregate_sha256": aggregate,
         "count": len(per),
         "schema_version": 1,
         "globs": globs,
     }
+    cap_digest: str | None = None
+    if args.include_detectors:
+        try:
+            from codex_ml.detectors.aggregate import aggregate_scores  # lazy import
+            caps = aggregate_scores(root)
+            # Stable JSON encoding for hashing
+            data = json.dumps(caps, sort_keys=True, separators=(",", ":")).encode("utf-8")
+            cap_digest = hashlib.sha256(data).hexdigest()
+            payload["capabilities"] = caps  # helpful when writing to stdout
+            payload["capabilities_digest"] = cap_digest
+        except Exception as e:
+            payload["capabilities_error"] = str(e)
     # Print JSON to stdout
     print(json.dumps(payload, indent=2))
-    if args.write_readme:
-        _update_readme_badge(root / "README.md", aggregate)
+    if args.write_readme:
+        # Update badge block with optional capabilities digest
+        readme = root / "README.md"
+        # Render with capabilities line by rebuilding inside helper
+        def _render_with_optional(readme_path: Path, digest: str, cap_digest: str | None):
+            text = readme_path.read_text(encoding="utf-8") if readme_path.exists() else ""
+            # construct template and replace (reuse the internal function by re-calling this method with patched tmpl)
+            # Simple strategy: call with digest only first, then append Capabilities line if requested.
+            pass
+        _update_readme_badge(readme, aggregate)
+        if cap_digest:
+            text = readme.read_text(encoding="utf-8")
+            text = text.replace("<!-- END-CAPABILITY-HASH-BADGE -->", f"Capabilities: `{cap_digest}` (SHA256)\n<!-- END-CAPABILITY-HASH-BADGE -->")
+            readme.write_text(text, encoding="utf-8")
     return 0
 def _cmd_tracking_decide(args: argparse.Namespace) -> int:
@@ -130,6 +161,7 @@ def _build_parser() -> argparse.ArgumentParser:
     ap_manifest = sub_hash.add_parser("manifest", help="Compute manifest aggregate hash")
     ap_manifest.add_argument("--root", type=str, default=".", help="Repository root (default: .)")
     ap_manifest.add_argument("--glob", action="append", default=[], help="Glob(s) to include in manifest")
     ap_manifest.add_argument("--write-readme", action="store_true", help="Update README badge with the digest")
+    ap_manifest.add_argument("--include-detectors", action="store_true", help="Include capability digest and show detector summary")
     ap_manifest.set_defaults(func=_cmd_hash_manifest)
     # tracking decide
     ap_track = sub.add_parser("tracking", help="Tracking utilities")
diff --git a/tests/cli/test_cli_manifest.py b/tests/cli/test_cli_manifest.py
index 4b3a6b1..c78c2f3 100644
--- a/tests/cli/test_cli_manifest.py
+++ b/tests/cli/test_cli_manifest.py
@@ -1,11 +1,12 @@
 import json
 import sys
 import subprocess
 from pathlib import Path
 def _badge_present(text: str) -> bool:
     return "<!-- BEGIN-CAPABILITY-HASH-BADGE -->" in text and "<!-- END-CAPABILITY-HASH-BADGE -->" in text
@@ -31,3 +32,29 @@ def test_cli_manifest_hash_and_badge_update(tmp_path: Path):
     data = json.loads(p.stdout)
     assert "aggregate_sha256" in data and len(data["aggregate_sha256"]) == 64
     readme = (tmp_path / "README.md").read_text(encoding="utf-8")
     assert _badge_present(readme)
     assert data["aggregate_sha256"] in readme
+
+
+def test_cli_manifest_with_detectors_digest(tmp_path: Path):
+    # Minimal training file to satisfy detector for a stable digest
+    (tmp_path / "src" / "codex_ml" / "training").mkdir(parents=True, exist_ok=True)
+    (tmp_path / "src" / "codex_ml" / "training" / "unified_training.py").write_text(
+        "class UnifiedTrainingConfig: pass\n"
+        "def run_unified_training(*a, **k): pass\n"
+        "def train_loop(*a, **k): pass\n"
+        "def functional_training(*a, **k): pass\n",
+        encoding="utf-8",
+    )
+    (tmp_path / "README.md").write_text("# Project\n\n", encoding="utf-8")
+    p = subprocess.run(
+        [
+            sys.executable, "-m", "codex_ml", "hash", "manifest",
+            "--root", str(tmp_path),
+            "--include-detectors",
+            "--write-readme",
+        ],
+        capture_output=True,
+        text=True,
+        check=True,
+    )
+    data = json.loads(p.stdout)
+    assert "capabilities_digest" in data and len(data["capabilities_digest"]) == 64
+    readme = (tmp_path / "README.md").read_text(encoding="utf-8")
+    assert "Capabilities: `" in readme
diff --git a/docs/observability.md b/docs/observability.md
index 9a7a0e2..0b6b28c 100644
--- a/docs/observability.md
+++ b/docs/observability.md
@@ -1,26 +1,32 @@
 # Observability (Offline-First)
 We gate outbound tracking by default and emit **structured NDJSON** logs locally.
 ## Tracking guards
 - **MLflow**: We treat remote URIs (e.g., `http://host:5000`, `s3://bucket`) as **remote**. In offline posture or without an explicit allow flag, remote URIs are blocked and rewritten to a local `file://...` run store.
   - MLflow remote behavior is configured via `MLFLOW_TRACKING_URI`. See docs.
   - Our allow-flag is `CODEX_ALLOW_REMOTE_TRACKING=1`.
-- **W&B**: Gate via `WANDB_MODE` set to `offline` (or `disabled`), which prevents runs from syncing to the hosted service.
+- **W&B**:
+  - Offline posture if `WANDB_MODE=offline` or `WANDB_DISABLED=true`.
+  - An explicit `WANDB_ENABLE=1` overrides the offline posture check.
+  - These variables are **advisory** for posture calculation and do not call external services.
 CLI helper:
 ```bash
 python -m codex_ml tracking decide --summary tracking_summary.ndjson --print
````
## Structured logs
Use the JSON logger to write NDJSON:
```python
from codex_ml.logging import json_logger
log = json_logger("codex")
log.info("step", extra={"extra": {"loss": 0.123}})
```
Convert NDJSON to CSV:
```bash
python tools/ndjson_summarize.py my.ndjson --output my.csv
```
diff --git a/docs/manifest_integrity.md b/docs/manifest_integrity.md
index 1db4a0f..21a3d8f 100644
--- a/docs/manifest_integrity.md
+++ b/docs/manifest_integrity.md
@@ -18,11 +18,21 @@ python -m codex_ml hash manifest --write-readme
```
This replaces the contents between the badge markers in `README.md`:
```
 <!-- BEGIN-CAPABILITY-HASH-BADGE -->
Integrity: `<digest>` (SHA256)
+Capabilities: `<cap_digest>` (SHA256)   # present when using --include-detectors
 <!-- END-CAPABILITY-HASH-BADGE -->
````
No network calls are made; everything is computed locally.
+### Include capability digest (optional)
+
+You can also include a **capability score digest** (SHA-256 of the detector aggregate JSON) by passing:
+
+```bash
+python -m codex_ml hash manifest --include-detectors --write-readme
+```
+
## Why NDJSON for audit trails?
We prefer **NDJSON** (one JSON object per line, UTF-8) for logs and metrics because it composes well with
shell tools and avoids partial-object issues with streaming. See `docs/observability.md` for details,
and note our structured logger (`codex_ml.logging.structured.json_logger`) emits exactly one JSON per line.
diff --git a/docs/observability_metrics.md b/docs/observability_metrics.md
new file mode 100644
index 0000000..f5f7d5a
--- /dev/null
+++ b/docs/observability_metrics.md
@@ -0,0 +1,62 @@
+# Observability: Metrics (NDJSON)
+
+We emit **lightweight metrics** as one JSON object per line, suitable for piping into local tools.
+
+## Logger
+
+```python
+from codex_ml.logging import json_logger
+log = json_logger("codex")
+log.info("train/epoch_end", extra={"extra": {"epoch": 0, "loss": 0.123}})
+```
+
+Example NDJSON line:
+```json
+{"ts": 1712345678.9, "name": "codex", "level": "info", "msg": "train/epoch_end", "extra": {"epoch": 0, "loss": 0.123}}
+```
+
+## Perplexity helper
+
+```python
+from codex_ml.metrics import perplexity_from_losses
+ppl = perplexity_from_losses([1.0, 1.25, 0.9])
+```
+
+Perplexity is computed as `exp(mean(loss))`.
+
+## Summarize NDJSON â CSV
+
+```bash
+python tools/ndjson_summarize.py training.ndjson --output training.csv
+```
+
+The CSV uses **dot-notation** to flatten nested keys (e.g., `extra.loss`).
diff --git a/docs/cli.md b/docs/cli.md
new file mode 100644
index 0000000..b6b9e35
--- /dev/null
+++ b/docs/cli.md
@@ -0,0 +1,61 @@
+# CLI (`python -m codex_ml`)
+
+All commands are **local/offline** and safe to run in air-gapped environments.
+
+## Hash manifest
+
+Compute a stable digest of project files and optionally update the README badge:
+
+```bash
+python -m codex_ml hash manifest --write-readme
+```
+
+Include a digest of the **capability score** (detector aggregate):
+
+```bash
+python -m codex_ml hash manifest --include-detectors --write-readme
+```
+
+Output JSON includes:
+```json
+{
+  "aggregate_sha256": "<manifest_digest>",
+  "capabilities_digest": "<detector_digest>",   // when --include-detectors
+  "files": [{"path": "...", "sha256": "..."}, ...],
+  "capabilities": { ... }                       // detector summary when --include-detectors
+}
+```
+
+## Tracking decision (offline posture)
+
+Print a decision (and optionally append to an NDJSON file):
+
+```bash
+python -m codex_ml tracking decide --summary tracking_summary.ndjson --print
+```
+
+Environment variables considered:
+- MLflow: `MLFLOW_OFFLINE`, `MLFLOW_TRACKING_URI`, `CODEX_ALLOW_REMOTE_TRACKING`
+- Weights & Biases: `WANDB_MODE`, `WANDB_DISABLED`, `WANDB_ENABLE`
+
+If remote MLflow URIs are blocked, the decision returns a **local file URI**.
diff --git a/CHANGELOG_CODEX.md b/CHANGELOG_CODEX.md
index 4e8a0de..8c1b9a2 100644
--- a/CHANGELOG_CODEX.md
+++ b/CHANGELOG_CODEX.md
@@ -1,5 +1,33 @@
# Codex Changelog
+## 2025-10-07 â Tracking: W&B env coverage; Manifest: capability digest; Docs cleanup
+
+### WHY
+- Close remaining tracking test coverage by handling `WANDB_DISABLED` and a local override `WANDB_ENABLE=1`.
+- Improve auditability by optionally including a **capability (detector) digest** in the README badge and CLI JSON.
+- Fill in referenced docs for CLI and observability metrics.
+
+### Changes
+- Tracking:
+  - Guards now consider `WANDB_DISABLED` and override `WANDB_ENABLE=1`.
+  - Matrix tests extended with new permutations.
+- CLI:
+  - `hash manifest --include-detectors` computes and prints `capabilities_digest` and adds a second line to the README badge.
+- Docs:
+  - `docs/observability.md` updated with W&B vars.
+  - `docs/manifest_integrity.md` documents the optional capability digest.
+  - Added `docs/observability_metrics.md` and `docs/cli.md`.
+- Tests:
+  - CLI manifest test for capabilities digest and README second line.
+
+### Risk
+- Low. Changes are additive and default-safe; badge line appears only when flag is used.
+
+### Rollback
+- Revert guards/env handling, CLI flag handling, and docs; remove added tests.
+
## 2025-10-07 â Checkpoint compat shims + unified training docs + tokenizer bench
### WHY
- Provide **deprecated** checkpointing wrappers to avoid churn while consolidating on the core API.
- Document the **unified training faÃ§ade**.
````
```diff
diff --git a/src/codex_ml/training/unified_training.py b/src/codex_ml/training/unified_training.py
index 6a1f4a9..a3c59b1 100644
--- a/src/codex_ml/training/unified_training.py
+++ b/src/codex_ml/training/unified_training.py
@@ -1,12 +1,32 @@
 from __future__ import annotations
+from dataclasses import dataclass
+from pathlib import Path
+from typing import Any, Dict, Optional
+
+try:
+    import torch
+    from torch import nn
+except Exception as e:  # pragma: no cover
+    torch = None  # type: ignore
+    nn = None  # type: ignore
+
+from codex_ml.checkpointing import save_checkpoint, load_checkpoint, apply_rng_state
@@
-# ... existing imports and utilities ...
+# ... existing utilities (e.g., _set_seed, _to_device, etc.) ...
-# (existing) UnifiedTrainingConfig dataclass here
+@dataclass
+class UnifiedTrainingConfig:
+    epochs: int = 1
+    lr: float = 1e-3
+    deterministic: bool = True
+    device: str = "cpu"                      # "cpu" or "cuda"
+    checkpoint_dir: Optional[str] = None     # e.g., "runs/ck"
+    resume_from: Optional[str] = None        # e.g., "runs/ck/epoch-0003"
+    keep_last_k: int = 5
+    clip_grad_norm: Optional[float] = None   # threshold for clipping; None disables
+    clip_grad_type: str = "norm"             # "norm" | "value"
+    norm_type: float = 2.0                   # p-norm for norm clipping (e.g., 2.0)
+
+    def __post_init__(self) -> None:
+        # epochs
+        if not isinstance(self.epochs, int) or self.epochs <= 0:
+            raise ValueError("epochs must be a positive integer")
+        # lr
+        if not (isinstance(self.lr, (int, float)) and self.lr > 0):
+            raise ValueError("lr must be > 0")
+        # device
+        if self.device not in {"cpu", "cuda"}:
+            raise ValueError("device must be 'cpu' or 'cuda'")
+        # clip grad
+        if self.clip_grad_norm is not None and self.clip_grad_norm < 0:
+            raise ValueError("clip_grad_norm must be >= 0 when provided")
+        if self.clip_grad_type not in {"norm", "value"}:
+            raise ValueError("clip_grad_type must be 'norm' or 'value'")
+        # norm_type numeric
+        if not isinstance(self.norm_type, (int, float)):
+            raise ValueError("norm_type must be numeric (int or float)")
@@
-def run_unified_training(cfg: UnifiedTrainingConfig, *, model, optimizer, loss_fn, train_loader, val_loader=None, **kwargs):
-    # existing training implementation
-    ...
+def run_unified_training(
+    cfg: UnifiedTrainingConfig,
+    *,
+    model,
+    optimizer,
+    loss_fn,
+    train_loader,
+    val_loader=None,
+    scheduler=None,
+    grad_accum_steps: int = 1,
+    **kwargs: Any,
+):
+    """
+    Unified training faÃ§ade with optional resume and checkpointing.
+    Returns a small history dict: {"loss": [epoch_avg_losses], ...}
+    """
+    if torch is None:
+        raise RuntimeError("PyTorch is required for training")
+
+    device = torch.device(cfg.device if (cfg.device == "cpu" or torch.cuda.is_available()) else "cpu")
+    model.to(device)
+
+    # Determinism posture
+    if cfg.deterministic:
+        try:
+            torch.use_deterministic_algorithms(True, warn_only=True)  # newer PyTorch
+        except Exception:
+            try:
+                torch.use_deterministic_algorithms(True)  # fallback
+            except Exception:
+                pass
+        torch.manual_seed(0)
+        if hasattr(torch, "cuda") and torch.cuda.is_available():
+            torch.cuda.manual_seed_all(0)  # type: ignore[attr-defined]
+
+    # Optional resume
+    start_epoch = 0
+    if cfg.resume_from:
+        state, meta = load_checkpoint(cfg.resume_from, map_location=cfg.device)
+        if "model" in state:
+            model.load_state_dict(state["model"])
+        if "optimizer" in state:
+            optimizer.load_state_dict(state["optimizer"])
+        if "rng" in state:
+            apply_rng_state(state["rng"])
+        # Prefer metadata epoch; otherwise infer from folder name epoch-XXXX
+        if isinstance(meta, dict) and "epoch" in meta:
+            start_epoch = int(meta["epoch"]) + 1
+        else:
+            p = Path(cfg.resume_from)
+            if p.name.startswith("epoch-") and p.name[6:].isdigit():
+                start_epoch = int(p.name.split("-")[1]) + 1
+
+    history: Dict[str, list] = {"loss": []}
+    scaler = kwargs.get("scaler", None)
+    clip_norm = cfg.clip_grad_norm
+    use_norm = (clip_norm is not None) and (cfg.clip_grad_type == "norm")
+    use_val = (clip_norm is not None) and (cfg.clip_grad_type == "value")
+
+    for epoch in range(start_epoch, cfg.epochs):
+        model.train(True)
+        running_loss = 0.0
+        n_batches = 0
+        optimizer.zero_grad(set_to_none=True)
+        for i, batch in enumerate(train_loader):
+            if isinstance(batch, (list, tuple)) and len(batch) == 2:
+                x, y = batch
+            else:
+                x = batch
+                y = None
+            x = x.to(device)
+            y = y.to(device) if y is not None else None
+
+            def _step():
+                out = model(x)
+                loss = loss_fn(out, y) if y is not None else loss_fn(out)
+                return loss
+
+            if scaler is not None:
+                with torch.autocast(device_type="cuda" if device.type == "cuda" else "cpu", enabled=True):
+                    loss = _step()
+                scaler.scale(loss).backward()
+                # gradient clipping
+                try:
+                    if clip_norm is not None:
+                        if use_norm:
+                            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=clip_norm, norm_type=cfg.norm_type)
+                        elif use_val:
+                            torch.nn.utils.clip_grad_value_(model.parameters(), clip_value=clip_norm)
+                except Exception:
+                    pass
+                if (i + 1) % grad_accum_steps == 0:
+                    scaler.step(optimizer)
+                    scaler.update()
+                    optimizer.zero_grad(set_to_none=True)
+            else:
+                loss = _step()
+                loss.backward()
+                try:
+                    if clip_norm is not None:
+                        if use_norm:
+                            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=clip_norm, norm_type=cfg.norm_type)
+                        elif use_val:
+                            torch.nn.utils.clip_grad_value_(model.parameters(), clip_value=clip_norm)
+                except Exception:
+                    pass
+                if (i + 1) % grad_accum_steps == 0:
+                    optimizer.step()
+                    optimizer.zero_grad(set_to_none=True)
+
+            running_loss += float(loss.detach().cpu().item())
+            n_batches += 1
+
+        epoch_loss = running_loss / max(1, n_batches)
+        history["loss"].append(epoch_loss)
+
+        # Optional scheduler
+        if scheduler is not None:
+            try:
+                scheduler.step()
+            except Exception:
+                pass
+
+        # Checkpoint at epoch boundary
+        if cfg.checkpoint_dir:
+            out_dir = Path(cfg.checkpoint_dir) / f"epoch-{epoch:04d}"
+            meta = {"epoch": epoch, "loss": epoch_loss}
+            save_checkpoint(out_dir, state={"model": model.state_dict(), "optimizer": optimizer.state_dict()}, meta=meta, keep_last_k=cfg.keep_last_k)
+
+    return history
@@
 def train_loop(cfg: UnifiedTrainingConfig, *args, **kwargs):
     """
     Deprecated wrapper for old entrypoint name.
     """
     import warnings
     warnings.warn(
         "train_loop is deprecated; use run_unified_training(cfg, ...)",
         category=DeprecationWarning,
         stacklevel=2,
     )
     return run_unified_training(cfg, *args, **kwargs)
@@
 def functional_training(cfg: UnifiedTrainingConfig, *args, **kwargs):
     """
     Deprecated functional entrypoint for training.
     Mirrors `train_loop` and forwards to `run_unified_training`.
     """
     import warnings
     warnings.warn(
         "functional_training is deprecated; use run_unified_training(cfg, ...)",
         category=DeprecationWarning,
         stacklevel=2,
     )
     return run_unified_training(cfg, *args, **kwargs)
 # End of module
diff --git a/tests/training/test_unified_training_parity.py b/tests/training/test_unified_training_parity.py
new file mode 100644
index 0000000..cdbf1ef
--- /dev/null
+++ b/tests/training/test_unified_training_parity.py
@@ -0,0 +1,78 @@
+import math
+import pytest
+
+torch = pytest.importorskip("torch")
+nn = torch.nn
+
+from codex_ml.training.unified_training import UnifiedTrainingConfig, run_unified_training, train_loop
+
+
+def _loader(n=256, batch=32, device="cpu"):
+    x = torch.linspace(-1, 1, steps=n, device=device).reshape(-1, 1)
+    y = 2.5 * x - 0.3
+    ds = torch.utils.data.TensorDataset(x, y)
+    return torch.utils.data.DataLoader(ds, batch_size=batch, shuffle=False)
+
+
+def _model(device="cpu"):
+    return nn.Sequential(nn.Linear(1, 8), nn.ReLU(), nn.Linear(8, 1)).to(device)
+
+
+@pytest.mark.parametrize("epochs", [1, 2])
+def test_parity_unified_vs_legacy_wrapper(epochs):
+    device = "cuda" if torch.cuda.is_available() else "cpu"
+    cfg = UnifiedTrainingConfig(epochs=epochs, lr=1e-2, deterministic=True, device=device)
+    # Unified
+    m1 = _model(device)
+    opt1 = torch.optim.SGD(m1.parameters(), lr=cfg.lr)
+    loss = nn.MSELoss()
+    h1 = run_unified_training(cfg, model=m1, optimizer=opt1, loss_fn=loss, train_loader=_loader(device=device))
+    # Legacy wrapper (forwards)
+    m2 = _model(device)
+    opt2 = torch.optim.SGD(m2.parameters(), lr=cfg.lr)
+    h2 = train_loop(cfg, model=m2, optimizer=opt2, loss_fn=loss, train_loader=_loader(device=device))
+    # Compare epoch losses within a tiny epsilon for deterministic run
+    assert len(h1["loss"]) == len(h2["loss"]) == epochs
+    for a, b in zip(h1["loss"], h2["loss"]):
+        assert math.isclose(a, b, rel_tol=0.0, abs_tol=1e-6)
diff --git a/CHANGELOG_CODEX.md b/CHANGELOG_CODEX.md
index 8c1b9a2..a3b9f31 100644
--- a/CHANGELOG_CODEX.md
+++ b/CHANGELOG_CODEX.md
@@ -1,5 +1,24 @@
 # Codex Changelog
+## 2025-10-07 â Unified training: validations + resume/checkpoint integration + parity test
+
+### WHY
+- Close P1 acceptance criteria by:
+  - adding **config validations** (`__post_init__`),
+  - wiring **resume_from** and **checkpoint_dir** into the training loop,
+  - providing a **deterministic parity test** (legacy wrapper vs faÃ§ade).
+
+### Changes
+- `training/unified_training.py`:
+  - Dataclass validations; resume + checkpoint integration; gradient clipping hooks; deterministic posture.
+- Tests:
+  - `tests/training/test_unified_training_parity.py` asserts â¤1e-6 epoch-loss delta for 1â2 epochs.
+
+### Risk
+- Low. Defaults retain prior behavior; resume/ckpt paths are opt-in.
+
+### Rollback
+- Revert the module changes and remove the new test.
```
````diff
diff --git a/src/codex_ml/logging/__init__.py b/src/codex_ml/logging/__init__.py
new file mode 100644
index 0000000..5c2c1de
--- /dev/null
+++ b/src/codex_ml/logging/__init__.py
@@ -0,0 +1,7 @@
+"""
+Structured logging (NDJSON) public surface.
+"""
+from .structured import json_logger
+
+__all__ = ["json_logger"]
+
diff --git a/src/codex_ml/logging/structured.py b/src/codex_ml/logging/structured.py
new file mode 100644
index 0000000..b2b4f33
--- /dev/null
+++ b/src/codex_ml/logging/structured.py
@@ -0,0 +1,94 @@
+from __future__ import annotations
+import json
+import sys
+import time
+from typing import Any, Dict, IO, Optional
+
+
+class _JsonLogger:
+    __slots__ = ("_name", "_stream")
+
+    def __init__(self, name: str, stream: IO[str]) -> None:
+        self._name = str(name)
+        self._stream = stream
+
+    def _emit(self, level: str, msg: str, *, extra: Optional[Dict[str, Any]] = None) -> None:
+        obj = {
+            "ts": time.time(),
+            "name": self._name,
+            "level": str(level),
+            "msg": str(msg),
+        }
+        if extra:
+            # ensure stability: embed under "extra"
+            obj["extra"] = extra
+        line = json.dumps(obj, ensure_ascii=False, separators=(",", ":"))
+        # always terminate with newline for NDJSON
+        self._stream.write(line + "\n")
+        # do not auto-flush to avoid perf hit; callers may flush if desired
+
+    def info(self, msg: str, *, extra: Optional[Dict[str, Any]] = None) -> None:
+        self._emit("info", msg, extra=extra)
+
+    def debug(self, msg: str, *, extra: Optional[Dict[str, Any]] = None) -> None:
+        self._emit("debug", msg, extra=extra)
+
+    def warning(self, msg: str, *, extra: Optional[Dict[str, Any]] = None) -> None:
+        self._emit("warning", msg, extra=extra)
+
+    def error(self, msg: str, *, extra: Optional[Dict[str, Any]] = None) -> None:
+        self._emit("error", msg, extra=extra)
+
+
+def json_logger(name: str, *, stream: Optional[IO[str]] = None) -> _JsonLogger:
+    """
+    Create a tiny NDJSON logger writing one JSON object per line.
+    """
+    return _JsonLogger(name, stream or sys.stdout)
diff --git a/tests/logging/test_json_logger.py b/tests/logging/test_json_logger.py
new file mode 100644
index 0000000..b71d9c6
--- /dev/null
+++ b/tests/logging/test_json_logger.py
@@ -0,0 +1,38 @@
+import io
+import json
+from codex_ml.logging import json_logger
+
+
+def test_json_logger_emits_one_line_ndjson():
+    buf = io.StringIO()
+    log = json_logger("codex", stream=buf)
+    log.info("train/epoch_end", extra={"epoch": 0, "loss": 0.123})
+    data = buf.getvalue().splitlines()
+    assert len(data) == 1
+    obj = json.loads(data[0])
+    assert obj["name"] == "codex"
+    assert obj["level"] == "info"
+    assert obj["msg"] == "train/epoch_end"
+    assert obj["extra"]["epoch"] == 0
+    assert obj["extra"]["loss"] == 0.123
+
+
+def test_json_logger_unicode_and_no_extra():
+    buf = io.StringIO()
+    log = json_logger("æ¸¬è©¦", stream=buf)
+    log.error("Î±Î²Î³")
+    obj = json.loads(buf.getvalue())
+    assert obj["name"] == "æ¸¬è©¦"
+    assert obj["msg"] == "Î±Î²Î³"
+    assert "extra" not in obj
diff --git a/src/codex_ml/metrics/__init__.py b/src/codex_ml/metrics/__init__.py
new file mode 100644
index 0000000..3243a50
--- /dev/null
+++ b/src/codex_ml/metrics/__init__.py
@@ -0,0 +1,6 @@
+"""
+Metrics helpers.
+"""
+from .text import perplexity_from_losses
+
+__all__ = ["perplexity_from_losses"]
diff --git a/src/codex_ml/metrics/text.py b/src/codex_ml/metrics/text.py
new file mode 100644
index 0000000..e1b1f89
--- /dev/null
+++ b/src/codex_ml/metrics/text.py
@@ -0,0 +1,22 @@
+from __future__ import annotations
+import math
+from statistics import fmean
+from typing import Iterable
+
+
+def perplexity_from_losses(losses: Iterable[float]) -> float:
+    """
+    Compute perplexity from a sequence of per-token (or per-step) losses.
+    Defined as exp(mean(loss)).
+    """
+    # convert to list once to allow multiple passes in callers if needed
+    vals = [float(x) for x in losses]
+    if not vals:
+        raise ValueError("losses must be non-empty")
+    return float(math.exp(fmean(vals)))
diff --git a/tests/metrics/test_perplexity.py b/tests/metrics/test_perplexity.py
new file mode 100644
index 0000000..cde9a2a
--- /dev/null
+++ b/tests/metrics/test_perplexity.py
@@ -0,0 +1,16 @@
+import math
+import pytest
+from codex_ml.metrics import perplexity_from_losses
+
+
+def test_perplexity_basic():
+    v = perplexity_from_losses([0.0])
+    assert v == 1.0
+    v2 = perplexity_from_losses([math.log(2.0)])
+    assert pytest.approx(v2, rel=0, abs=1e-12) == 2.0
+
+
+def test_perplexity_nonempty():
+    with pytest.raises(ValueError):
+        perplexity_from_losses([])
+
diff --git a/tools/apply_ml_metrics.py b/tools/apply_ml_metrics.py
new file mode 100755
index 0000000..a8a75f2
--- /dev/null
+++ b/tools/apply_ml_metrics.py
@@ -0,0 +1,36 @@
+#!/usr/bin/env python3
+from __future__ import annotations
+"""
+Deprecated shim for metrics helpers.
+
+Redirects to `codex_ml.metrics.perplexity_from_losses`.
+"""
+import warnings
+from typing import Iterable
+from codex_ml.metrics import perplexity_from_losses
+
+
+def perplexity(losses: Iterable[float]) -> float:
+    warnings.warn(
+        "tools.apply_ml_metrics.perplexity is deprecated; use codex_ml.metrics.perplexity_from_losses",
+        category=DeprecationWarning,
+        stacklevel=2,
+    )
+    return perplexity_from_losses(losses)
+
+
+if __name__ == "__main__":
+    import sys, json
+    # Read losses from stdin as JSON list for convenience
+    try:
+        losses = json.loads(sys.stdin.read())
+    except Exception:
+        print("[]")
+        raise SystemExit(2)
+    print(perplexity(losses))
diff --git a/src/codex_ml/safety/__init__.py b/src/codex_ml/safety/__init__.py
new file mode 100644
index 0000000..8f7a83a
--- /dev/null
+++ b/src/codex_ml/safety/__init__.py
@@ -0,0 +1,6 @@
+"""
+Safety utilities public surface.
+"""
+from .filters import sanitize_text
+
+__all__ = ["sanitize_text"]
diff --git a/src/codex_ml/safety/filters.py b/src/codex_ml/safety/filters.py
new file mode 100644
index 0000000..b1f2a9c
--- /dev/null
+++ b/src/codex_ml/safety/filters.py
@@ -0,0 +1,63 @@
+from __future__ import annotations
+import unicodedata
+from typing import Iterable
+
+_ZERO_WIDTH = {
+    "\u200b",  # ZERO WIDTH SPACE
+    "\u200c",  # ZERO WIDTH NON-JOINER
+    "\u200d",  # ZERO WIDTH JOINER
+    "\ufeff",  # ZERO WIDTH NO-BREAK SPACE (BOM)
+}
+
+
+def _is_control(ch: str) -> bool:
+    # Control characters are category Cc; allow \n and \t explicitly
+    if ch in ("\n", "\t"):
+        return False
+    return unicodedata.category(ch) == "Cc"
+
+
+def sanitize_text(text: str) -> str:
+    """
+    Normalize and strip problematic characters for log-safety:
+      - Normalize to NFKC
+      - Remove control characters (except LF and TAB)
+      - Remove common zero-width characters (ZWSP, ZWNJ, ZWJ, BOM)
+      - Normalize line endings to LF
+    """
+    if not isinstance(text, str):
+        text = str(text)
+    s = text.replace("\r\n", "\n").replace("\r", "\n")
+    s = unicodedata.normalize("NFKC", s)
+    out = []
+    for ch in s:
+        if ch in _ZERO_WIDTH:
+            continue
+        if _is_control(ch):
+            continue
+        out.append(ch)
+    return "".join(out)
diff --git a/src/codex_ml/safety/sanitizers.py b/src/codex_ml/safety/sanitizers.py
new file mode 100644
index 0000000..d7c3c1c
--- /dev/null
+++ b/src/codex_ml/safety/sanitizers.py
@@ -0,0 +1,21 @@
+from __future__ import annotations
+"""
+Deprecated high-level sanitizers; forward to canonical filters.
+"""
+import warnings
+from .filters import sanitize_text as _sanitize_text
+
+
+def sanitize_string(s: str) -> str:
+    warnings.warn(
+        "codex_ml.safety.sanitizers.sanitize_string is deprecated; use codex_ml.safety.sanitize_text",
+        category=DeprecationWarning,
+        stacklevel=2,
+    )
+    return _sanitize_text(s)
diff --git a/tests/safety/test_safety_filters.py b/tests/safety/test_safety_filters.py
new file mode 100644
index 0000000..e4adf5f
--- /dev/null
+++ b/tests/safety/test_safety_filters.py
@@ -0,0 +1,34 @@
+import warnings
+from codex_ml.safety import sanitize_text
+from codex_ml.safety.sanitizers import sanitize_string
+
+
+def test_sanitize_text_basic_controls_and_zero_width():
+    s = "A\u200bB\u200cC\u200dD\ufeffE\nF\tG\x00H"
+    out = sanitize_text(s)
+    # zero-width removed, NUL removed, LF and TAB preserved
+    assert out == "ABCDEFG\nF\tG"  # 'E' kept; NUL dropped; zero-width removed
+
+
+def test_sanitizers_deprecation_wrapper():
+    with warnings.catch_warnings(record=True) as rec:
+        warnings.simplefilter("always", DeprecationWarning)
+        out = sanitize_string("hi\u200b")
+        assert out == "hi"
+        assert any(isinstance(w.message, DeprecationWarning) for w in rec)
diff --git a/docs/safety_api.md b/docs/safety_api.md
new file mode 100644
index 0000000..8a1f4a7
--- /dev/null
+++ b/docs/safety_api.md
@@ -0,0 +1,62 @@
+# Safety API
+
+Utilities for **text sanitization** before logging or persistence.
+
+## Canonical function
+
+```python
+from codex_ml.safety import sanitize_text
+clean = sanitize_text(raw_text)
+```
+
+### Behavior
+- Normalizes to **NFKC**.
+- Converts CR/CRLF to **LF**.
+- Removes control characters (Unicode category `Cc`) **except** `\\n` and `\\t`.
+- Removes common zero-width characters: ZWSP (`\\u200B`), ZWNJ (`\\u200C`), ZWJ (`\\u200D`), and BOM (`\\uFEFF`).
+
+## Deprecated shims
+
+```python
+from codex_ml.safety.sanitizers import sanitize_string  # emits DeprecationWarning
+```
+
+Use the canonical `sanitize_text` going forward.
+
+## Rationale
+Sanitizing log text prevents rendering issues, invisible control characters, and copy/paste traps in terminals and dashboards. The normalization choices here are conservative and deterministic.
diff --git a/docs/index.md b/docs/index.md
index 1f3d28e..a5a0c7b 100644
--- a/docs/index.md
+++ b/docs/index.md
@@ -3,10 +3,12 @@
 - **Unified Training**: `docs/unified_training.md`
 - **Tokenizer Benchmark**: `tools/bench_tokenizer.py`
 - **Checkpoint Core**: `docs/checkpointing_core.md`
 - **Tracking & Observability**: `docs/observability.md`, `docs/observability_metrics.md`
 - **Tokenization API**: `docs/tokenization_api.md`
 - **Data Determinism**: `docs/data_determinism.md`
+- **Safety API**: `docs/safety_api.md`
+- **Metrics**: `docs/observability_metrics.md`
 - **Quality Gates**: `docs/quality_gates.md`
 - **CLI**: `docs/cli.md`
 - **Releasing**: `docs/releasing.md`
 - **Detectors**: `docs/detectors.md`
 - **Security / Secrets**: `docs/secrets.md`
diff --git a/README.md b/README.md
index 2e2c6a0..0c8a6e3 100644
--- a/README.md
+++ b/README.md
@@ -60,6 +60,11 @@ See `docs/cli.md` for more.
 - Unified training throughput: `python tools/bench_unified_training.py`
 - Markdown fence validator (pre-commit hook): `python tools/validate_fences.py --paths README.md docs`
 See `docs/unified_training.md` and `docs/tokenization_api.md` for details.
+## Safety & Metrics
+- **Safety API**: `codex_ml.safety.sanitize_text` removes control/format characters after normalization. See `docs/safety_api.md`.
+- **Metrics**:
+  - Canonical: `codex_ml.metrics.perplexity_from_losses`.
+  - Deprecated shim: `tools/apply_ml_metrics.py::perplexity` (emits `DeprecationWarning`).
diff --git a/CHANGELOG_CODEX.md b/CHANGELOG_CODEX.md
index a3b9f31..a7a55f6 100644
--- a/CHANGELOG_CODEX.md
+++ b/CHANGELOG_CODEX.md
@@ -1,5 +1,33 @@
 # Codex Changelog
+## 2025-10-07 â Structured logging (NDJSON), Metrics helper, Safety API + shims
+
+### WHY
+- Close **observability** references with a minimal **NDJSON logger** usable offline.
+- Provide a canonical **metrics** entrypoint (`perplexity_from_losses`) and a deprecated CLI-friendly shim.
+- Unify **safety** sanitization under one API with deprecation wrappers to reduce duplication.
+
+### Changes
+- Logging:
+  - `src/codex_ml/logging/structured.py` + surface in `__init__.py`.
+  - Tests for one-line NDJSON emission.
+- Metrics:
+  - `src/codex_ml/metrics/text.py` + surface in `__init__.py`.
+  - Tests for perplexity and empty input.
+  - Deprecated shim `tools/apply_ml_metrics.py` (warning).
+- Safety:
+  - `src/codex_ml/safety/filters.py` + surface in `__init__.py`.
+  - Deprecated shim `src/codex_ml/safety/sanitizers.py` (warning).
+  - Tests for sanitization and deprecation.
+- Docs:
+  - `docs/safety_api.md`; README/index updates.
+
+### Risk
+- Low. Additive modules and conservative defaults; shims only warn and forward.
+
+### Rollback
+- Remove the new modules/tests/docs and revert README/docs links.
+
 ## 2025-10-07 â Unified training: validations + resume/checkpoint integration + parity test
 ### WHY
 - Close P1 acceptance criteria by:
 - adding **config validations** (`__post_init__`),
````
````diff
diff --git a/src/codex_ml/cli.py b/src/codex_ml/cli.py
index 2a8d7b1..b7a12a4 100644
--- a/src/codex_ml/cli.py
+++ b/src/codex_ml/cli.py
@@ -1,6 +1,6 @@
 """Minimal local/offline CLI for _codex_.
 Usage examples:
-  python -m codex_ml hash manifest --write-readme
+  python -m codex_ml hash manifest --write-readme [--include-detectors]
   python -m codex_ml tracking decide --summary tracking_summary.ndjson
 """
 import argparse
@@ -23,7 +23,7 @@ BADGE_BEGIN = "<!-- BEGIN-CAPABILITY-HASH-BADGE -->"
 BADGE_END = "<!-- END-CAPABILITY-HASH-BADGE -->"
-def _iter_globs(root: Path, globs: Iterable[str]) -> List[str]:
+def _iter_globs(root: Path, globs: Iterable[str]) -> List[str]:
     out: List[str] = []
     seen = set()
     for pattern in globs:
         for p in root.glob(pattern):
             if p.is_file():
@@ -39,27 +39,32 @@ def _default_manifest_globs() -> List[str]:
     ]
-def _update_readme_badge(readme_path: Path, digest: str) -> None:
+def _render_badge(digest: str, cap_digest: str | None) -> str:
+    lines = [
+        BADGE_BEGIN,
+        f"Integrity: `{digest}` (SHA256)",
+    ]
+    if cap_digest:
+        lines.append(f"Capabilities: `{cap_digest}` (SHA256)")
+    lines.append(BADGE_END)
+    return "\n".join(lines) + "\n"
+
+
+def _update_readme_badge(readme_path: Path, digest: str, *, cap_digest: str | None = None) -> None:
     """
     Replace or insert the capability hash badge block in README.
     """
-    def _tmpl(digest: str, cap_digest: str | None) -> str:
-        lines = [
-            BADGE_BEGIN,
-            f"Integrity: `{digest}` (SHA256)",
-        ]
-        if cap_digest:
-            lines.append(f"Capabilities: `{cap_digest}` (SHA256)")
-        lines.append(BADGE_END)
-        return "\n".join(lines) + "\n"
-
-    # patched later with optional capabilities digest
-    tmpl = _tmpl(digest, None)
+    tmpl = _render_badge(digest, cap_digest)
     if not readme_path.exists():
         # Create a minimal README with the badge at top
         readme_path.write_text(f"# _codex_\n\n{tmpl}\n", encoding="utf-8")
         return
     old = readme_path.read_text(encoding="utf-8")
     if BADGE_BEGIN in old and BADGE_END in old:
         before, _, rest = old.partition(BADGE_BEGIN)
         _, _, after = rest.partition(BADGE_END)
         new_text = f"{before}{tmpl}{after}"
     else:
         # Insert near top if no previous block
         new_text = old
         if not new_text.startswith("#"):
             new_text = "# _codex_\n\n" + new_text
         if not new_text.endswith("\n"):
             new_text += "\n"
-        new_text = f"{tmpl}\n{new_text}"
+        new_text = f"{tmpl}\n{new_text}"
     if not new_text.endswith("\n"):
         new_text += "\n"
     readme_path.write_text(new_text, encoding="utf-8")
@@ -88,27 +93,19 @@ def _cmd_hash_manifest(args: argparse.Namespace) -> int:
         "count": len(per),
         "schema_version": 1,
         "globs": globs,
     }
-    cap_digest: str | None = None
+    cap_digest: str | None = None
     if args.include_detectors:
         try:
             from codex_ml.detectors.aggregate import aggregate_scores  # lazy import
             caps = aggregate_scores(root)
             # Stable JSON encoding for hashing
             data = json.dumps(caps, sort_keys=True, separators=(",", ":")).encode("utf-8")
             cap_digest = hashlib.sha256(data).hexdigest()
             payload["capabilities"] = caps  # helpful when writing to stdout
             payload["capabilities_digest"] = cap_digest
         except Exception as e:
             payload["capabilities_error"] = str(e)
     # Print JSON to stdout
     print(json.dumps(payload, indent=2))
     if args.write_readme:
-        # Update badge block with optional capabilities digest
-        readme = root / "README.md"
-        # Render with capabilities line by rebuilding inside helper
-        def _render_with_optional(readme_path: Path, digest: str, cap_digest: str | None):
-            text = readme_path.read_text(encoding="utf-8") if readme_path.exists() else ""
-            # construct template and replace (reuse the internal function by re-calling this method with patched tmpl)
-            # Simple strategy: call with digest only first, then append Capabilities line if requested.
-            pass
-        _update_readme_badge(readme, aggregate)
-        if cap_digest:
-            text = readme.read_text(encoding="utf-8")
-            text = text.replace("<!-- END-CAPABILITY-HASH-BADGE -->", f"Capabilities: `{cap_digest}` (SHA256)\n<!-- END-CAPABILITY-HASH-BADGE -->")
-            readme.write_text(text, encoding="utf-8")
+        _update_readme_badge(root / "README.md", aggregate, cap_digest=cap_digest)
     return 0
 def _cmd_tracking_decide(args: argparse.Namespace) -> int:
diff --git a/tests/cli/test_cli_manifest.py b/tests/cli/test_cli_manifest.py
index c78c2f3..d7f2a44 100644
--- a/tests/cli/test_cli_manifest.py
+++ b/tests/cli/test_cli_manifest.py
@@ -46,6 +46,7 @@ def test_cli_manifest_with_detectors_digest(tmp_path: Path):
     data = json.loads(p.stdout)
     assert "capabilities_digest" in data and len(data["capabilities_digest"]) == 64
     readme = (tmp_path / "README.md").read_text(encoding="utf-8")
     assert "Capabilities: `" in readme
+    assert data["capabilities_digest"] in readme
diff --git a/tests/training/test_deprecation_warnings.py b/tests/training/test_deprecation_warnings.py
new file mode 100644
index 0000000..8b2a8a1
--- /dev/null
+++ b/tests/training/test_deprecation_warnings.py
@@ -0,0 +1,52 @@
+import warnings
+import pytest
+
+torch = pytest.importorskip("torch")
+nn = torch.nn
+
+from codex_ml.training.unified_training import UnifiedTrainingConfig, train_loop, functional_training, run_unified_training
+
+
+def _loader(n=64, batch=16, device="cpu"):
+    x = torch.linspace(-1, 1, steps=n, device=device).reshape(-1, 1)
+    y = 1.2 * x + 0.4
+    ds = torch.utils.data.TensorDataset(x, y)
+    return torch.utils.data.DataLoader(ds, batch_size=batch, shuffle=False)
+
+
+def _model(device="cpu"):
+    return nn.Sequential(nn.Linear(1, 4), nn.ReLU(), nn.Linear(4, 1)).to(device)
+
+
+def test_train_loop_emits_deprecation():
+    device = "cuda" if torch.cuda.is_available() else "cpu"
+    cfg = UnifiedTrainingConfig(epochs=1, lr=1e-2, deterministic=True, device=device)
+    model = _model(device)
+    opt = torch.optim.SGD(model.parameters(), lr=cfg.lr)
+    loss = nn.MSELoss()
+    with warnings.catch_warnings(record=True) as rec:
+        warnings.simplefilter("always", DeprecationWarning)
+        train_loop(cfg, model=model, optimizer=opt, loss_fn=loss, train_loader=_loader(device=device))
+        assert any(isinstance(w.message, DeprecationWarning) for w in rec)
+
+
+def test_functional_training_emits_deprecation():
+    device = "cuda" if torch.cuda.is_available() else "cpu"
+    cfg = UnifiedTrainingConfig(epochs=1, lr=1e-2, deterministic=True, device=device)
+    model = _model(device)
+    opt = torch.optim.SGD(model.parameters(), lr=cfg.lr)
+    loss = nn.MSELoss()
+    with warnings.catch_warnings(record=True) as rec:
+        warnings.simplefilter("always", DeprecationWarning)
+        functional_training(cfg, model=model, optimizer=opt, loss_fn=loss, train_loader=_loader(device=device))
+        assert any(isinstance(w.message, DeprecationWarning) for w in rec)
diff --git a/tests/tokenization/test_deprecation_snapshot.py b/tests/tokenization/test_deprecation_snapshot.py
new file mode 100644
index 0000000..2c1b3c0
--- /dev/null
+++ b/tests/tokenization/test_deprecation_snapshot.py
@@ -0,0 +1,22 @@
+import importlib
+import sys
+import warnings
+
+
+def test_tokenization_shim_warns_on_import():
+    # ensure a fresh import
+    sys.modules.pop("tokenization", None)
+    with warnings.catch_warnings(record=True) as rec:
+        warnings.simplefilter("always", DeprecationWarning)
+        import tokenization  # noqa: F401
+    assert any(isinstance(w.message, DeprecationWarning) for w in rec)
diff --git a/docs/detectors.md b/docs/detectors.md
new file mode 100644
index 0000000..3c0d6a2
--- /dev/null
+++ b/docs/detectors.md
@@ -0,0 +1,72 @@
+# Capability Detectors
+
+Detectors scan the local source tree to infer the presence and **richness** of features (e.g., unified training).
+
+## Unified training detector
+
+```python
+from codex_ml.detectors.unified_training import detect_unified_training
+res = detect_unified_training(".")
+print(res["score"], res["signals"])
+```
+
+Signals:
+- `config`: `class UnifiedTrainingConfig`
+- `runner`: `def run_unified_training`
+- `wrappers`: legacy wrappers present
+- `resume_hooks`: mentions of `resume_from`, `checkpoint_dir`, or `save_checkpoint`
+
+Heuristic score (0.0â1.0):
+- Base 0.60 if both `config` and `runner`.
+- +0.10 if wrappers present.
+- +0.10 if resume hooks present.
+- If only one of {config, runner} exists: base 0.30.
+
+## Aggregation
+
+Detectors are aggregated and can be hashed to a **capability digest** via:
+
+```bash
+python -m codex_ml hash manifest --include-detectors --write-readme
+```
+
+This appends a second line to the README badge:
+
+```
+<!-- BEGIN-CAPABILITY-HASH-BADGE -->
+Integrity: `<digest>` (SHA256)
+Capabilities: `<cap_digest>` (SHA256)
+<!-- END-CAPABILITY-HASH-BADGE -->
+```
+
+No network access is required; everything runs locally.
diff --git a/docs/releasing.md b/docs/releasing.md
new file mode 100644
index 0000000..e9b4e3d
--- /dev/null
+++ b/docs/releasing.md
@@ -0,0 +1,82 @@
+# Releasing (_local-only workflow_)
+
+This project favors **small, reviewable diffs** with a clear WHY/Risk/Rollback section in the PR and `CHANGELOG_CODEX.md`.
+
+## Checklist
+1. Ensure pre-commit hooks pass:
+   ```bash
+   pre-commit run --all-files
+   ```
+2. Run tests locally:
+   ```bash
+   pytest -q
+   ```
+3. Update the changelog (`CHANGELOG_CODEX.md`) with a concise entry:
+   - WHY / Changes / Risk / Rollback.
+4. Refresh manifest badge (optional capability digest):
+   ```bash
+   python -m codex_ml hash manifest --include-detectors --write-readme
+   ```
+5. Tag a release:
+   ```bash
+   git tag -a vX.Y.Z -m "Release vX.Y.Z"
+   git push --tags
+   ```
+
+## Notes
+- **No CI activation**: keep operations local and offline-friendly.
+- **Determinism**: use the deterministic smoke tests and data invariants to catch regressions early.
+- **Docs**: add or adjust one-liners in relevant docs when introducing new public surfaces.
diff --git a/docs/tests_overview.md b/docs/tests_overview.md
new file mode 100644
index 0000000..5d9a3d0
--- /dev/null
+++ b/docs/tests_overview.md
@@ -0,0 +1,68 @@
+# Tests Overview
+
+This suite emphasizes **determinism** and **offline** operation.
+
+## Areas
+- **Training**:
+  - Parity tests ensure the legacy wrappers produce identical loss trajectories to the unified faÃ§ade for 1â2 epochs.
+  - Resume tests verify optimizer state and RNG continuity.
+- **Checkpointing**:
+  - Golden schema presence; digest validation; retention of last _K_ checkpoints.
+- **Tracking**:
+  - Guard matrix for MLflow/W&B environment permutations (offline posture and allow flag).
+- **Data**:
+  - Manifest checksum stability, seed-based shuffling, and shard coverage.
+- **Tokenization**:
+  - Canonical API roundtrip; deprecation shim warnings.
+- **Safety**:
+  - Text sanitization and shim deprecation.
+- **Logging/Metrics**:
+  - One-line NDJSON emission; perplexity helper.
+
+## Running
+```bash
+pytest -q
+```
+
+Run with visible warnings to catch deprecations:
+```bash
+PYTHONWARNINGS=default pytest -q
+```
diff --git a/docs/secrets.md b/docs/secrets.md
new file mode 100644
index 0000000..0d6f0e1
--- /dev/null
+++ b/docs/secrets.md
@@ -0,0 +1,60 @@
+# Secrets & Safety (Local)
+
+This repository is designed to be **air-gapped friendly**.
+
+## Scanning
+
+Run the local secret scan (wired into pre-commit):
+
+```bash
+python tools/scan_secrets.py --fail-on-find
+```
+
+This scans tracked files for patterns resembling API keys and tokens. It does not contact any external service.
+
+## Environment
+
+- Prefer ephemeral per-run environment files (ignored by VCS).
+- Never commit API tokens; for local experimentation set variables in your shell session only.
+
+## Logging redaction
+
+Combine `codex_ml.safety.sanitize_text` with structured logs to avoid storing invisible or control characters.
+
+## Tracking posture
+
+See `docs/observability.md` for offline-first tracking guards for MLflow and Weights & Biases.
diff --git a/docs/ops.md b/docs/ops.md
new file mode 100644
index 0000000..b7c7a1a
--- /dev/null
+++ b/docs/ops.md
@@ -0,0 +1,70 @@
+# Operational Notes
+
+Minimal runbooks and pointers for local-only operation.
+
+## Incident response
+See `docs/incident_runbook.md` for a short checklist to capture context, reproduce, and rollback safely.
+
+## Deterministic posture
+- Enable deterministic algorithms in PyTorch via the unified training config.
+- Validate data invariants with the data tests and manifest checksum utilities.
+
+## Tracking posture
+- Use `python -m codex_ml tracking decide --print` to audit the current environment.
+- Remote URIs are blocked unless `CODEX_ALLOW_REMOTE_TRACKING=1` is present.
+
+## Benchmarks
+- Tokenizer: `python tools/bench_tokenizer.py`
+- Unified training: `python tools/bench_unified_training.py`
+
+## Housekeeping
+- Keep PRs small and focused (WHY/Risk/Rollback).
+- Update `CHANGELOG_CODEX.md` with each significant change set.
diff --git a/CONTRIBUTING.md b/CONTRIBUTING.md
new file mode 100644
index 0000000..0f0e1aa
--- /dev/null
+++ b/CONTRIBUTING.md
@@ -0,0 +1,86 @@
+# Contributing
+
+Thanks for helping keep this project clean, deterministic, and offline-friendly.
+
+## Development quickstart
+
+```bash
+python -m venv .venv
+source .venv/bin/activate
+pip install -e ".[dev]"  # if a setup exists; otherwise install requirements as needed
+pre-commit install
+```
+
+Run tests:
+```bash
+pytest -q
+```
+
+## Commit discipline
+- Keep diffs small and cohesive.
+- Include **WHY / Risk / Rollback** in your PR description.
+- Update `CHANGELOG_CODEX.md`.
+
+## Quality gates (local)
+- Secret scan and fence validation run via pre-commit.
+- Determinism smoke tests guard training parity and resume semantics.
+
+## Docs
+- Add or update the relevant page in `docs/` when introducing public surfaces.
+- Keep examples minimal and local (no internet dependencies).
diff --git a/docs/index.md b/docs/index.md
index a5a0c7b..8cd8c46 100644
--- a/docs/index.md
+++ b/docs/index.md
@@ -1,15 +1,17 @@
 # Documentation Index
 - **Unified Training**: `docs/unified_training.md`
 - **Tokenizer Benchmark**: `tools/bench_tokenizer.py`
 - **Checkpoint Core**: `docs/checkpointing_core.md`
 - **Tracking & Observability**: `docs/observability.md`, `docs/observability_metrics.md`
 - **Tokenization API**: `docs/tokenization_api.md`
 - **Data Determinism**: `docs/data_determinism.md`
 - **Safety API**: `docs/safety_api.md`
 - **Metrics**: `docs/observability_metrics.md`
+- **Detectors**: `docs/detectors.md`
 - **Quality Gates**: `docs/quality_gates.md`
 - **CLI**: `docs/cli.md`
 - **Releasing**: `docs/releasing.md`
-- **Detectors**: `docs/detectors.md`
 - **Security / Secrets**: `docs/secrets.md`
 - **Incident Runbook**: `docs/incident_runbook.md`
 - **Tests Overview**: `docs/tests_overview.md`
+- **Ops Notes**: `docs/ops.md`
diff --git a/CHANGELOG_CODEX.md b/CHANGELOG_CODEX.md
index a7a55f6..b8c9f40 100644
--- a/CHANGELOG_CODEX.md
+++ b/CHANGELOG_CODEX.md
@@ -1,5 +1,33 @@
 # Codex Changelog
+## 2025-10-07 â Docs polish + deprecation tests + CLI badge cleanup
+
+### WHY
+- Finish documentation coverage (detectors, releasing, tests overview, ops, secrets).
+- Add explicit **deprecation warning tests** for legacy training wrappers and the tokenization shim.
+- Simplify CLI README badge update with an optional capability digest parameter (no string hacks).
+
+### Changes
+- CLI:
+  - `_update_readme_badge(..., cap_digest=...)` and clean rendering of the double-line badge.
+- Tests:
+  - `tests/training/test_deprecation_warnings.py`
+  - `tests/tokenization/test_deprecation_snapshot.py`
+- Docs:
+  - `docs/detectors.md`, `docs/releasing.md`, `docs/tests_overview.md`, `docs/ops.md`, `docs/secrets.md`
+  - `docs/index.md` updated.
+- Meta:
+  - `CONTRIBUTING.md` added with local dev guidelines and quality gates.
+
+### Risk
+- Low. Purely additive tests/docs; CLI change is backward-compatible.
+
+### Rollback
+- Remove new test/doc files and restore prior CLI badge behavior if needed.
+
 ## 2025-10-07 â Structured logging (NDJSON), Metrics helper, Safety API + shims
 ### WHY
 - Close **observability** references with a minimal **NDJSON logger** usable offline.
 - Provide a canonical **metrics** entrypoint (`perplexity_from_losses`) and a deprecated CLI-friendly shim.
````
```diff
diff --git a/src/codex_ml/__init__.py b/src/codex_ml/__init__.py
new file mode 100644
index 0000000..9b0b3a1
--- /dev/null
+++ b/src/codex_ml/__init__.py
@@ -0,0 +1,10 @@
+"""
+codex_ml package
+"""
+__all__ = []
+__version__ = "0.0.1"
+
+# Expose a minimal CLI entry (import side-effect free)
+try:  # pragma: no cover
+    from . import cli as _cli  # noqa: F401
+except Exception:
+    pass
diff --git a/src/codex_ml/__main__.py b/src/codex_ml/__main__.py
new file mode 100644
index 0000000..b2d13f1
--- /dev/null
+++ b/src/codex_ml/__main__.py
@@ -0,0 +1,19 @@
+from __future__ import annotations
+"""
+Module entrypoint: `python -m codex_ml`
+"""
+import sys
+
+from .cli import main
+
+
+def _run() -> int:
+    try:
+        return int(main() or 0)
+    except SystemExit as e:  # pragma: no cover
+        return int(getattr(e, "code", 1) or 0)
+
+
+if __name__ == "__main__":
+    code = _run()
+    sys.exit(code)
diff --git a/src/codex_ml/detectors/__init__.py b/src/codex_ml/detectors/__init__.py
new file mode 100644
index 0000000..f7a3f22
--- /dev/null
+++ b/src/codex_ml/detectors/__init__.py
@@ -0,0 +1,9 @@
+"""
+Capability detectors.
+"""
+from .unified_training import detect_unified_training  # re-export
+from .aggregate import aggregate_scores
+
+__all__ = [
+    "detect_unified_training",
+    "aggregate_scores",
+]
diff --git a/src/codex_ml/detectors/aggregate.py b/src/codex_ml/detectors/aggregate.py
new file mode 100644
index 0000000..3a4c5e7
--- /dev/null
+++ b/src/codex_ml/detectors/aggregate.py
@@ -0,0 +1,80 @@
+from __future__ import annotations
+"""
+Aggregate capability detectors into a single, stable structure.
+
+This module is intentionally offline-only and reads local source files.
+"""
+from dataclasses import asdict, dataclass
+from pathlib import Path
+from typing import Any, Dict, Tuple
+
+from .unified_training import detect_unified_training
+
+
+@dataclass
+class CapabilityScore:
+    score: float
+    signals: Dict[str, Any]
+    path: str | None = None
+
+    @classmethod
+    def from_detector(cls, obj: Dict[str, Any]) -> "CapabilityScore":
+        return cls(
+            score=float(obj.get("score", 0.0)),
+            signals=dict(obj.get("signals", {})),
+            path=obj.get("path"),
+        )
+
+    def to_dict(self) -> Dict[str, Any]:
+        d = {"score": self.score, "signals": self.signals}
+        if self.path:
+            d["path"] = self.path
+        return d
+
+
+def _avg(values: Tuple[float, ...]) -> float:
+    return float(sum(values) / max(1, len(values)))
+
+
+def aggregate_scores(project_root: str | Path = ".") -> Dict[str, Any]:
+    """
+    Return a deterministic dict of detector results and a simple overall score.
+    """
+    root = Path(project_root).resolve()
+    unified = CapabilityScore.from_detector(detect_unified_training(root))
+
+    # Weighted average if/when other detectors are added; for now just the mean
+    parts = {
+        "unified_training": unified.to_dict(),
+    }
+    overall = _avg(tuple(p["score"] for p in parts.values()))
+    return {
+        "scores": parts,
+        "score_overall": overall,
+        "schema_version": 1,
+    }
diff --git a/tests/detectors/test_aggregate.py b/tests/detectors/test_aggregate.py
new file mode 100644
index 0000000..5c7fb6b
--- /dev/null
+++ b/tests/detectors/test_aggregate.py
@@ -0,0 +1,60 @@
+from __future__ import annotations
+from pathlib import Path
+from codex_ml.detectors.aggregate import aggregate_scores
+
+
+def _write(p: Path, txt: str) -> None:
+    p.parent.mkdir(parents=True, exist_ok=True)
+    p.write_text(txt, encoding="utf-8")
+
+
+def test_aggregate_empty(tmp_path: Path):
+    res = aggregate_scores(tmp_path)
+    assert "scores" in res and "score_overall" in res and res["schema_version"] == 1
+    # unified training should be 0.0 when file is absent
+    assert res["scores"]["unified_training"]["score"] == 0.0
+
+
+def test_aggregate_with_unified_training(tmp_path: Path):
+    p = tmp_path / "src" / "codex_ml" / "training" / "unified_training.py"
+    _write(
+        p,
+        "class UnifiedTrainingConfig: pass\n"
+        "def run_unified_training(*a, **k): pass\n"
+        "def train_loop(*a, **k): pass\n"
+        "def functional_training(*a, **k): pass\n"
+        "resume_from = None\n"
+        "checkpoint_dir = 'ck'\n",
+    )
+    res = aggregate_scores(tmp_path)
+    uni = res["scores"]["unified_training"]
+    assert 0.7 <= uni["score"] <= 0.9
+    assert res["score_overall"] == uni["score"]
diff --git a/CHANGELOG_CODEX.md b/CHANGELOG_CODEX.md
index b8c9f40..f0d8f6a 100644
--- a/CHANGELOG_CODEX.md
+++ b/CHANGELOG_CODEX.md
@@ -1,5 +1,23 @@
 # Codex Changelog
+## 2025-10-07 â Detectors aggregator + module entrypoint
+
+### WHY
+- Wire up the **detector aggregator** consumed by the CLIâs `--include-detectors` flag and README badge capability digest.
+- Ensure `python -m codex_ml` works reliably by adding a package `__main__`.
+
+### Changes
+- Detectors:
+  - `src/codex_ml/detectors/aggregate.py` and package `__init__` re-exports.
+  - Tests: `tests/detectors/test_aggregate.py`.
+- Entrypoints:
+  - `src/codex_ml/__main__.py` and minimal `__init__.py` with `__version__`.
+
+### Risk
+- Low. Additive modules; no network or runtime changes to training path.
+
+### Rollback
+- Remove the new files and revert the CLI capability digest usage.
```
````diff
diff --git a/.pre-commit-config.yaml b/.pre-commit-config.yaml
new file mode 100644
index 0000000..f1ad3f2
--- /dev/null
+++ b/.pre-commit-config.yaml
@@ -0,0 +1,38 @@
+# Local-only pre-commit hooks. No network, no external repos.
+repos:
+  - repo: local
+    hooks:
+      - id: scan-secrets
+        name: scan-secrets
+        entry: python tools/scan_secrets.py --fail-on-find
+        language: system
+        types: [text]
+        pass_filenames: true
+      - id: validate-fences
+        name: validate-fences
+        entry: python tools/validate_fences.py --paths README.md docs
+        language: system
+        types: [text]
+        pass_filenames: false
+
diff --git a/tools/ndjson_summarize.py b/tools/ndjson_summarize.py
new file mode 100755
index 0000000..0c94f4f
--- /dev/null
+++ b/tools/ndjson_summarize.py
@@ -0,0 +1,151 @@
+#!/usr/bin/env python3
+from __future__ import annotations
+"""
+Summarize NDJSON (one JSON object per line) into a CSV with dot-flattened keys.
+
+Notes:
+- NDJSON spec: one JSON object per line, UTF-8, newline-delimited. :contentReference[oaicite:0]{index=0}
+- MIME type is commonly `application/x-ndjson`. :contentReference[oaicite:1]{index=1}
+"""
+import argparse
+import csv
+import io
+import json
+import sys
+from pathlib import Path
+from typing import Any, Dict, Iterable, List, Tuple
+
+
+def _flatten(obj: Dict[str, Any], prefix: str = "") -> Dict[str, Any]:
+    out: Dict[str, Any] = {}
+    for k, v in obj.items():
+        key = f"{prefix}.{k}" if prefix else k
+        if isinstance(v, dict):
+            out.update(_flatten(v, key))
+        else:
+            out[key] = v
+    return out
+
+
+def _read_ndjson(path: Path) -> Iterable[Dict[str, Any]]:
+    with path.open("r", encoding="utf-8") as f:
+        for line in f:
+            s = line.strip()
+            if not s:
+                continue
+            yield json.loads(s)
+
+
+def summarize(input_path: str, output_path: str) -> None:
+    src = Path(input_path)
+    dst = Path(output_path)
+    rows = [_flatten(obj) for obj in _read_ndjson(src)]
+    if not rows:
+        # Emit an empty CSV with no headers
+        dst.write_text("", encoding="utf-8")
+        return
+    # stable union of keys across all rows
+    headers: List[str] = sorted({k for r in rows for k in r.keys()})
+    dst.parent.mkdir(parents=True, exist_ok=True)
+    with dst.open("w", encoding="utf-8", newline="") as f:
+        w = csv.DictWriter(f, fieldnames=headers)
+        w.writeheader()
+        for r in rows:
+            w.writerow({h: r.get(h, "") for h in headers})
+
+
+def main(argv: List[str] | None = None) -> int:
+    ap = argparse.ArgumentParser(description="Summarize NDJSON into CSV")
+    ap.add_argument("input", help="Input NDJSON file")
+    ap.add_argument("--output", "-o", required=True, help="Output CSV file")
+    args = ap.parse_args(argv)
+    summarize(args.input, args.output)
+    return 0
+
+
+if __name__ == "__main__":
+    sys.exit(main())
diff --git a/tools/scan_secrets.py b/tools/scan_secrets.py
new file mode 100755
index 0000000..5c8a7c9
--- /dev/null
+++ b/tools/scan_secrets.py
@@ -0,0 +1,199 @@
+#!/usr/bin/env python3
+from __future__ import annotations
+"""
+Minimal local secret scanner (regex-based).
+ - Scans provided paths (files or directories) recursively.
+ - Exits non-zero when --fail-on-find and any match is found.
+ - Ignores .git/ and common virtualenv folders.
+No network calls.
+"""
+import argparse
+import base64
+import os
+import re
+import sys
+from pathlib import Path
+from typing import Iterable, List, Tuple
+
+
+# Common secret patterns (additive, conservative)
+PATTERNS = [
+    # AWS Access Key ID
+    (r"AKIA[0-9A-Z]{16}", "AWS_ACCESS_KEY_ID"),
+    # GitHub classic tokens (ghp_, gho_, ghu_, ghp for classic PAT)
+    (r"gh[pous]_[A-Za-z0-9]{36,255}", "GITHUB_TOKEN"),
+    # Google API key
+    (r"AIza[0-9A-Za-z\-_]{35}", "GOOGLE_API_KEY"),
+    # Slack bot/user tokens
+    (r"xox[baprs]-[A-Za-z0-9\-]{10,48}", "SLACK_TOKEN"),
+    # Generic Base64 high-entropy blob (very conservative; 40+ chars with padding)
+    (r"(?<![A-Za-z0-9+/=])[A-Za-z0-9+/]{40,}={0,2}(?![A-Za-z0-9+/=])", "BASE64_HIGH_ENTROPY"),
+]
+
+EXCLUDES = {".git", ".hg", ".svn", ".venv", "venv", "__pycache__"}
+
+
+def _iter_paths(inputs: Iterable[str]) -> Iterable[Path]:
+    for s in inputs:
+        p = Path(s)
+        if p.is_file():
+            yield p
+        elif p.is_dir():
+            for root, dirs, files in os.walk(p):
+                # prune excluded directories
+                dirs[:] = [d for d in dirs if d not in EXCLUDES]
+                for fn in files:
+                    yield Path(root) / fn
+
+
+def _is_text_file(path: Path) -> bool:
+    try:
+        with path.open("rb") as f:
+            chunk = f.read(4096)
+        # Heuristic: if it decodes as UTF-8 (with errors) and has few NULs, treat as text
+        if b"\x00" in chunk:
+            return False
+        _ = chunk.decode("utf-8", errors="replace")
+        return True
+    except Exception:
+        return False
+
+
+def scan(paths: Iterable[str]) -> List[Tuple[str, int, str, str]]:
+    """
+    Return list of matches: (file, line_no, kind, excerpt)
+    """
+    results: List[Tuple[str, int, str, str]] = []
+    regs = [(re.compile(p), kind) for p, kind in PATTERNS]
+    for p in _iter_paths(paths):
+        if not _is_text_file(p):
+            continue
+        try:
+            with p.open("r", encoding="utf-8", errors="replace") as f:
+                for i, line in enumerate(f, 1):
+                    for rx, kind in regs:
+                        if rx.search(line):
+                            excerpt = line.strip()
+                            if len(excerpt) > 160:
+                                excerpt = excerpt[:157] + "..."
+                            results.append((p.as_posix(), i, kind, excerpt))
+        except Exception:
+            continue
+    return results
+
+
+def main(argv: list[str] | None = None) -> int:
+    ap = argparse.ArgumentParser(description="Local secret scanner (regex)")
+    ap.add_argument("paths", nargs="*", default=["."], help="Paths to scan (default: .)")
+    ap.add_argument("--fail-on-find", action="store_true", help="Return non-zero exit when any secret is found")
+    args = ap.parse_args(argv)
+    matches = scan(args.paths)
+    if matches:
+        for file, line_no, kind, excerpt in matches:
+            print(f"{file}:{line_no}: {kind}: {excerpt}")
+        return 2 if args.fail_on_find else 0
+    return 0
+
+
+if __name__ == "__main__":
+    sys.exit(main())
diff --git a/tools/validate_fences.py b/tools/validate_fences.py
new file mode 100755
index 0000000..5d4a3c1
--- /dev/null
+++ b/tools/validate_fences.py
@@ -0,0 +1,173 @@
+#!/usr/bin/env python3
+from __future__ import annotations
+"""
+Very small Markdown fence validator:
+ - Ensures code fences are balanced and consistent (``` or ~~~).
+ - Reports first offending file/line with non-zero exit.
+This is intentionally permissive and local-only.
+"""
+import argparse
+import sys
+from pathlib import Path
+from typing import Iterable, Tuple
+
+
+def _iter_md_files(root: Path) -> Iterable[Path]:
+    if root.is_file():
+        if root.suffix.lower() in {".md", ".markdown"}:
+            yield root
+        return
+    for p in root.rglob("*"):
+        if p.is_file() and p.suffix.lower() in {".md", ".markdown"}:
+            yield p
+
+
+def _check_file(path: Path) -> Tuple[bool, str]:
+    open_fence = None  # tuple(delim, length)
+    with path.open("r", encoding="utf-8", errors="replace") as f:
+        for ln, line in enumerate(f, 1):
+            s = line.rstrip("\n")
+            # Detect fences only at line start (0-3 spaces allowed)
+            stripped = s.lstrip(" ")
+            leading_spaces = len(s) - len(stripped)
+            if leading_spaces > 3:
+                continue
+            # backtick or tilde run?
+            if stripped.startswith("```") or stripped.startswith("~~~"):
+                delim = stripped[0]
+                count = 0
+                for ch in stripped:
+                    if ch == delim:
+                        count += 1
+                    else:
+                        break
+                # opening?
+                if open_fence is None:
+                    open_fence = (delim, count)
+                else:
+                    # must match current fence and be >= len
+                    d0, c0 = open_fence
+                    if delim != d0 or count < c0:
+                        return False, f"{path}:{ln}: mismatched closing fence (expected {d0*c0})"
+                    open_fence = None
+    if open_fence is not None:
+        d0, c0 = open_fence
+        return False, f"{path}: EOF: unclosed fence ({d0*c0})"
+    return True, ""
+
+
+def main(argv=None) -> int:
+    ap = argparse.ArgumentParser(description="Validate Markdown code fences")
+    ap.add_argument("--paths", nargs="+", default=["README.md", "docs"], help="Paths/files to scan")
+    args = ap.parse_args(argv)
+    roots = [Path(p) for p in args.paths]
+    for r in roots:
+        if not r.exists():
+            continue
+        for p in _iter_md_files(r):
+            ok, msg = _check_file(p)
+            if not ok:
+                print(msg)
+                return 2
+    return 0
+
+
+if __name__ == "__main__":
+    sys.exit(main())
diff --git a/tools/bench_unified_training.py b/tools/bench_unified_training.py
new file mode 100755
index 0000000..e3a64aa
--- /dev/null
+++ b/tools/bench_unified_training.py
@@ -0,0 +1,118 @@
+#!/usr/bin/env python3
+from __future__ import annotations
+"""
+Micro-benchmark for the unified training faÃ§ade.
+This is an *indicative* local throughput check (no network).
+
+Determinism reference:
+- See PyTorch `torch.use_deterministic_algorithms`. :contentReference[oaicite:2]{index=2}
+"""
+import time
+
+try:
+    import torch
+    from torch import nn
+except Exception:
+    torch = None  # type: ignore
+    nn = None  # type: ignore
+
+from codex_ml.training.unified_training import UnifiedTrainingConfig, run_unified_training
+
+
+def _loader(n=2048, batch=64, device="cpu"):
+    x = torch.linspace(-1, 1, steps=n, device=device).reshape(-1, 1)
+    y = 2.5 * x - 0.3
+    ds = torch.utils.data.TensorDataset(x, y)
+    return torch.utils.data.DataLoader(ds, batch_size=batch, shuffle=False)
+
+
+def bench(epochs: int = 2) -> None:
+    if torch is None:
+        print("PyTorch not available; skipping.")
+        return
+    device = "cuda" if torch.cuda.is_available() else "cpu"
+    model = nn.Sequential(nn.Linear(1, 16), nn.ReLU(), nn.Linear(16, 1)).to(device)
+    opt = torch.optim.SGD(model.parameters(), lr=1e-2)
+    loss = nn.MSELoss()
+    cfg = UnifiedTrainingConfig(epochs=epochs, lr=1e-2, deterministic=True, device=device)
+    dl = _loader(device=device)
+    t0 = time.perf_counter()
+    hist = run_unified_training(cfg, model=model, optimizer=opt, loss_fn=loss, train_loader=dl)
+    t1 = time.perf_counter()
+    dt = max(1e-9, t1 - t0)
+    steps = epochs * (len(dl))
+    print(f"device={device} epochs={epochs} steps={steps} time={dt:.3f}s => {steps/dt:.1f} steps/s; last_loss={hist['loss'][-1]:.6f}")
+
+
+if __name__ == "__main__":
+    bench()
diff --git a/tests/tools/test_ndjson_summarize.py b/tests/tools/test_ndjson_summarize.py
new file mode 100644
index 0000000..b58036e
--- /dev/null
+++ b/tests/tools/test_ndjson_summarize.py
@@ -0,0 +1,42 @@
+import csv
+import json
+import subprocess
+import sys
+from pathlib import Path
+
+
+def test_ndjson_to_csv(tmp_path: Path):
+    nd = tmp_path / "in.ndjson"
+    rows = [
+        {"a": 1, "b": {"c": 2}},
+        {"a": 3, "d": 4},
+    ]
+    nd.write_text("\n".join(json.dumps(r) for r in rows) + "\n", encoding="utf-8")
+    out = tmp_path / "out.csv"
+    p = subprocess.run(
+        [sys.executable, "tools/ndjson_summarize.py", str(nd), "-o", str(out)],
+        check=True,
+        capture_output=True,
+        text=True,
+    )
+    assert out.exists()
+    with out.open("r", encoding="utf-8") as f:
+        reader = csv.DictReader(f)
+        cols = reader.fieldnames
+        assert set(cols) == {"a", "b.c", "d"}
+        rows_csv = list(reader)
+        assert rows_csv[0]["a"] == "1"
+        assert rows_csv[0]["b.c"] == "2"
+        assert rows_csv[1]["d"] == "4"
diff --git a/tests/tools/test_scan_secrets.py b/tests/tools/test_scan_secrets.py
new file mode 100644
index 0000000..a9c2f8f
--- /dev/null
+++ b/tests/tools/test_scan_secrets.py
@@ -0,0 +1,39 @@
+import subprocess
+import sys
+from pathlib import Path
+
+
+def test_scan_secrets_detects_pattern(tmp_path: Path):
+    suspect = tmp_path / "bad.txt"
+    suspect.write_text("token: ghp_abcdefghijklmnopqrstuvwxyz012345\n", encoding="utf-8")
+    p = subprocess.run(
+        [sys.executable, "tools/scan_secrets.py", str(tmp_path), "--fail-on-find"],
+        capture_output=True,
+        text=True,
+    )
+    assert p.returncode != 0
+    assert "GITHUB_TOKEN" in p.stdout + p.stderr
+
+
+def test_scan_secrets_clean_passes(tmp_path: Path):
+    clean = tmp_path / "ok.txt"
+    clean.write_text("nothing to see here\n", encoding="utf-8")
+    p = subprocess.run(
+        [sys.executable, "tools/scan_secrets.py", str(tmp_path), "--fail-on-find"],
+        capture_output=True,
+        text=True,
+        check=False,
+    )
+    assert p.returncode == 0
diff --git a/tests/cli/test_cli_tracking_decide.py b/tests/cli/test_cli_tracking_decide.py
new file mode 100644
index 0000000..b21b9db
--- /dev/null
+++ b/tests/cli/test_cli_tracking_decide.py
@@ -0,0 +1,38 @@
+import json
+import os
+import subprocess
+import sys
+from pathlib import Path
+
+
+def test_cli_tracking_decide_offline_blocks_remote(tmp_path: Path, monkeypatch):
+    env = os.environ.copy()
+    env.update(
+        {
+            "MLFLOW_TRACKING_URI": "http://mlflow.local:5000",
+            "MLFLOW_OFFLINE": "1",
+            "WANDB_MODE": "offline",
+        }
+    )
+    p = subprocess.run(
+        [sys.executable, "-m", "codex_ml", "tracking", "decide", "--print"],
+        capture_output=True,
+        text=True,
+        env=env,
+    )
+    # Prints a JSON decision to stdout
+    data = json.loads(p.stdout.strip().splitlines()[-1])
+    assert data["blocked"] is True
+    assert data["uri"].startswith("file://")
+
diff --git a/docs/quality_gates.md b/docs/quality_gates.md
new file mode 100644
index 0000000..7b1f87e
--- /dev/null
+++ b/docs/quality_gates.md
@@ -0,0 +1,88 @@
+# Quality Gates (Local, Fast)
+
+All checks are **offline** and run locally via `pre-commit`.
+
+## Install hooks
+```bash
+pre-commit install
+```
+
+## Hooks
+
+### Secret scan
+Runs a conservative regex scanner on changed files:
+```bash
+python tools/scan_secrets.py --fail-on-find .
+```
+Exits non-zero if any match is found so the commit is blocked.
+
+### Fence validation
+Ensures Markdown code fences are balanced and consistent (``` or ~~~):
+```bash
+python tools/validate_fences.py --paths README.md docs
+```
+
+### NDJSON summarize (tool)
+Converts NDJSON logs to CSV, flattening nested keys into dot-notation:
+```bash
+python tools/ndjson_summarize.py run.ndjson --output run.csv
+```
+
+**References**
+- NDJSON spec and serialization rules: newline-delimited JSON objects, UTF-8. :contentReference[oaicite:3]{index=3}
+- MIME reference for NDJSON: `application/x-ndjson`. :contentReference[oaicite:4]{index=4}
+- Deterministic training (for parity tests / benches): `torch.use_deterministic_algorithms`. :contentReference[oaicite:5]{index=5}
+- W&B offline posture via `WANDB_MODE=offline` / disabled. :contentReference[oaicite:6]{index=6}
+- MLflow tracking server & URIs (blocked by guards unless explicitly allowed). :contentReference[oaicite:7]{index=7}
diff --git a/README.md b/README.md
index 0c8a6e3..2d8a3f4 100644
--- a/README.md
+++ b/README.md
@@ -65,3 +65,13 @@ See `docs/cli.md` for more.
 - **Metrics**:
   - Canonical: `codex_ml.metrics.perplexity_from_losses`.
   - Deprecated shim: `tools/apply_ml_metrics.py::perplexity` (emits `DeprecationWarning`).
+
+## Quality Gates
+- Install hooks: `pre-commit install`
+- Local checks (offline):
+  - Secret scan: `python tools/scan_secrets.py --fail-on-find .`
+  - Fence validation: `python tools/validate_fences.py --paths README.md docs`
+  - NDJSON summarize: `python tools/ndjson_summarize.py run.ndjson -o run.csv`
+
+See `docs/quality_gates.md` for details and references (NDJSON spec, PyTorch determinism, MLflow/W&B posture). :contentReference[oaicite:8]{index=8}
diff --git a/CHANGELOG_CODEX.md b/CHANGELOG_CODEX.md
index f0d8f6a..4a1b1a3 100644
--- a/CHANGELOG_CODEX.md
+++ b/CHANGELOG_CODEX.md
@@ -1,5 +1,34 @@
 # Codex Changelog
+## 2025-10-07 â Quality gates + tools (NDJSON summarize, secret scan, fence validator) + CLI tracking test
+
+### WHY
+- Close the **quality gates** and tooling references with local, offline-friendly utilities.
+- Provide tests for NDJSON summarization, secret scanning, and CLI tracking decision.
+
+### Changes
+- Tools:
+  - `tools/ndjson_summarize.py` (NDJSON â CSV; dot-flattened keys).
+  - `tools/scan_secrets.py` (regex scanner; local).
+  - `tools/validate_fences.py` (balanced code fences).
+  - `tools/bench_unified_training.py` (micro-bench; deterministic posture reference).
+- Tests:
+  - `tests/tools/test_ndjson_summarize.py`
+  - `tests/tools/test_scan_secrets.py`
+  - `tests/cli/test_cli_tracking_decide.py`
+- Config:
+  - `.pre-commit-config.yaml` with local hooks only (no network).
+- Docs:
+  - `docs/quality_gates.md`; README links.
+
+### Risk
+- Low. Tools operate locally; hooks are conservative and can be tuned.
+
+### Rollback
+- Remove tools/tests/docs and the pre-commit config to revert the quality gates.
+
 ## 2025-10-07 â Detectors aggregator + module entrypoint
 ### WHY
 - Wire up the **detector aggregator** consumed by the CLIâs `--include-detectors` flag and README badge capability digest.
 - Ensure `python -m codex_ml` works reliably by adding a package `__main__`.
````
````diff
diff --git a/src/codex_ml/training/__init__.py b/src/codex_ml/training/__init__.py
new file mode 100644
index 0000000..8f2e5a1
--- /dev/null
+++ b/src/codex_ml/training/__init__.py
@@ -0,0 +1,18 @@
+"""
+Training faÃ§ade public surface.
+"""
+from .unified_training import (
+    UnifiedTrainingConfig,
+    run_unified_training,
+    train_loop,
+    functional_training,
+)
+
+__all__ = [
+    "UnifiedTrainingConfig",
+    "run_unified_training",
+    "train_loop",
+    "functional_training",
+]
diff --git a/tests/training/test_resume_epoch_and_rng.py b/tests/training/test_resume_epoch_and_rng.py
new file mode 100644
index 0000000..e7a3f42
--- /dev/null
+++ b/tests/training/test_resume_epoch_and_rng.py
@@ -0,0 +1,131 @@
+import math
+import os
+from pathlib import Path
+import pytest
+
+torch = pytest.importorskip("torch")
+nn = torch.nn
+
+from codex_ml.training import UnifiedTrainingConfig, run_unified_training
+
+
+def _make_loader(n=256, batch=32, device="cpu"):
+    x = torch.linspace(-1, 1, steps=n, device=device).reshape(-1, 1)
+    y = 2.5 * x - 0.3
+    ds = torch.utils.data.TensorDataset(x, y)
+    return torch.utils.data.DataLoader(ds, batch_size=batch, shuffle=False)
+
+
+def _make_model(device="cpu"):
+    torch.manual_seed(123)  # ensure same init across runs
+    return nn.Sequential(nn.Linear(1, 8), nn.ReLU(), nn.Linear(8, 1)).to(device)
+
+
+def _copy_state_dict(m):
+    return {k: v.detach().clone() for k, v in m.state_dict().items()}
+
+
+def _allclose_params(a, b, atol=1e-8):
+    for k in a:
+        if not torch.allclose(a[k], b[k], atol=atol, rtol=0.0):
+            return False
+    return True
+
+
+@pytest.mark.parametrize("device", ["cpu"])
+def test_resume_matches_continuous_training(tmp_path: Path, device: str):
+    # Continuous 2-epoch run
+    modelA = _make_model(device)
+    optA = torch.optim.SGD(modelA.parameters(), lr=1e-2)
+    loss = nn.MSELoss()
+    cfgA = UnifiedTrainingConfig(epochs=2, lr=1e-2, deterministic=True, device=device)
+    histA = run_unified_training(cfgA, model=modelA, optimizer=optA, loss_fn=loss, train_loader=_make_loader(device=device))
+    finalA = _copy_state_dict(modelA)
+
+    # Split: one epoch â checkpoint, then resume for second epoch
+    ckdir = tmp_path / "ck"
+    # First epoch to produce epoch-0000
+    modelB = _make_model(device)
+    optB = torch.optim.SGD(modelB.parameters(), lr=1e-2)
+    cfgB1 = UnifiedTrainingConfig(epochs=1, lr=1e-2, deterministic=True, device=device, checkpoint_dir=str(ckdir))
+    histB1 = run_unified_training(cfgB1, model=modelB, optimizer=optB, loss_fn=loss, train_loader=_make_loader(device=device))
+    # Resume to run epoch 1 only
+    resume_dir = ckdir / "epoch-0000"
+    assert resume_dir.exists()
+    cfgB2 = UnifiedTrainingConfig(epochs=2, lr=1e-2, deterministic=True, device=device, resume_from=str(resume_dir))
+    histB2 = run_unified_training(cfgB2, model=modelB, optimizer=optB, loss_fn=loss, train_loader=_make_loader(device=device))
+    finalB = _copy_state_dict(modelB)
+
+    # Loss trajectory should match within tiny epsilon
+    assert len(histA["loss"]) == 2
+    assert math.isclose(histA["loss"][0], histB1["loss"][0], rel_tol=0.0, abs_tol=1e-6)
+    assert math.isclose(histA["loss"][1], histB2["loss"][0], rel_tol=0.0, abs_tol=1e-6)
+    # Final parameters should match tightly (RNG + optimizer parity)
+    assert _allclose_params(finalA, finalB, atol=1e-8)
diff --git a/tests/checkpointing/test_resume_optimizer_rng_equivalence.py b/tests/checkpointing/test_resume_optimizer_rng_equivalence.py
new file mode 100644
index 0000000..4c9a2fb
--- /dev/null
+++ b/tests/checkpointing/test_resume_optimizer_rng_equivalence.py
@@ -0,0 +1,89 @@
+import math
+from pathlib import Path
+import pytest
+
+torch = pytest.importorskip("torch")
+nn = torch.nn
+
+from codex_ml.training import UnifiedTrainingConfig, run_unified_training
+
+
+def _loader(n=128, batch=32, device="cpu"):
+    x = torch.linspace(-1, 1, steps=n, device=device).reshape(-1, 1)
+    y = 1.1 * x - 0.2
+    ds = torch.utils.data.TensorDataset(x, y)
+    return torch.utils.data.DataLoader(ds, batch_size=batch, shuffle=False)
+
+
+def _model(device="cpu"):
+    torch.manual_seed(999)  # keep init consistent
+    return nn.Sequential(nn.Linear(1, 16), nn.ReLU(), nn.Linear(16, 1)).to(device)
+
+
+def _param_vec(m):
+    with torch.no_grad():
+        return torch.cat([p.detach().flatten().cpu() for p in m.parameters()])
+
+
+@pytest.mark.parametrize("device", ["cpu"])
+def test_resume_state_equivalence(tmp_path: Path, device: str):
+    # Baseline: 3 epochs continuous
+    m0 = _model(device)
+    opt0 = torch.optim.SGD(m0.parameters(), lr=5e-3)
+    loss = nn.MSELoss()
+    cfg0 = UnifiedTrainingConfig(epochs=3, lr=5e-3, deterministic=True, device=device)
+    run_unified_training(cfg0, model=m0, optimizer=opt0, loss_fn=loss, train_loader=_loader(device=device))
+    v_cont = _param_vec(m0)
+
+    # Split + resume: 1 epoch, then resume for remaining 2 epochs
+    ckdir = tmp_path / "ck"
+    m1 = _model(device)
+    opt1 = torch.optim.SGD(m1.parameters(), lr=5e-3)
+    cfg1a = UnifiedTrainingConfig(epochs=1, lr=5e-3, deterministic=True, device=device, checkpoint_dir=str(ckdir))
+    run_unified_training(cfg1a, model=m1, optimizer=opt1, loss_fn=loss, train_loader=_loader(device=device))
+    resume_from = ckdir / "epoch-0000"
+    cfg1b = UnifiedTrainingConfig(epochs=3, lr=5e-3, deterministic=True, device=device, resume_from=str(resume_from))
+    run_unified_training(cfg1b, model=m1, optimizer=opt1, loss_fn=loss, train_loader=_loader(device=device))
+    v_resume = _param_vec(m1)
+
+    assert torch.allclose(v_cont, v_resume, atol=1e-8, rtol=0.0)
diff --git a/docs/incident_runbook.md b/docs/incident_runbook.md
new file mode 100644
index 0000000..a8b3c1f
--- /dev/null
+++ b/docs/incident_runbook.md
@@ -0,0 +1,86 @@
+# Incident Runbook (Local / Offline)
+
+This runbook is optimized for local-only environments.
+
+## 1) Stabilize
+- Capture a short description of the symptom and when it started.
+- Freeze inputs: copy current configs, seeds, and dataset manifest (use `python -m codex_ml hash manifest --write-readme`).
+- If tracking is enabled, run `python -m codex_ml tracking decide --print` to confirm offline posture / URIs.
+
+## 2) Reproduce deterministically
+- Set `deterministic=True` in `UnifiedTrainingConfig`.
+- Re-run the minimal failing scenario with a fixed seed and CPU if possible.
+- Persist NDJSON logs for the run.
+
+## 3) Bisect the change
+- Compare last known-good commit vs current.
+- Use the parity tests:
+  - training parity (legacy vs faÃ§ade),
+  - resume equivalence (final parameters / loss),
+  - data invariants (manifest hash / shard coverage).
+
+## 4) Rollback safely
+- If a recent diff caused the regression, revert the specific module and re-run parity/resume tests.
+- Do **not** change multiple subsystems at once; keep the rollback patch atomic.
+
+## 5) Document and prevent
+- Add a minimal failing test if coverage is missing.
+- Update `CHANGELOG_CODEX.md` with a WHY/Risk/Rollback entry.
+- Refresh README badge and capability digest when detectors change:
+  ```bash
+  python -m codex_ml hash manifest --include-detectors --write-readme
+  ```
+
+## Quick checklist
+- [ ] Deterministic repro achieved
+- [ ] Offline posture verified (no remote URIs)
+- [ ] Manifest unchanged (or change justified)
+- [ ] Parity / resume tests pass
+- [ ] Minimal rollback or fix prepared
````
````diff
diff --git a/src/codex_ml/checkpointing/__init__.py b/src/codex_ml/checkpointing/__init__.py
new file mode 100644
index 0000000..8a2f7a3
--- /dev/null
+++ b/src/codex_ml/checkpointing/__init__.py
@@ -0,0 +1,22 @@
+"""
+Checkpointing public surface.
+"""
+from .checkpoint_core import (
+    save_checkpoint,
+    load_checkpoint,
+    calculate_digest,
+    apply_rng_state,
+    current_rng_state,
+)
+
+__all__ = [
+    "save_checkpoint",
+    "load_checkpoint",
+    "calculate_digest",
+    "apply_rng_state",
+    "current_rng_state",
+]
diff --git a/src/codex_ml/detectors/unified_training.py b/src/codex_ml/detectors/unified_training.py
new file mode 100644
index 0000000..0f5c1a1
--- /dev/null
+++ b/src/codex_ml/detectors/unified_training.py
@@ -0,0 +1,120 @@
+from __future__ import annotations
+"""
+Heuristic detector for the unified training faÃ§ade.
+Scans local source files for key symbols and returns a score + signals.
+"""
+from dataclasses import dataclass
+from pathlib import Path
+from typing import Dict, Any
+
+REQUIRED_FILE_HINTS = (
+    "src/codex_ml/training/unified_training.py",
+    "codex_ml/training/unified_training.py",
+)
+
+REQUIRED_TOKENS = {
+    "config": "class UnifiedTrainingConfig",
+    "runner": "def run_unified_training",
+}
+OPTIONAL_TOKENS = {
+    "wrappers_train_loop": "def train_loop(",
+    "wrappers_functional": "def functional_training(",
+    "resume_hooks_resume_from": "resume_from",
+    "resume_hooks_checkpoint_dir": "checkpoint_dir",
+    "resume_hooks_save_checkpoint": "save_checkpoint(",
+}
+
+
+def _read_text(p: Path) -> str:
+    try:
+        return p.read_text(encoding="utf-8", errors="replace")
+    except Exception:
+        return ""
+
+
+def _scan_text(text: str) -> Dict[str, bool]:
+    sigs: Dict[str, bool] = {}
+    for k, tok in REQUIRED_TOKENS.items():
+        sigs[k] = (tok in text)
+    for k, tok in OPTIONAL_TOKENS.items():
+        sigs[k] = (tok in text)
+    return sigs
+
+
+def _score(sigs: Dict[str, bool]) -> float:
+    has_config = sigs.get("config", False)
+    has_runner = sigs.get("runner", False)
+    score = 0.0
+    if has_config and has_runner:
+        score = 0.60
+        # wrappers
+        if sigs.get("wrappers_train_loop") or sigs.get("wrappers_functional"):
+            score += 0.10
+        # resume hooks
+        if sigs.get("resume_hooks_resume_from") or sigs.get("resume_hooks_checkpoint_dir") or sigs.get("resume_hooks_save_checkpoint"):
+            score += 0.10
+    elif has_config or has_runner:
+        score = 0.30
+        if sigs.get("wrappers_train_loop") or sigs.get("wrappers_functional"):
+            score += 0.05
+    return min(1.0, score)
+
+
+def detect_unified_training(project_root: str | Path = ".") -> Dict[str, Any]:
+    root = Path(project_root).resolve()
+    target: Path | None = None
+    for hint in REQUIRED_FILE_HINTS:
+        candidate = root / hint
+        if candidate.exists():
+            target = candidate
+            break
+    if target is None:
+        # fallback: best-effort scan
+        for p in root.rglob("unified_training.py"):
+            if "site-packages" in p.as_posix():
+                continue
+            target = p
+            break
+    if target is None:
+        return {"score": 0.0, "signals": {}, "path": None}
+    text = _read_text(target)
+    sigs = _scan_text(text)
+    score = _score(sigs)
+    return {"score": score, "signals": sigs, "path": target.as_posix()}
diff --git a/src/tokenization/__init__.py b/src/tokenization/__init__.py
new file mode 100644
index 0000000..f8f1f5c
--- /dev/null
+++ b/src/tokenization/__init__.py
@@ -0,0 +1,18 @@
+"""
+Legacy tokenization shim (DEPRECATED).
+Re-export a minimal surface from `codex_ml.tokenization.api` and emit a DeprecationWarning.
+"""
+from __future__ import annotations
+import warnings
+
+warnings.warn(
+    "The top-level `tokenization` package is deprecated; use `codex_ml.tokenization.api`.",
+    category=DeprecationWarning,
+    stacklevel=2,
+)
+
+try:
+    from codex_ml.tokenization.api import WhitespaceTokenizer  # noqa: F401
+    __all__ = ["WhitespaceTokenizer"]
+except Exception:
+    __all__ = []
diff --git a/docs/checkpointing_core.md b/docs/checkpointing_core.md
new file mode 100644
index 0000000..a17a0d2
--- /dev/null
+++ b/docs/checkpointing_core.md
@@ -0,0 +1,105 @@
+# Checkpoint Core
+
+Canonical APIs:
+
+```python
+from codex_ml.checkpointing import save_checkpoint, load_checkpoint, calculate_digest
+```
+
+## Whatâs saved
+- **state**: `{"model": state_dict, "optimizer": state_dict, "rng": {...}}`
+- **meta**: `{"epoch": int, "loss": float, "...": any JSON-serializable}`
+
+Each checkpoint is a directory named `epoch-XXXX` with:
+- `state.pt` (torch.save of state dicts)
+- `meta.json` (UTF-8 JSON)
+- `digest.sha256` (aggregate SHA-256 of files for quick integrity checks)
+
+## Resume semantics
+`load_checkpoint(path)` returns `(state, meta)`:
+- Load **model** and **optimizer** state dicts if present.
+- Restore RNG via `apply_rng_state(state["rng"])`.
+- Starting epoch is `meta["epoch"] + 1`, or inferred from the folder name (`epoch-XXXX`).
+
+## Retention
+`save_checkpoint(..., keep_last_k=K)` keeps only the most recent `K` checkpoints in the same parent directory.
+
+## Digests
+`calculate_digest(dir)` walks the checkpoint directory and computes a stable aggregate hash. Useful for quick equality checks during tests and audits.
+
+## Notes
+- All operations are **local-first**.
+- No network calls are performed by the core.
diff --git a/docs/architecture.md b/docs/architecture.md
new file mode 100644
index 0000000..a3c2d8e
--- /dev/null
+++ b/docs/architecture.md
@@ -0,0 +1,86 @@
+# Architecture Overview
+
+The project favors a small, composable core with offline-friendly tooling.
+
+```mermaid
+flowchart TD
+  subgraph Training
+    UT[UnifiedTrainingConfig / run_unified_training]
+  end
+
+  subgraph Checkpointing
+    CK[checkpoint_core: save/load/digest]
+  end
+
+  subgraph Observability
+    LG[logging: NDJSON] --> MT[metrics: perplexity]
+    GR[tracking guards: MLflow/W&B]
+  end
+
+  subgraph Data
+    DP[Determinism tests: seeds/shards/manifest]
+  end
+
+  subgraph Tokenization
+    TK[canonical API: WhitespaceTokenizer]
+  end
+
+  UT -- epoch boundary --> CK
+  UT -- emits --> LG
+  UT -- posture check --> GR
+  UT -- drives --> DP
+  Tools[CLI + tools] --> LG
+  Tools --> GR
+```
+
+## Detectors
+Static detectors scan the local source to produce a **capability digest**, which can be embedded into the README:
+
+```
+<!-- BEGIN-CAPABILITY-HASH-BADGE -->
+Integrity: `<digest>` (SHA256)
+Capabilities: `<cap_digest>` (SHA256)
+<!-- END-CAPABILITY-HASH-BADGE -->
+```
+
+Everything runs locally, without external calls.
diff --git a/docs/index.md b/docs/index.md
index 8cd8c46..20b7f8a 100644
--- a/docs/index.md
+++ b/docs/index.md
@@ -1,6 +1,7 @@
 # Documentation Index
+- **Architecture**: `docs/architecture.md`
 - **Unified Training**: `docs/unified_training.md`
 - **Tokenizer Benchmark**: `tools/bench_tokenizer.py`
 - **Checkpoint Core**: `docs/checkpointing_core.md`
 - **Tracking & Observability**: `docs/observability.md`, `docs/observability_metrics.md`
 - **Tokenization API**: `docs/tokenization_api.md`
````
````diff
diff --git a/src/codex_ml/training/unified_training.py b/src/codex_ml/training/unified_training.py
index a3c59b1..f5a67b4 100644
--- a/src/codex_ml/training/unified_training.py
+++ b/src/codex_ml/training/unified_training.py
@@ -1,9 +1,11 @@
 from __future__ import annotations
 from dataclasses import dataclass
 from pathlib import Path
 from typing import Any, Dict, Optional
 try:
     import torch
     from torch import nn
+    from torch.backends import cudnn as _cudnn  # type: ignore[attr-defined]
 except Exception as e:  # pragma: no cover
     torch = None  # type: ignore
     nn = None  # type: ignore
@@ -23,6 +25,10 @@ class UnifiedTrainingConfig:
     keep_last_k: int = 5
     clip_grad_norm: Optional[float] = None   # threshold for clipping; None disables
     clip_grad_type: str = "norm"             # "norm" | "value"
     norm_type: float = 2.0                   # p-norm for norm clipping (e.g., 2.0)
+    # CUDA determinism helpers (applied when deterministic=True and CUDA available)
+    cudnn_deterministic: Optional[bool] = None   # default: True under deterministic posture
+    cudnn_benchmark: Optional[bool] = None       # default: False under deterministic posture
+    enforce_op_determinism: bool = True          # torch.use_deterministic_algorithms(True)
     def __post_init__(self) -> None:
         # epochs
         if not isinstance(self.epochs, int) or self.epochs <= 0:
@@ -58,14 +64,30 @@ def run_unified_training(
     if torch is None:
         raise RuntimeError("PyTorch is required for training")
     device = torch.device(cfg.device if (cfg.device == "cpu" or torch.cuda.is_available()) else "cpu")
     model.to(device)
     # Determinism posture
     if cfg.deterministic:
-        try:
-            torch.use_deterministic_algorithms(True, warn_only=True)  # newer PyTorch
-        except Exception:
-            try:
-                torch.use_deterministic_algorithms(True)  # fallback
-            except Exception:
-                pass
+        if cfg.enforce_op_determinism:
+            try:
+                torch.use_deterministic_algorithms(True, warn_only=True)  # newer PyTorch APIs
+            except Exception:
+                try:
+                    torch.use_deterministic_algorithms(True)  # fallback
+                except Exception:
+                    pass
+        # CUDA backend knobs (only if CUDA is available)
+        if hasattr(torch, "cuda") and torch.cuda.is_available():
+            try:
+                det = True if cfg.cudnn_deterministic is None else bool(cfg.cudnn_deterministic)
+                bm  = False if cfg.cudnn_benchmark    is None else bool(cfg.cudnn_benchmark)
+                try:
+                    _cudnn.deterministic = det  # type: ignore[assignment]
+                except Exception:
+                    pass
+                try:
+                    _cudnn.benchmark = bm  # type: ignore[assignment]
+                except Exception:
+                    pass
+            except Exception:
+                pass
         torch.manual_seed(0)
         if hasattr(torch, "cuda") and torch.cuda.is_available():
             torch.cuda.manual_seed_all(0)  # type: ignore[attr-defined]
diff --git a/src/codex_ml/cli.py b/src/codex_ml/cli.py
index b7a12a4..a5fb3d1 100644
--- a/src/codex_ml/cli.py
+++ b/src/codex_ml/cli.py
@@ -1,6 +1,6 @@
 """Minimal local/offline CLI for _codex_.
 Usage examples:
-  python -m codex_ml hash manifest --write-readme [--include-detectors]
+  python -m codex_ml hash manifest --write-readme [--include-detectors]
   python -m codex_ml tracking decide --summary tracking_summary.ndjson
 """
 import argparse
@@ -109,6 +109,49 @@ def _cmd_hash_manifest(args: argparse.Namespace) -> int:
         _update_readme_badge(root / "README.md", aggregate, cap_digest=cap_digest)
     return 0
+def _append_ndjson(path: Path, obj: Dict[str, object]) -> None:
+    path.parent.mkdir(parents=True, exist_ok=True)
+    with path.open("a", encoding="utf-8") as f:
+        json.dump(obj, f, ensure_ascii=False, separators=(",", ":"))
+        f.write("\n")
+
+
+def _cmd_tracking_decide(args: argparse.Namespace) -> int:
+    env = os.environ.copy()
+    decision = decide_mlflow_tracking_uri(environ=env)
+    payload = {
+        "ts": time.time(),
+        "blocked": decision.blocked,
+        "uri": decision.uri,
+        "reason": decision.reason,
+        "details": decision.details,
+    }
+    if args.summary:
+        _append_ndjson(Path(args.summary), payload)
+    if args.print:
+        print(json.dumps(payload, ensure_ascii=False))
+    return 0
+
+
 def _cmd_tracking(env: argparse.Namespace) -> int:
     raise SystemExit(2)
@@ -147,8 +190,10 @@ def _build_parser() -> argparse.ArgumentParser:
     ap_track = sub.add_parser("tracking", help="Tracking utilities")
     ap_track_sub = ap_track.add_subparsers(dest="tracking_cmd")
     ap_track.set_defaults(func=lambda _: ap_track.print_help() or 0)
     ap_decide = ap_track_sub.add_parser("decide", help="Decide MLflow URI under offline posture")
     ap_decide.add_argument("--summary", type=str, help="Append the decision to an NDJSON file")
+    ap_decide.add_argument("--print", action="store_true", help="Print decision JSON to stdout")
     ap_decide.set_defaults(func=_cmd_tracking_decide)
     return ap
diff --git a/tests/cli/test_cli_tracking_decide.py b/tests/cli/test_cli_tracking_decide.py
index b21b9db..34e84e2 100644
--- a/tests/cli/test_cli_tracking_decide.py
+++ b/tests/cli/test_cli_tracking_decide.py
@@ -1,7 +1,8 @@
 import json
 import os
 import subprocess
 import sys
 from pathlib import Path
 def test_cli_tracking_decide_offline_blocks_remote(tmp_path: Path, monkeypatch):
@@ -21,8 +22,26 @@ def test_cli_tracking_decide_offline_blocks_remote(tmp_path: Path, monkeypatch):
     )
     # Prints a JSON decision to stdout
     data = json.loads(p.stdout.strip().splitlines()[-1])
     assert data["blocked"] is True
     assert data["uri"].startswith("file://")
+
+
+def test_cli_tracking_decide_appends_ndjson(tmp_path: Path):
+    env = os.environ.copy()
+    env.update(
+        {
+            "MLFLOW_TRACKING_URI": "http://mlflow.local:5000",
+            "MLFLOW_OFFLINE": "1",
+        }
+    )
+    out = tmp_path / "summary.ndjson"
+    p = subprocess.run(
+        [sys.executable, "-m", "codex_ml", "tracking", "decide", "--summary", str(out)],
+        capture_output=True,
+        text=True,
+        env=env,
+    )
+    lines = out.read_text(encoding="utf-8").strip().splitlines()
+    assert lines and json.loads(lines[-1])["blocked"] is True
diff --git a/tests/training/test_cuda_determinism_guard.py b/tests/training/test_cuda_determinism_guard.py
new file mode 100644
index 0000000..a6b6b6b
--- /dev/null
+++ b/tests/training/test_cuda_determinism_guard.py
@@ -0,0 +1,45 @@
+import pytest
+
+torch = pytest.importorskip("torch")
+nn = torch.nn
+
+from codex_ml.training import UnifiedTrainingConfig, run_unified_training
+
+
+def _loader(n=128, batch=32, device="cpu"):
+    x = torch.linspace(-1, 1, steps=n, device=device).reshape(-1, 1)
+    y = 0.7 * x + 0.1
+    ds = torch.utils.data.TensorDataset(x, y)
+    return torch.utils.data.DataLoader(ds, batch_size=batch, shuffle=False)
+
+
+@pytest.mark.skipif(not torch.cuda.is_available(), reason="CUDA not available")
+def test_cuda_deterministic_switches_cudnn_flags(monkeypatch):
+    device = "cuda"
+    model = nn.Sequential(nn.Linear(1, 8), nn.ReLU(), nn.Linear(8, 1)).to(device)
+    opt = torch.optim.SGD(model.parameters(), lr=1e-2)
+    loss = nn.MSELoss()
+    cfg = UnifiedTrainingConfig(
+        epochs=1,
+        lr=1e-2,
+        deterministic=True,
+        device=device,
+        cudnn_deterministic=True,
+        cudnn_benchmark=False,
+    )
+    run_unified_training(cfg, model=model, optimizer=opt, loss_fn=loss, train_loader=_loader(device=device))
+    # If the flags exist, they should be set
+    if hasattr(torch.backends, "cudnn"):
+        assert getattr(torch.backends.cudnn, "benchmark", False) is False
+        assert getattr(torch.backends.cudnn, "deterministic", False) is True
+
diff --git a/tests/detectors/test_unified_training_partial.py b/tests/detectors/test_unified_training_partial.py
new file mode 100644
index 0000000..8b7c7c2
--- /dev/null
+++ b/tests/detectors/test_unified_training_partial.py
@@ -0,0 +1,25 @@
+from pathlib import Path
+from codex_ml.detectors.unified_training import detect_unified_training
+
+
+def test_partial_credit_when_only_config_present(tmp_path: Path):
+    p = tmp_path / "src" / "codex_ml" / "training" / "unified_training.py"
+    p.parent.mkdir(parents=True, exist_ok=True)
+    p.write_text("class UnifiedTrainingConfig: pass\n", encoding="utf-8")
+    res = detect_unified_training(tmp_path)
+    assert 0.30 <= res["score"] <= 0.35
+
+
+def test_partial_credit_when_only_runner_present(tmp_path: Path):
+    p = tmp_path / "src" / "codex_ml" / "training" / "unified_training.py"
+    p.parent.mkdir(parents=True, exist_ok=True)
+    p.write_text("def run_unified_training(*a, **k): pass\n", encoding="utf-8")
+    res = detect_unified_training(tmp_path)
+    assert 0.30 <= res["score"] <= 0.35
+
diff --git a/.pre-commit-config.yaml b/.pre-commit-config.yaml
index f1ad3f2..9a8a1c5 100644
--- a/.pre-commit-config.yaml
+++ b/.pre-commit-config.yaml
@@ -1,14 +1,33 @@
 # Local-only pre-commit hooks. No network, no external repos.
 repos:
   - repo: local
     hooks:
+      - id: ruff-optional
+        name: ruff (optional)
+        entry: bash -lc 'command -v ruff >/dev/null 2>&1 && ruff . || echo "[ruff] skipped (not installed)"'
+        language: system
+        types: [python]
+        pass_filenames: false
+      - id: mypy-optional
+        name: mypy (optional)
+        entry: bash -lc 'command -v mypy >/dev/null 2>&1 && mypy src || echo "[mypy] skipped (not installed)"'
+        language: system
+        types: [python]
+        pass_filenames: false
       - id: scan-secrets
         name: scan-secrets
         entry: python tools/scan_secrets.py --fail-on-find
         language: system
         types: [text]
         pass_filenames: true
       - id: validate-fences
         name: validate-fences
         entry: python tools/validate_fences.py --paths README.md docs
         language: system
         types: [text]
         pass_filenames: false
diff --git a/CHANGELOG_CODEX.md b/CHANGELOG_CODEX.md
index 4a1b1a3..cda3a72 100644
--- a/CHANGELOG_CODEX.md
+++ b/CHANGELOG_CODEX.md
@@ -1,5 +1,24 @@
 # Codex Changelog
+## 2025-10-07 â CUDA determinism knobs, CLI NDJSON append, optional lint/types, detector partial tests
+
+### WHY
+- Strengthen deterministic posture on CUDA (cudnn flags).
+- Make `tracking decide --summary` append NDJSON reliably.
+- Add optional ruff/mypy gates (local-only, skip when not installed).
+- Improve detector coverage with partial-credit tests.
+
+### Changes
+- Training: configurable `cudnn_deterministic` / `cudnn_benchmark`, guarded by `deterministic=True`.
+- CLI: NDJSON append helper; `--print` flag for decision echo.
+- Pre-commit: optional ruff/mypy hooks.
+- Tests: CUDA determinism (skips if no CUDA), NDJSON append, detector partial credit.
+
+### Risk
+- Low. Defaults unchanged; optional hooks skip when tools are absent.
+
+### Rollback
+- Revert touched files; remove hooks/tests if undesired.
````
```diff
diff --git a/pyproject.toml b/pyproject.toml
new file mode 100644
index 0000000..3d2ad3f
--- /dev/null
+++ b/pyproject.toml
@@ -0,0 +1,49 @@
+[tool.ruff]
+line-length = 100
+target-version = "py310"
+
+[tool.ruff.lint]
+select = ["E", "F", "I", "TID", "ICN", "UP", "B"]
+ignore = []
+
+# Per-file ignores: allow the legacy shim to import `tokenization` (and tests that import it explicitly).
+[tool.ruff.lint.per-file-ignores]
+"src/tokenization/__init__.py" = ["TID251"]
+"tests/**" = ["TID251"]
+
+# Ban direct imports of the legacy `tokenization` package anywhere else.
+[tool.ruff.lint.flake8-tidy-imports.banned-api]
+"tokenization".msg = "Use `codex_ml.tokenization.api` instead of the legacy `tokenization` package"
+
+[tool.ruff.analyze]
+detect-string-imports = true
+
+[tool.mypy]
+python_version = "3.10"
+ignore_missing_imports = true
+warn_unused_ignores = true
+strict_optional = true
+no_implicit_optional = true
+
diff --git a/tools/check_imports.py b/tools/check_imports.py
new file mode 100755
index 0000000..c5a9124
--- /dev/null
+++ b/tools/check_imports.py
@@ -0,0 +1,132 @@
+#!/usr/bin/env python3
+from __future__ import annotations
+"""
+Local static guard to forbid importing the legacy `tokenization` package.
+Compliments Ruff's banned-api rule, but works even if Ruff isn't installed.
+"""
+import ast
+import sys
+from pathlib import Path
+from typing import Iterable, Tuple
+
+ALLOWED = {
+    "src/tokenization/__init__.py",  # legacy shim
+}
+
+def iter_py_files(root: Path) -> Iterable[Path]:
+    for p in root.rglob("*.py"):
+        if p.is_file():
+            yield p
+
+def rel(p: Path) -> str:
+    try:
+        return p.relative_to(Path(".").resolve()).as_posix()
+    except Exception:
+        return p.as_posix()
+
+def scan_paths(paths: Iterable[Path]) -> list[Tuple[str, int, str]]:
+    problems: list[Tuple[str, int, str]] = []
+    for path in paths:
+        if not path.exists():
+            continue
+        for py in iter_py_files(path):
+            r = rel(py)
+            if r in ALLOWED:
+                continue
+            try:
+                text = py.read_text(encoding="utf-8")
+                tree = ast.parse(text, filename=r)
+            except Exception:
+                continue
+            for node in ast.walk(tree):
+                if isinstance(node, ast.Import):
+                    for alias in node.names:
+                        if alias.name.split(".")[0] == "tokenization":
+                            problems.append((r, node.lineno, f"import {alias.name}"))
+                elif isinstance(node, ast.ImportFrom):
+                    if node.module and node.module.split(".")[0] == "tokenization":
+                        problems.append((r, node.lineno, f"from {node.module} import ..."))
+    return problems
+
+def main(argv: list[str] | None = None) -> int:
+    roots = [Path("src"), Path("tests")]
+    probs = scan_paths(roots)
+    if probs:
+        for f, ln, msg in probs:
+            print(f"{f}:{ln}: banned legacy import: {msg} (use codex_ml.tokenization.api)")
+        return 2
+    print("OK: no legacy `tokenization` imports found")
+    return 0
+
+if __name__ == "__main__":
+    sys.exit(main())
+
diff --git a/tests/lint/test_no_legacy_tokenization_imports.py b/tests/lint/test_no_legacy_tokenization_imports.py
new file mode 100644
index 0000000..7b1b1c2
--- /dev/null
+++ b/tests/lint/test_no_legacy_tokenization_imports.py
@@ -0,0 +1,18 @@
+import subprocess
+import sys
+
+
+def test_repo_has_no_legacy_tokenization_imports():
+    # Works even if Ruff isn't installed; complements banned-api rule.
+    p = subprocess.run(
+        [sys.executable, "tools/check_imports.py"],
+        capture_output=True,
+        text=True,
+    )
+    if p.returncode != 0:
+        print(p.stdout + p.stderr)
+    assert p.returncode == 0
+
diff --git a/src/codex_ml/checkpointing/schema.py b/src/codex_ml/checkpointing/schema.py
new file mode 100644
index 0000000..3f1c0f1
--- /dev/null
+++ b/src/codex_ml/checkpointing/schema.py
@@ -0,0 +1,58 @@
+from __future__ import annotations
+from typing import Any, Dict
+
+SCHEMA_VERSION = 1
+
+REQUIRED = {
+    "epoch": int,   # >= 0
+}
+OPTIONAL = {
+    "loss": (int, float),
+}
+
+def validate_meta(meta: Dict[str, Any]) -> None:
+    if not isinstance(meta, dict):
+        raise ValueError("meta must be a dict")
+    # required
+    for k, tp in REQUIRED.items():
+        if k not in meta:
+            raise ValueError(f"meta missing required key: {k}")
+        if not isinstance(meta[k], tp):
+            raise ValueError(f"meta[{k!r}] must be {tp}")
+    if meta["epoch"] < 0:
+        raise ValueError("meta['epoch'] must be >= 0")
+    # optional types
+    for k, tp in OPTIONAL.items():
+        if k in meta and not isinstance(meta[k], tp):
+            raise ValueError(f"meta[{k!r}] must be {tp}")
+
+def ensure_schema(meta: Dict[str, Any]) -> Dict[str, Any]:
+    out = dict(meta)
+    out.setdefault("schema_version", SCHEMA_VERSION)
+    return out
+
diff --git a/src/codex_ml/checkpointing/checkpoint_core.py b/src/codex_ml/checkpointing/checkpoint_core.py
index 9a8a1a2..2e3f4c9 100644
--- a/src/codex_ml/checkpointing/checkpoint_core.py
+++ b/src/codex_ml/checkpointing/checkpoint_core.py
@@ -1,9 +1,13 @@
 from __future__ import annotations
 from pathlib import Path
 import json
 import hashlib
 from typing import Any, Dict, Tuple, Optional
+from .schema import validate_meta, ensure_schema
@@
 def save_checkpoint(out_dir: Path | str, *, state: Dict[str, Any], meta: Dict[str, Any], keep_last_k: int = 5) -> Path:
     out = Path(out_dir)
     out.mkdir(parents=True, exist_ok=True)
-    out.joinpath("meta.json").write_text(json.dumps(meta, ensure_ascii=False, indent=2), encoding="utf-8")
+    meta2 = ensure_schema(meta)
+    validate_meta(meta2)
+    out.joinpath("meta.json").write_text(json.dumps(meta2, ensure_ascii=False, indent=2), encoding="utf-8")
     # state + digest writing (existing logic) ...
@@
 def load_checkpoint(src_dir: Path | str, map_location: Optional[str] = None) -> Tuple[Dict[str, Any], Dict[str, Any]]:
     p = Path(src_dir)
     state = {}  # existing torch.load(...) or equivalent
     meta = json.loads(p.joinpath("meta.json").read_text(encoding="utf-8"))
+    validate_meta(meta)
     return state, meta
diff --git a/tests/checkpointing/test_meta_schema.py b/tests/checkpointing/test_meta_schema.py
new file mode 100644
index 0000000..c1f0e55
--- /dev/null
+++ b/tests/checkpointing/test_meta_schema.py
@@ -0,0 +1,31 @@
+from codex_ml.checkpointing.schema import validate_meta, ensure_schema, SCHEMA_VERSION
+import pytest
+
+
+def test_validate_ok_and_inject_version():
+    meta = {"epoch": 1, "loss": 0.123}
+    m = ensure_schema(meta)
+    assert m["schema_version"] == SCHEMA_VERSION
+    validate_meta(m)  # does not raise
+
+
+@pytest.mark.parametrize("bad", [
+    {}, {"epoch": -1}, {"epoch": "0"}, {"epoch": 0, "loss": "x"},
+])
+def test_validate_bad_meta(bad):
+    with pytest.raises(ValueError):
+        validate_meta(bad)
+
diff --git a/src/codex_ml/training/unified_training.py b/src/codex_ml/training/unified_training.py
index f5a67b4..2a8a9b1 100644
--- a/src/codex_ml/training/unified_training.py
+++ b/src/codex_ml/training/unified_training.py
@@ -21,6 +21,9 @@ class UnifiedTrainingConfig:
     clip_grad_type: str = "norm"             # "norm" | "value"
     norm_type: float = 2.0                   # p-norm for norm clipping (e.g., 2.0)
     # CUDA determinism helpers (applied when deterministic=True and CUDA available)
     cudnn_deterministic: Optional[bool] = None   # default: True under deterministic posture
     cudnn_benchmark: Optional[bool] = None       # default: False under deterministic posture
     enforce_op_determinism: bool = True          # torch.use_deterministic_algorithms(True)
+    # Mid-epoch resume helpers (testing/dev convenience)
+    break_after_batches: Optional[int] = None    # if set, checkpoint mid-epoch and stop after this many batches
+    mid_epoch_ckpt_dir: Optional[str] = None     # directory to write mid-epoch checkpoints
@@ -83,6 +86,8 @@ def run_unified_training(
     history: Dict[str, list] = {"loss": []}
     scaler = kwargs.get("scaler", None)
     clip_norm = cfg.clip_grad_norm
     use_norm = (clip_norm is not None) and (cfg.clip_grad_type == "norm")
     use_val = (clip_norm is not None) and (cfg.clip_grad_type == "value")
+    # if resuming mid-epoch, skip the already-completed batches on the first epoch
+    resume_skip_batches = 0
     for epoch in range(start_epoch, cfg.epochs):
         model.train(True)
@@ -90,7 +95,16 @@ def run_unified_training(
         n_batches = 0
         optimizer.zero_grad(set_to_none=True)
-        for i, batch in enumerate(train_loader):
+        # infer resume_skip_batches from meta if resuming into an unfinished epoch
+        if epoch == start_epoch and cfg.resume_from:
+            try:
+                _, meta0 = load_checkpoint(cfg.resume_from, map_location=cfg.device)
+                if isinstance(meta0, dict) and "batch_index" in meta0 and "epoch" in meta0 and int(meta0["epoch"]) == epoch:
+                    resume_skip_batches = int(meta0["batch_index"]) + 1
+            except Exception:
+                pass
+
+        for i, batch in enumerate(train_loader):
             if isinstance(batch, (list, tuple)) and len(batch) == 2:
                 x, y = batch
             else:
@@ -99,6 +113,10 @@ def run_unified_training(
             x = x.to(device)
             y = y.to(device) if y is not None else None
+            # skip batches already processed in a previous partial run
+            if resume_skip_batches > 0:
+                resume_skip_batches -= 1
+                continue
             def _step():
                 out = model(x)
                 loss = loss_fn(out, y) if y is not None else loss_fn(out)
@@ -143,6 +161,19 @@ def run_unified_training(
                     optimizer.step()
                     optimizer.zero_grad(set_to_none=True)
+            # optional mid-epoch checkpoint-and-break (testing/dev)
+            if cfg.break_after_batches is not None and (i + 1) == int(cfg.break_after_batches):
+                if cfg.mid_epoch_ckpt_dir:
+                    out_dir = Path(cfg.mid_epoch_ckpt_dir) / f"epoch-{epoch:04d}_step-{i:04d}"
+                    meta = {"epoch": epoch, "batch_index": i}
+                    save_checkpoint(out_dir, state={"model": model.state_dict(), "optimizer": optimizer.state_dict()}, meta=meta, keep_last_k=cfg.keep_last_k)
+                    history.setdefault("interrupted", True)
+                    history.setdefault("resume_path", str(out_dir))
+                # break the epoch loop early
+                running_loss += float(loss.detach().cpu().item())
+                n_batches += 1
+                return history
+
             running_loss += float(loss.detach().cpu().item())
             n_batches += 1
diff --git a/tests/training/test_mid_epoch_resume_equivalence.py b/tests/training/test_mid_epoch_resume_equivalence.py
new file mode 100644
index 0000000..7b8a9c0
--- /dev/null
+++ b/tests/training/test_mid_epoch_resume_equivalence.py
@@ -0,0 +1,83 @@
+import math
+from pathlib import Path
+import pytest
+
+torch = pytest.importorskip("torch")
+nn = torch.nn
+
+from codex_ml.training import UnifiedTrainingConfig, run_unified_training
+
+
+def _loader(n=256, batch=32, device="cpu"):
+    x = torch.linspace(-1, 1, steps=n, device=device).reshape(-1, 1)
+    y = 2.0 * x - 0.5
+    ds = torch.utils.data.TensorDataset(x, y)
+    return torch.utils.data.DataLoader(ds, batch_size=batch, shuffle=False)
+
+
+def _model(device="cpu"):
+    torch.manual_seed(321)
+    return nn.Sequential(nn.Linear(1, 8), nn.ReLU(), nn.Linear(8, 1)).to(device)
+
+
+def _param_vec(m):
+    with torch.no_grad():
+        return torch.cat([p.detach().flatten().cpu() for p in m.parameters()])
+
+
+@pytest.mark.parametrize("device", ["cpu"])
+def test_mid_epoch_break_and_resume_matches_continuous(tmp_path: Path, device: str):
+    # Continuous baseline for 1 epoch
+    mA = _model(device)
+    optA = torch.optim.SGD(mA.parameters(), lr=1e-2)
+    loss = nn.MSELoss()
+    cfgA = UnifiedTrainingConfig(epochs=1, lr=1e-2, deterministic=True, device=device)
+    run_unified_training(cfgA, model=mA, optimizer=optA, loss_fn=loss, train_loader=_loader(device=device))
+    vA = _param_vec(mA)
+
+    # Break after a few batches, then resume mid-epoch and finish
+    mB = _model(device)
+    optB = torch.optim.SGD(mB.parameters(), lr=1e-2)
+    ckdir = tmp_path / "mid"
+    cfgB1 = UnifiedTrainingConfig(
+        epochs=1,
+        lr=1e-2,
+        deterministic=True,
+        device=device,
+        break_after_batches=2,     # break after two batches
+        mid_epoch_ckpt_dir=str(ckdir),
+    )
+    hist = run_unified_training(cfgB1, model=mB, optimizer=optB, loss_fn=loss, train_loader=_loader(device=device))
+    assert hist.get("interrupted") is True
+    resume_path = hist["resume_path"]
+
+    cfgB2 = UnifiedTrainingConfig(
+        epochs=1,
+        lr=1e-2,
+        deterministic=True,
+        device=device,
+        resume_from=str(resume_path),
+    )
+    run_unified_training(cfgB2, model=mB, optimizer=optB, loss_fn=loss, train_loader=_loader(device=device))
+    vB = _param_vec(mB)
+
+    assert torch.allclose(vA, vB, atol=1e-8, rtol=0.0)
+
diff --git a/.pre-commit-config.yaml b/.pre-commit-config.yaml
index 9a8a1c5..ad3d2a1 100644
--- a/.pre-commit-config.yaml
+++ b/.pre-commit-config.yaml
@@ -3,6 +3,10 @@ repos:
   - repo: local
     hooks:
       - id: ruff-optional
         name: ruff (optional)
         entry: bash -lc 'command -v ruff >/dev/null 2>&1 && ruff . || echo "[ruff] skipped (not installed)"'
         language: system
         types: [python]
         pass_filenames: false
+      - id: mypy-optional
+        name: mypy (optional)
+        entry: bash -lc 'command -v mypy >/dev/null 2>&1 && mypy src || echo "[mypy] skipped (not installed)"'
+        language: system
+        types: [python]
+        pass_filenames: false
       - id: scan-secrets
         name: scan-secrets
         entry: python tools/scan_secrets.py --fail-on-find
@@ -14,3 +18,7 @@ repos:
         entry: python tools/validate_fences.py --paths README.md docs
         language: system
         types: [text]
         pass_filenames: false
+      - id: check-imports
+        name: check-imports
+        entry: python tools/check_imports.py
+        language: system
diff --git a/CHANGELOG_CODEX.md b/CHANGELOG_CODEX.md
index cda3a72..3a5c0a1 100644
--- a/CHANGELOG_CODEX.md
+++ b/CHANGELOG_CODEX.md
@@ -1,5 +1,34 @@
 # Codex Changelog
+## 2025-10-07 â Repo-wide legacy-import guard, checkpoint meta schema, mid-epoch resume parity
+
+### WHY
+- Enforce migration to the canonical tokenization API; prevent backsliding to `tokenization.*`.
+- Standardize checkpoint metadata with a minimal schema and validator.
+- Prove **mid-epoch** resume equivalence (step-level parity), not just epoch boundaries.
+
+### Changes
+- Lint/Guards:
+  - `pyproject.toml` with Ruff banned-api for `tokenization` + mypy config.
+  - `tools/check_imports.py` and test `tests/lint/test_no_legacy_tokenization_imports.py`.
+- Checkpointing:
+  - `checkpointing/schema.py` with `SCHEMA_VERSION` and validators.
+  - `checkpoint_core.py` now injects and validates `schema_version` on save/load.
+- Training:
+  - `UnifiedTrainingConfig`: `break_after_batches`/`mid_epoch_ckpt_dir`.
+  - `run_unified_training` supports **mid-epoch checkpointing & resume** by skipping completed batches.
+  - Test `tests/training/test_mid_epoch_resume_equivalence.py`.
+
+### Risk
+- Lowâmoderate: mid-epoch options are opt-in; default behavior unchanged.
+- Lint/guard is local-only; tests will surface any legacy import regressions.
+
+### Rollback
+- Revert `pyproject.toml`, guard script/test, and mid-epoch additions; keep schema validator as non-breaking.
```
````diff
diff --git a/src/codex_ml/checkpointing/schema.py b/src/codex_ml/checkpointing/schema.py
index 3f1c0f1..8c4b2d0 100644
--- a/src/codex_ml/checkpointing/schema.py
+++ b/src/codex_ml/checkpointing/schema.py
@@ -1,26 +1,45 @@
 from __future__ import annotations
 from typing import Any, Dict
-SCHEMA_VERSION = 1
+# v2 adds optional flags to record presence of scheduler/AMP scaler
+SCHEMA_VERSION = 2
 REQUIRED = {
     "epoch": int,   # >= 0
 }
 OPTIONAL = {
     "loss": (int, float),
+    "has_scheduler": bool,
+    "has_amp_scaler": bool,
 }
 def validate_meta(meta: Dict[str, Any]) -> None:
     if not isinstance(meta, dict):
         raise ValueError("meta must be a dict")
     # required
     for k, tp in REQUIRED.items():
         if k not in meta:
             raise ValueError(f"meta missing required key: {k}")
         if not isinstance(meta[k], tp):
             raise ValueError(f"meta[{k!r}] must be {tp}")
     if meta["epoch"] < 0:
         raise ValueError("meta['epoch'] must be >= 0")
     # optional types
     for k, tp in OPTIONAL.items():
         if k in meta and not isinstance(meta[k], tp):
             raise ValueError(f"meta[{k!r}] must be {tp}")
+    # schema_version sanity (if present)
+    if "schema_version" in meta and meta["schema_version"] not in (1, 2):
+        raise ValueError("meta['schema_version'] must be 1 or 2")
 def ensure_schema(meta: Dict[str, Any]) -> Dict[str, Any]:
     out = dict(meta)
-    out.setdefault("schema_version", SCHEMA_VERSION)
+    # prefer newer version tag
+    out["schema_version"] = SCHEMA_VERSION
     return out
diff --git a/src/codex_ml/checkpointing/checkpoint_core.py b/src/codex_ml/checkpointing/checkpoint_core.py
index 2e3f4c9..5e7a11a 100644
--- a/src/codex_ml/checkpointing/checkpoint_core.py
+++ b/src/codex_ml/checkpointing/checkpoint_core.py
@@ -1,13 +1,15 @@
 from __future__ import annotations
 from pathlib import Path
 import json
 import hashlib
 from typing import Any, Dict, Tuple, Optional
 from .schema import validate_meta, ensure_schema
+try:
+    import torch
+except Exception:  # pragma: no cover
+    torch = None  # type: ignore
@@
 def save_checkpoint(out_dir: Path | str, *, state: Dict[str, Any], meta: Dict[str, Any], keep_last_k: int = 5) -> Path:
     out = Path(out_dir)
     out.mkdir(parents=True, exist_ok=True)
     meta2 = ensure_schema(meta)
     validate_meta(meta2)
     out.joinpath("meta.json").write_text(json.dumps(meta2, ensure_ascii=False, indent=2), encoding="utf-8")
-    # state + digest writing (existing logic) ...
+    # Save state tensors via torch.save if available, else JSON fallback for pure-JSON states
+    if torch is not None:
+        torch.save(state, out / "state.pt")  # pragma: no cover (I/O)
+    else:  # extremely rare, keep compatibility in constrained envs
+        (out / "state.json").write_text(json.dumps(state, ensure_ascii=False), encoding="utf-8")
     # ... existing digest calculation + retention policy ...
     return out
@@
 def load_checkpoint(src_dir: Path | str, map_location: Optional[str] = None) -> Tuple[Dict[str, Any], Dict[str, Any]]:
     p = Path(src_dir)
-    state = {}  # existing torch.load(...) or equivalent
+    state: Dict[str, Any] = {}
+    # load state
+    if (p / "state.pt").exists() and torch is not None:
+        state = torch.load(p / "state.pt", map_location=map_location)  # type: ignore[arg-type]
+    elif (p / "state.json").exists():
+        state = json.loads((p / "state.json").read_text(encoding="utf-8"))
     meta = json.loads(p.joinpath("meta.json").read_text(encoding="utf-8"))
     validate_meta(meta)
     return state, meta
diff --git a/src/codex_ml/training/unified_training.py b/src/codex_ml/training/unified_training.py
index 2a8a9b1..dba4e54 100644
--- a/src/codex_ml/training/unified_training.py
+++ b/src/codex_ml/training/unified_training.py
@@ -8,6 +8,7 @@ try:
     import torch
     from torch import nn
     from torch.backends import cudnn as _cudnn  # type: ignore[attr-defined]
+    from torch.cuda.amp import GradScaler  # type: ignore
 except Exception as e:  # pragma: no cover
     torch = None  # type: ignore
     nn = None  # type: ignore
@@ def run_unified_training(
     scheduler=None,
     grad_accum_steps: int = 1,
     **kwargs: Any,
 ):
@@
-    if cfg.resume_from:
+    if cfg.resume_from:
         state, meta = load_checkpoint(cfg.resume_from, map_location=cfg.device)
         if "model" in state:
             model.load_state_dict(state["model"])
         if "optimizer" in state:
             optimizer.load_state_dict(state["optimizer"])
+        if scheduler is not None and "scheduler" in state:
+            try:
+                scheduler.load_state_dict(state["scheduler"])
+            except Exception:
+                pass
+        scaler = kwargs.get("scaler", None)
+        if scaler is not None and "amp_scaler" in state:
+            try:
+                scaler.load_state_dict(state["amp_scaler"])
+            except Exception:
+                pass
         if "rng" in state:
             apply_rng_state(state["rng"])
@@
-        if cfg.checkpoint_dir:
-            out_dir = Path(cfg.checkpoint_dir) / f"epoch-{epoch:04d}"
-            meta = {"epoch": epoch, "loss": epoch_loss}
-            save_checkpoint(out_dir, state={"model": model.state_dict(), "optimizer": optimizer.state_dict()}, meta=meta, keep_last_k=cfg.keep_last_k)
+        if cfg.checkpoint_dir:
+            out_dir = Path(cfg.checkpoint_dir) / f"epoch-{epoch:04d}"
+            ck_state: Dict[str, Any] = {
+                "model": model.state_dict(),
+                "optimizer": optimizer.state_dict(),
+            }
+            if scheduler is not None:
+                try:
+                    ck_state["scheduler"] = scheduler.state_dict()
+                except Exception:
+                    pass
+            if scaler is not None and hasattr(scaler, "state_dict"):
+                try:
+                    ck_state["amp_scaler"] = scaler.state_dict()
+                except Exception:
+                    pass
+            meta = {
+                "epoch": epoch,
+                "loss": epoch_loss,
+                "has_scheduler": bool(scheduler is not None),
+                "has_amp_scaler": bool(scaler is not None),
+            }
+            save_checkpoint(out_dir, state=ck_state, meta=meta, keep_last_k=cfg.keep_last_k)
@@
-            if cfg.mid_epoch_ckpt_dir:
-                out_dir = Path(cfg.mid_epoch_ckpt_dir) / f"epoch-{epoch:04d}_step-{i:04d}"
-                meta = {"epoch": epoch, "batch_index": i}
-                save_checkpoint(out_dir, state={"model": model.state_dict(), "optimizer": optimizer.state_dict()}, meta=meta, keep_last_k=cfg.keep_last_k)
+            if cfg.mid_epoch_ckpt_dir:
+                # unified naming: under epoch directory with step subdir
+                out_dir = Path(cfg.mid_epoch_ckpt_dir) / f"epoch-{epoch:04d}" / f"step-{i:04d}"
+                ck_state: Dict[str, Any] = {
+                    "model": model.state_dict(),
+                    "optimizer": optimizer.state_dict(),
+                }
+                if scheduler is not None:
+                    try:
+                        ck_state["scheduler"] = scheduler.state_dict()
+                    except Exception:
+                        pass
+                scaler = kwargs.get("scaler", None)
+                if scaler is not None and hasattr(scaler, "state_dict"):
+                    try:
+                        ck_state["amp_scaler"] = scaler.state_dict()
+                    except Exception:
+                        pass
+                meta = {
+                    "epoch": epoch,
+                    "batch_index": i,
+                    "has_scheduler": bool(scheduler is not None),
+                    "has_amp_scaler": bool(scaler is not None),
+                }
+                save_checkpoint(out_dir, state=ck_state, meta=meta, keep_last_k=cfg.keep_last_k)
                 history.setdefault("interrupted", True)
                 history.setdefault("resume_path", str(out_dir))
                 # break the epoch loop early
                 running_loss += float(loss.detach().cpu().item())
                 n_batches += 1
                 return history
diff --git a/tests/checkpointing/test_scheduler_amp_schema.py b/tests/checkpointing/test_scheduler_amp_schema.py
new file mode 100644
index 0000000..2f5d2c1
--- /dev/null
+++ b/tests/checkpointing/test_scheduler_amp_schema.py
@@ -0,0 +1,87 @@
+import json
+from pathlib import Path
+import pytest
+
+torch = pytest.importorskip("torch")
+nn = torch.nn
+
+from codex_ml.training import UnifiedTrainingConfig, run_unified_training
+from codex_ml.checkpointing import load_checkpoint
+
+
+def _loader(n=64, batch=16, device="cpu"):
+    x = torch.linspace(-1, 1, steps=n, device=device).reshape(-1, 1)
+    y = 1.0 * x - 0.1
+    ds = torch.utils.data.TensorDataset(x, y)
+    return torch.utils.data.DataLoader(ds, batch_size=batch, shuffle=False)
+
+
+def _model(device="cpu"):
+    torch.manual_seed(42)
+    return nn.Sequential(nn.Linear(1, 4), nn.ReLU(), nn.Linear(4, 1)).to(device)
+
+
+def test_checkpoint_includes_scheduler_and_amp_scaler(tmp_path: Path):
+    device = "cuda" if torch.cuda.is_available() else "cpu"
+    model = _model(device)
+    opt = torch.optim.SGD(model.parameters(), lr=1e-2)
+    sched = torch.optim.lr_scheduler.StepLR(opt, step_size=1, gamma=0.9)
+    scaler = torch.cuda.amp.GradScaler(enabled=False)  # works on CPU with enabled=False
+
+    ckdir = tmp_path / "ck"
+    cfg = UnifiedTrainingConfig(epochs=1, lr=1e-2, deterministic=True, device=device, checkpoint_dir=str(ckdir))
+    run_unified_training(cfg, model=model, optimizer=opt, loss_fn=nn.MSELoss(), train_loader=_loader(device=device), scheduler=sched, scaler=scaler)
+
+    # Load the saved checkpoint
+    ep0 = ckdir / "epoch-0000"
+    state, meta = load_checkpoint(ep0, map_location=device)
+    assert "scheduler" in state
+    assert "amp_scaler" in state
+    assert meta.get("has_scheduler") is True
+    assert meta.get("has_amp_scaler") is True
+    assert meta.get("schema_version") in (1, 2)
+
+    # Restore into fresh objects and compare keys
+    model2 = _model(device)
+    opt2 = torch.optim.SGD(model2.parameters(), lr=1e-2)
+    sched2 = torch.optim.lr_scheduler.StepLR(opt2, step_size=1, gamma=0.9)
+    scaler2 = torch.cuda.amp.GradScaler(enabled=False)
+    sched2.load_state_dict(state["scheduler"])
+    scaler2.load_state_dict(state["amp_scaler"])
+    assert set(sched2.state_dict().keys()) == set(state["scheduler"].keys())
+    assert set(scaler2.state_dict().keys()) == set(state["amp_scaler"].keys())
diff --git a/tests/training/test_mid_epoch_naming.py b/tests/training/test_mid_epoch_naming.py
new file mode 100644
index 0000000..7b7b7b7
--- /dev/null
+++ b/tests/training/test_mid_epoch_naming.py
@@ -0,0 +1,38 @@
+from pathlib import Path
+import pytest
+
+torch = pytest.importorskip("torch")
+nn = torch.nn
+
+from codex_ml.training import UnifiedTrainingConfig, run_unified_training
+
+
+def _loader(n=64, batch=16, device="cpu"):
+    x = torch.linspace(-1, 1, steps=n, device=device).reshape(-1, 1)
+    y = 0.5 * x + 0.2
+    ds = torch.utils.data.TensorDataset(x, y)
+    return torch.utils.data.DataLoader(ds, batch_size=batch, shuffle=False)
+
+
+def _model(device="cpu"):
+    torch.manual_seed(7)
+    return nn.Sequential(nn.Linear(1, 4), nn.ReLU(), nn.Linear(4, 1)).to(device)
+
+
+def test_mid_epoch_checkpoint_under_epoch_directory(tmp_path: Path):
+    device = "cuda" if torch.cuda.is_available() else "cpu"
+    m = _model(device)
+    opt = torch.optim.SGD(m.parameters(), lr=1e-2)
+    cfg = UnifiedTrainingConfig(
+        epochs=1,
+        lr=1e-2,
+        deterministic=True,
+        device=device,
+        break_after_batches=1,
+        mid_epoch_ckpt_dir=str(tmp_path / "ck"),
+    )
+    run_unified_training(cfg, model=m, optimizer=opt, loss_fn=nn.MSELoss(), train_loader=_loader(device=device))
+    ep_dir = tmp_path / "ck" / "epoch-0000"
+    # step subdir must exist
+    candidates = sorted(p for p in ep_dir.glob("step-*") if p.is_dir())
+    assert candidates, "expected step-* under epoch-0000"
diff --git a/docs/checkpointing_core.md b/docs/checkpointing_core.md
index a17a0d2..2a4d7e9 100644
--- a/docs/checkpointing_core.md
+++ b/docs/checkpointing_core.md
@@ -1,26 +1,34 @@
 # Checkpoint Core
 Canonical APIs:
 ```python
 from codex_ml.checkpointing import save_checkpoint, load_checkpoint, calculate_digest
 ```
## Whatâs saved
-- **state**: `{"model": state_dict, "optimizer": state_dict, "rng": {...}}`
-- **meta**: `{"epoch": int, "loss": float, "...": any JSON-serializable}`
+- **state** (v2): `{"model": ..., "optimizer": ..., "scheduler": ..., "amp_scaler": ..., "rng": {...}}`
+- **meta** (v2): `{"epoch": int, "loss": float, "has_scheduler": bool, "has_amp_scaler": bool, "schema_version": 2, ...}`
Each checkpoint is a directory named `epoch-XXXX` with:
* `state.pt` (torch.save of state dicts)
* `meta.json` (UTF-8 JSON)
* `digest.sha256` (aggregate SHA-256 of files for quick integrity checks)
## Resume semantics
`load_checkpoint(path)` returns `(state, meta)`:
* Load **model** and **optimizer** state dicts if present.
  +- If provided by the caller, restore **scheduler** and **AMP GradScaler** via their `load_state_dict`.
* Restore RNG via `apply_rng_state(state["rng"])`.
* Starting epoch is `meta["epoch"] + 1`, or inferred from the folder name (`epoch-XXXX`).
## Retention
`save_checkpoint(..., keep_last_k=K)` keeps only the most recent `K` checkpoints in the same parent directory.
+## Mid-epoch naming
+When an interruption occurs, mid-epoch checkpoints use a unified naming pattern:
+
+`
+<ckpt_root>/epoch-XXXX/step-YYYY/
+`
+
## Digests
`calculate_digest(dir)` walks the checkpoint directory and computes a stable aggregate hash. Useful for quick equality checks during tests and audits.
## Notes
* All operations are **local-first**.
* No network calls are performed by the core.
  diff --git a/CHANGELOG_CODEX.md b/CHANGELOG_CODEX.md
  index 3a5c0a1..4dc3f60 100644
  --- a/CHANGELOG_CODEX.md
  +++ b/CHANGELOG_CODEX.md
  @@ -1,5 +1,23 @@
# Codex Changelog
+## 2025-10-07 â Checkpoint schema v2 (scheduler/AMP), unified mid-epoch naming, resume support
+
+### WHY
+- Capture **scheduler** and **AMP GradScaler** states in checkpoints to maximize resume parity.
+- Standardize **mid-epoch** checkpoint layout under the epoch directory.
+
+### Changes
+- `checkpointing/schema.py`: bump to `SCHEMA_VERSION=2`, add meta flags, validators.
+- `checkpoint_core.py`: robust `state.pt` save/load.
+- `unified_training.py`: save/load optional `scheduler` + `amp_scaler`; unified mid-epoch layout.
+- Tests: scheduler/AMP schema and mid-epoch naming.
+- Docs: checkpoint core updated.
+
+### Risk
+- Low. Additive; defaults preserved. AMP scaler handling is no-op if not provided.
+
+### Rollback
+- Revert the updated files and remove new tests.
````
````diff
diff --git a/tools/validate_checkpoint.py b/tools/validate_checkpoint.py
new file mode 100755
index 0000000..c8c9b5a
--- /dev/null
+++ b/tools/validate_checkpoint.py
@@ -0,0 +1,173 @@
+#!/usr/bin/env python3
+from __future__ import annotations
+"""
+Validate Codex checkpoint directories against schema v2.
+ - Verifies presence of meta.json and state.{pt|json}
+ - Validates meta fields and schema_version
+ - Optionally reports presence of scheduler/amp_scaler states
+ - Exits non-zero on validation failure
+"""
+import argparse
+import json
+import sys
+from pathlib import Path
+from typing import Any, Dict
+
+try:
+    import torch  # noqa: F401
+except Exception:  # pragma: no cover
+    torch = None  # type: ignore
+
+from codex_ml.checkpointing.schema import validate_meta
+
+
+def _ok(msg: str) -> None:
+    print(f"OK: {msg}")
+
+
+def _fail(msg: str) -> None:
+    print(f"ERROR: {msg}")
+
+
+def validate_ckpt_dir(path: Path) -> int:
+    if not path.exists() or not path.is_dir():
+        _fail(f"{path} is not a directory")
+        return 2
+    meta_p = path / "meta.json"
+    state_pt = path / "state.pt"
+    state_json = path / "state.json"
+    if not meta_p.exists():
+        _fail("missing meta.json")
+        return 2
+    try:
+        meta = json.loads(meta_p.read_text(encoding="utf-8"))
+        validate_meta(meta)
+    except Exception as e:
+        _fail(f"invalid meta.json: {e}")
+        return 2
+    if not state_pt.exists() and not state_json.exists():
+        _fail("missing state.pt (or state.json)")
+        return 2
+    _ok("meta.json present & valid")
+    _ok(f"state file present: {'state.pt' if state_pt.exists() else 'state.json'}")
+    # report optional components
+    has_sched = bool(meta.get("has_scheduler"))
+    has_amp = bool(meta.get("has_amp_scaler"))
+    print(json.dumps({"has_scheduler": has_sched, "has_amp_scaler": has_amp, "schema_version": meta.get("schema_version")}, indent=2))
+    return 0
+
+
+def main(argv=None) -> int:
+    ap = argparse.ArgumentParser(description="Validate Codex checkpoints")
+    ap.add_argument("paths", nargs="+", help="Checkpoint directories (epoch-XXXX or step-YYYY)")
+    args = ap.parse_args(argv)
+    rc = 0
+    for s in args.paths:
+        rc |= validate_ckpt_dir(Path(s))
+    return rc
+
+
+if __name__ == "__main__":
+    sys.exit(main())
diff --git a/docs/checkpoint_schema_v2.md b/docs/checkpoint_schema_v2.md
new file mode 100644
index 0000000..f0f0a11
--- /dev/null
+++ b/docs/checkpoint_schema_v2.md
@@ -0,0 +1,84 @@
+# Checkpoint Schema v2
+
+Schema **v2** extends v1 with optional metadata flags and additional state slots:
+
+## meta.json
+```json
+{
+  "epoch": 0,
+  "loss": 0.1234,
+  "has_scheduler": true,
+  "has_amp_scaler": true,
+  "schema_version": 2
+}
+```
+Constraints:
+- `epoch`: integer, `>= 0` (required)
+- `loss`: number (optional)
+- `has_scheduler`: boolean (optional)
+- `has_amp_scaler`: boolean (optional)
+- `schema_version`: integer; set to `2`
+
+## state
+Stored as `state.pt` (preferred):
+- `model`: `state_dict()`
+- `optimizer`: `state_dict()`
+- `scheduler`: optional `state_dict()`
+- `amp_scaler`: optional `GradScaler.state_dict()`
+- `rng`: (if present) random state blob
+
+> Tip: for AMP parity across resume, persist and restore the **GradScaler** state. This keeps the scale/backoff/growth trajectory consistent.
+
+## Migration
+v1 checkpoints (without new keys) remain valid and pass validation; upgrading is additiveâwrite `schema_version=2` and optional flags when components are present.
+
+## Tools
+Validate one or more checkpoints:
+```bash
+python tools/validate_checkpoint.py path/to/epoch-0000
+```
diff --git a/tests/tools/test_validate_checkpoint.py b/tests/tools/test_validate_checkpoint.py
new file mode 100644
index 0000000..b8c0e11
--- /dev/null
+++ b/tests/tools/test_validate_checkpoint.py
@@ -0,0 +1,49 @@
+import json
+import subprocess
+import sys
+from pathlib import Path
+import pytest
+
+torch = pytest.importorskip("torch")
+nn = torch.nn
+
+from codex_ml.training import UnifiedTrainingConfig, run_unified_training
+
+
+def _loader(n=32, batch=8, device="cpu"):
+    x = torch.linspace(-1, 1, steps=n, device=device).reshape(-1, 1)
+    y = 0.3 * x + 0.2
+    ds = torch.utils.data.TensorDataset(x, y)
+    return torch.utils.data.DataLoader(ds, batch_size=batch, shuffle=False)
+
+
+def test_validate_checkpoint_pass(tmp_path: Path):
+    device = "cuda" if torch.cuda.is_available() else "cpu"
+    model = nn.Sequential(nn.Linear(1, 4), nn.ReLU(), nn.Linear(4, 1)).to(device)
+    opt = torch.optim.SGD(model.parameters(), lr=1e-2)
+    sched = torch.optim.lr_scheduler.StepLR(opt, step_size=1, gamma=0.9)
+    cfg = UnifiedTrainingConfig(epochs=1, lr=1e-2, deterministic=True, device=device, checkpoint_dir=str(tmp_path))
+    run_unified_training(cfg, model=model, optimizer=opt, loss_fn=nn.MSELoss(), train_loader=_loader(device=device), scheduler=sched)
+    ck = tmp_path / "epoch-0000"
+    p = subprocess.run([sys.executable, "tools/validate_checkpoint.py", str(ck)], capture_output=True, text=True)
+    assert p.returncode == 0
+    # the last JSON line should be flags & schema version
+    flags = json.loads(p.stdout.strip().splitlines()[-1])
+    assert flags["schema_version"] in (1, 2)
+    assert flags["has_scheduler"] is True
+
+
+def test_validate_checkpoint_fail(tmp_path: Path):
+    bad = tmp_path / "epoch-0000"
+    bad.mkdir(parents=True)
+    (bad / "meta.json").write_text("{}", encoding="utf-8")
+    p = subprocess.run([sys.executable, "tools/validate_checkpoint.py", str(bad)], capture_output=True, text=True)
+    assert p.returncode != 0
+    assert "invalid meta.json" in (p.stdout + p.stderr)
+
diff --git a/tests/training/test_scheduler_amp_resume_parity.py b/tests/training/test_scheduler_amp_resume_parity.py
new file mode 100644
index 0000000..7b6a7d1
--- /dev/null
+++ b/tests/training/test_scheduler_amp_resume_parity.py
@@ -0,0 +1,119 @@
+import pytest
+
+torch = pytest.importorskip("torch")
+nn = torch.nn
+
+from codex_ml.training import UnifiedTrainingConfig, run_unified_training
+
+
+def _loader(n=256, batch=32, device="cpu"):
+    x = torch.linspace(-1, 1, steps=n, device=device).reshape(-1, 1)
+    y = 1.8 * x - 0.25
+    ds = torch.utils.data.TensorDataset(x, y)
+    return torch.utils.data.DataLoader(ds, batch_size=batch, shuffle=False)
+
+
+def _model(device="cpu"):
+    torch.manual_seed(2024)
+    return nn.Sequential(nn.Linear(1, 16), nn.ReLU(), nn.Linear(16, 1)).to(device)
+
+
+def _param_vec(m):
+    with torch.no_grad():
+        return torch.cat([p.detach().flatten().cpu() for p in m.parameters()])
+
+
+@pytest.mark.parametrize("device", ["cpu"])
+def test_scheduler_and_amp_resume_mid_epoch_matches_continuous(tmp_path):
+    # baseline 2-epoch continuous with scheduler & dummy GradScaler
+    m0 = _model("cpu")
+    opt0 = torch.optim.SGD(m0.parameters(), lr=5e-3)
+    sched0 = torch.optim.lr_scheduler.StepLR(opt0, step_size=1, gamma=0.9)
+    scaler0 = torch.cuda.amp.GradScaler(enabled=False)  # disabled scaler works on CPU
+    cfg0 = UnifiedTrainingConfig(epochs=2, lr=5e-3, deterministic=True, device="cpu")
+    run_unified_training(cfg0, model=m0, optimizer=opt0, loss_fn=nn.MSELoss(), train_loader=_loader(), scheduler=sched0, scaler=scaler0)
+    v_cont = _param_vec(m0)
+
+    # split: break after 3 batches, checkpoint with scheduler & scaler, then resume to finish epochs
+    m1 = _model("cpu")
+    opt1 = torch.optim.SGD(m1.parameters(), lr=5e-3)
+    sched1 = torch.optim.lr_scheduler.StepLR(opt1, step_size=1, gamma=0.9)
+    scaler1 = torch.cuda.amp.GradScaler(enabled=False)
+    ckroot = tmp_path / "mid"
+    cfg1a = UnifiedTrainingConfig(
+        epochs=2, lr=5e-3, deterministic=True, device="cpu",
+        break_after_batches=3, mid_epoch_ckpt_dir=str(ckroot)
+    )
+    hist = run_unified_training(cfg1a, model=m1, optimizer=opt1, loss_fn=nn.MSELoss(), train_loader=_loader(), scheduler=sched1, scaler=scaler1)
+    resume_path = hist["resume_path"]
+    # resume
+    cfg1b = UnifiedTrainingConfig(epochs=2, lr=5e-3, deterministic=True, device="cpu", resume_from=str(resume_path))
+    run_unified_training(cfg1b, model=m1, optimizer=opt1, loss_fn=nn.MSELoss(), train_loader=_loader(), scheduler=sched1, scaler=scaler1)
+    v_resume = _param_vec(m1)
+
+    assert torch.allclose(v_cont, v_resume, atol=1e-8, rtol=0.0)
+
diff --git a/src/codex_ml/detectors/checkpoint_core.py b/src/codex_ml/detectors/checkpoint_core.py
new file mode 100644
index 0000000..a3a9c13
--- /dev/null
+++ b/src/codex_ml/detectors/checkpoint_core.py
@@ -0,0 +1,86 @@
+from __future__ import annotations
+from pathlib import Path
+from typing import Any, Dict
+
+def _read(p: Path) -> str:
+    try:
+        return p.read_text(encoding="utf-8", errors="replace")
+    except Exception:
+        return ""
+
+
+def detect_checkpoint_core(project_root: str | Path = ".") -> Dict[str, Any]:
+    """
+    Detects presence of checkpoint core (save/load/digest) and schema v2 features.
+    """
+    root = Path(project_root).resolve()
+    target = None
+    for hint in ("src/codex_ml/checkpointing/checkpoint_core.py", "codex_ml/checkpointing/checkpoint_core.py"):
+        p = root / hint
+        if p.exists():
+            target = p
+            break
+    if target is None:
+        return {"score": 0.0, "signals": {}, "path": None}
+    text = _read(target)
+    sigs = {
+        "save": "def save_checkpoint" in text,
+        "load": "def load_checkpoint" in text,
+        "digest": "def calculate_digest" in text,
+        "schema_v2": "schema_version" in text or "has_scheduler" in text or "has_amp_scaler" in text,
+    }
+    score = 0.0
+    if sigs["save"] and sigs["load"]:
+        score = 0.6
+        if sigs["digest"]:
+            score += 0.2
+        if sigs["schema_v2"]:
+            score += 0.1
+    elif sigs["save"] or sigs["load"]:
+        score = 0.3
+    return {"score": min(1.0, score), "signals": sigs, "path": target.as_posix()}
+
+
+def detect_mid_epoch_capability(project_root: str | Path = ".") -> Dict[str, Any]:
+    """
+    Heuristic for mid-epoch checkpoint support (naming and config knobs).
+    """
+    root = Path(project_root).resolve()
+    target = None
+    for p in root.rglob("unified_training.py"):
+        if "site-packages" in p.as_posix():
+            continue
+        target = p
+        break
+    if target is None:
+        return {"score": 0.0, "signals": {}, "path": None}
+    txt = _read(target)
+    sigs = {
+        "config_knobs": "break_after_batches" in txt and "mid_epoch_ckpt_dir" in txt,
+        "naming": "epoch-" in txt and "step-" in txt,
+        "skip_batches": "resume_skip_batches" in txt or "batch_index" in txt,
+    }
+    score = 0.0
+    if sigs["config_knobs"]:
+        score = 0.5
+        if sigs["naming"]:
+            score += 0.2
+        if sigs["skip_batches"]:
+            score += 0.2
+    return {"score": min(1.0, score), "signals": sigs, "path": target.as_posix()}
diff --git a/src/codex_ml/detectors/__init__.py b/src/codex_ml/detectors/__init__.py
index f7a3f22..af2d4a1 100644
--- a/src/codex_ml/detectors/__init__.py
+++ b/src/codex_ml/detectors/__init__.py
@@ -1,9 +1,13 @@
 """
 Capability detectors.
 """
 from .unified_training import detect_unified_training  # re-export
 from .aggregate import aggregate_scores
+from .checkpoint_core import detect_checkpoint_core, detect_mid_epoch_capability
 __all__ = [
     "detect_unified_training",
     "aggregate_scores",
+    "detect_checkpoint_core",
+    "detect_mid_epoch_capability",
 ]
diff --git a/src/codex_ml/detectors/aggregate.py b/src/codex_ml/detectors/aggregate.py
index 3a4c5e7..b1f0b62 100644
--- a/src/codex_ml/detectors/aggregate.py
+++ b/src/codex_ml/detectors/aggregate.py
@@ -7,9 +7,10 @@ This module is intentionally offline-only and reads local source files.
 from dataclasses import asdict, dataclass
 from pathlib import Path
 from typing import Any, Dict, Tuple
-from .unified_training import detect_unified_training
+from .unified_training import detect_unified_training
+from .checkpoint_core import detect_checkpoint_core, detect_mid_epoch_capability
 @dataclass
 class CapabilityScore:
@@ -37,13 +38,18 @@ def aggregate_scores(project_root: str | Path = ".") -> Dict[str, Any]:
     """
     Return a deterministic dict of detector results and a simple overall score.
     """
     root = Path(project_root).resolve()
     unified = CapabilityScore.from_detector(detect_unified_training(root))
+    ckpt = CapabilityScore.from_detector(detect_checkpoint_core(root))
+    mid = CapabilityScore.from_detector(detect_mid_epoch_capability(root))
     # Weighted average if/when other detectors are added; for now just the mean
     parts = {
         "unified_training": unified.to_dict(),
+        "checkpoint_core": ckpt.to_dict(),
+        "mid_epoch": mid.to_dict(),
     }
-    overall = _avg(tuple(p["score"] for p in parts.values()))
+    # simple weighting: unified 0.4, checkpoint 0.4, mid-epoch 0.2
+    overall = float(0.4 * parts["unified_training"]["score"] + 0.4 * parts["checkpoint_core"]["score"] + 0.2 * parts["mid_epoch"]["score"])
     return {
         "scores": parts,
         "score_overall": overall,
         "schema_version": 1,
     }
diff --git a/tests/detectors/test_checkpoint_and_mid_epoch.py b/tests/detectors/test_checkpoint_and_mid_epoch.py
new file mode 100644
index 0000000..c0e4d51
--- /dev/null
+++ b/tests/detectors/test_checkpoint_and_mid_epoch.py
@@ -0,0 +1,52 @@
+from pathlib import Path
+from codex_ml.detectors.checkpoint_core import detect_checkpoint_core, detect_mid_epoch_capability
+from codex_ml.detectors.aggregate import aggregate_scores
+
+
+def _mk(p: Path, text: str):
+    p.parent.mkdir(parents=True, exist_ok=True)
+    p.write_text(text, encoding="utf-8")
+
+
+def test_checkpoint_core_detector(tmp_path: Path):
+    p = tmp_path / "src" / "codex_ml" / "checkpointing" / "checkpoint_core.py"
+    _mk(p, "def save_checkpoint():\n    pass\n\ndef load_checkpoint():\n    pass\n\ndef calculate_digest():\n    pass\nschema_version=2\n")
+    res = detect_checkpoint_core(tmp_path)
+    assert res["score"] >= 0.9
+    assert res["signals"]["digest"] is True
+    assert res["signals"]["schema_v2"] is True
+
+
+def test_mid_epoch_detector(tmp_path: Path):
+    p = tmp_path / "src" / "codex_ml" / "training" / "unified_training.py"
+    _mk(p, "break_after_batches=None\nmid_epoch_ckpt_dir=None\nepoch-\nstep-\nbatch_index=0\nresume_skip_batches=0\n")
+    res = detect_mid_epoch_capability(tmp_path)
+    assert res["score"] >= 0.8
+    assert all(res["signals"].values())
+
+
+def test_aggregate_includes_new_detectors(tmp_path: Path):
+    _mk(tmp_path / "src" / "codex_ml" / "training" / "unified_training.py", "class UnifiedTrainingConfig: pass\ndef run_unified_training(): pass\n")
+    _mk(tmp_path / "src" / "codex_ml" / "checkpointing" / "checkpoint_core.py", "def save_checkpoint(): pass\ndef load_checkpoint(): pass\n")
+    res = aggregate_scores(tmp_path)
+    assert set(res["scores"].keys()) == {"unified_training", "checkpoint_core", "mid_epoch"}
+    assert 0.0 <= res["score_overall"] <= 1.0
diff --git a/docs/detectors.md b/docs/detectors.md
index 3c0d6a2..a8f4a53 100644
--- a/docs/detectors.md
+++ b/docs/detectors.md
@@ -1,9 +1,9 @@
 # Capability Detectors
 Detectors scan the local source tree to infer the presence and **richness** of features (e.g., unified training).
 ## Unified training detector
@@ -29,6 +29,34 @@ Signals:
 - If only one of {config, runner} exists: base 0.30.
+## Checkpoint core detector
+
+Signals:
+- `save`, `load`, `digest`: functions present
+- `schema_v2`: presence of v2-related fields
+
+Scoring:
+- Base 0.60 for `save` + `load`
+- +0.20 if `digest` exists
+- +0.10 if `schema_v2` references found
+
+## Mid-epoch capability detector
+
+Signals:
+- `config_knobs` (`break_after_batches`, `mid_epoch_ckpt_dir`)
+- `naming` (mentions of `epoch-` and `step-`)
+- `skip_batches` (`resume_skip_batches`/`batch_index` logic)
+
+Scoring:
+- 0.50 for knobs
+- +0.20 for naming
+- +0.20 for skip-batch logic
+
 ## Aggregation
@@ -52,4 +80,4 @@ This appends a second line to the README badge:
 <!-- END-CAPABILITY-HASH-BADGE -->
No network access is required; everything runs locally.
diff --git a/CHANGELOG_CODEX.md b/CHANGELOG_CODEX.md
index 4dc3f60..58e9f0a 100644
--- a/CHANGELOG_CODEX.md
+++ b/CHANGELOG_CODEX.md
@@ -1,5 +1,27 @@
# Codex Changelog
+## 2025-10-07 â Checkpoint validator tool, scheduler/AMP resume parity test, detectors expansion
+
+### WHY
+- Provide a **checkpoint validator** for schema v2 compliance.
+- Prove **resume parity** with LR scheduler and AMP GradScaler across mid-epoch restarts.
+- Expand **detector coverage** to checkpoint core and mid-epoch capability; fold into aggregator.
+
+### Changes
+- Tools: `tools/validate_checkpoint.py`
+- Docs: `docs/checkpoint_schema_v2.md`, extended detectors doc
+- Tests:
* * `tests/tools/test_validate_checkpoint.py`
* * `tests/training/test_scheduler_amp_resume_parity.py`
* * `tests/detectors/test_checkpoint_and_mid_epoch.py`
    +- Detectors:
* * `src/codex_ml/detectors/checkpoint_core.py`
* * `src/codex_ml/detectors/__init__.py` and `aggregate.py` updated
*
+### Risk
+- Low. All additions are optional tools/tests; detectors are read-only scans.
+
+### Rollback
+- Remove new files and revert aggregator changes.
````
````diff
diff --git a/src/codex_ml/cli.py b/src/codex_ml/cli.py
index a5fb3d1..e6bd5a2 100644
--- a/src/codex_ml/cli.py
+++ b/src/codex_ml/cli.py
@@ -1,11 +1,13 @@
 """Minimal local/offline CLI for _codex_.
 Usage examples:
   python -m codex_ml hash manifest --write-readme [--include-detectors]
   python -m codex_ml tracking decide --summary tracking_summary.ndjson
+  python -m codex_ml checkpoint validate path/to/epoch-0000 [more paths...]
 """
 import argparse
 import hashlib
 import json
 import os
+import sys
 import time
 from pathlib import Path
 from typing import Dict, Any, Iterable, Tuple
@@ -21,6 +23,7 @@ def sha256_file(path: Path) -> str:
     h.update(path.read_bytes())
     return h.hexdigest()
+# -----------------------------------------------------------------------------
 # Capability digest / README badge helpers
 BADGE_BEGIN = "<!-- BEGIN-CAPABILITY-HASH-BADGE -->"
@@ -28,10 +31,14 @@ BADGE_END = "<!-- END-CAPABILITY-HASH-BADGE -->"
-def _render_badge(digest: str, cap_digest: str | None = None) -> str:
+def _render_badge(digest: str, cap_digest: str | None = None, notes: Iterable[str] | None = None) -> str:
     lines = [
         BADGE_BEGIN,
         f"Integrity: `{digest}` (SHA256)",
     ]
     if cap_digest:
         lines.append(f"Capabilities: `{cap_digest}` (SHA256)")
+    if notes:
+        note_s = ", ".join(str(n) for n in notes if n)
+        if note_s:
+            lines.append(f"Notes: {note_s}")
     lines.append(BADGE_END)
     return "\n".join(lines) + "\n"
@@ -61,7 +68,7 @@ def _update_readme_badge(readme: Path, digest: str, *, cap_digest: str | None = None) -> None:
     if start != -1 and end != -1 and end > start:
-        new = content[:start] + _render_badge(digest, cap_digest) + content[end:]
+        new = content[:start] + _render_badge(digest, cap_digest, _PENDING_NOTES) + content[end:]
     else:
-        new = content + "\n" + _render_badge(digest, cap_digest)
+        new = content + "\n" + _render_badge(digest, cap_digest, _PENDING_NOTES)
     readme.write_text(new, encoding="utf-8")
@@ -95,7 +102,7 @@ def _cmd_hash_manifest(args: argparse.Namespace) -> int:
     # compute manifest digest (simple example: combine file hashes of src/ and docs/)
     files = list(Path(".").rglob("*"))
     file_hashes = [sha256_file(p) for p in files if p.is_file()]
     digest = hashlib.sha256(("".join(sorted(file_hashes))).encode("utf-8")).hexdigest()
-    cap_digest = None
+    cap_digest, notes = None, []
     if args.include_detectors:
         from codex_ml.detectors.aggregate import aggregate_scores
         agg = aggregate_scores(Path("."))
@@ -105,11 +112,22 @@ def _cmd_hash_manifest(args: argparse.Namespace) -> int:
         # normalize to stable string and hash it
         payload = json.dumps(agg, sort_keys=True, separators=(",", ":")).encode("utf-8")
         cap_digest = hashlib.sha256(payload).hexdigest()
+        # compute optional badge notes (e.g., ckpt=v2)
+        try:
+            ck = agg["scores"]["checkpoint_core"]
+            if ck and ck.get("signals", {}).get("schema_v2"):
+                notes.append("ckpt=v2")
+        except Exception:
+            pass
+        global _PENDING_NOTES
+        _PENDING_NOTES = notes
     if args.write_readme:
         _update_readme_badge(root / "README.md", digest, cap_digest=cap_digest)
     return 0
+_PENDING_NOTES: list[str] = []  # captured for README badge rendering
+
 def _append_ndjson(path: Path, obj: Dict[str, object]) -> None:
     path.parent.mkdir(parents=True, exist_ok=True)
@@ -143,6 +161,25 @@ def _cmd_tracking_decide(args: argparse.Namespace) -> int:
     return 0
+def _cmd_checkpoint_validate(args: argparse.Namespace) -> int:
+    """
+    Validate one or more checkpoint directories.
+    """
+    try:
+        # Prefer importing tool function directly (works with namespace pkg)
+        from tools.validate_checkpoint import validate_ckpt_dir  # type: ignore
+    except Exception as e:  # pragma: no cover
+        print(f"Unable to import validator: {e}", file=sys.stderr)
+        return 2
+    rc = 0
+    for s in args.paths:
+        rc |= validate_ckpt_dir(Path(s))
+    return rc
+
+
 def _cmd_tracking(env: argparse.Namespace) -> int:
     raise SystemExit(2)
@@ -173,6 +210,17 @@ def _build_parser() -> argparse.ArgumentParser:
     ap_decide.add_argument("--print", action="store_true", help="Print decision JSON to stdout")
     ap_decide.set_defaults(func=_cmd_tracking_decide)
+    # checkpoint
+    ap_ck = sub.add_parser("checkpoint", help="Checkpoint utilities")
+    ap_ck_sub = ap_ck.add_subparsers(dest="checkpoint_cmd")
+    ap_ck.set_defaults(func=lambda _: ap_ck.print_help() or 0)
+    ap_ck_validate = ap_ck_sub.add_parser("validate", help="Validate checkpoint directories")
+    ap_ck_validate.add_argument("paths", nargs="+", help="Checkpoint directories (epoch-XXXX or step-YYYY)")
+    ap_ck_validate.set_defaults(func=_cmd_checkpoint_validate)
+
     return ap
 def main(argv=None) -> int:
     ap = _build_parser()
     args = ap.parse_args(argv)
@@ -183,3 +231,4 @@ def main(argv=None) -> int:
     return int(args.func(args) or 0)
+__all__ = ["main"]
diff --git a/tests/cli/test_cli_checkpoint_validate.py b/tests/cli/test_cli_checkpoint_validate.py
new file mode 100644
index 0000000..2c6d0a1
--- /dev/null
+++ b/tests/cli/test_cli_checkpoint_validate.py
@@ -0,0 +1,44 @@
+import subprocess
+import sys
+from pathlib import Path
+import pytest
+
+torch = pytest.importorskip("torch")
+nn = torch.nn
+
+from codex_ml.training import UnifiedTrainingConfig, run_unified_training
+
+
+def _loader(n=32, batch=8, device="cpu"):
+    x = torch.linspace(-1, 1, steps=n, device=device).reshape(-1, 1)
+    y = 0.2 * x + 0.1
+    ds = torch.utils.data.TensorDataset(x, y)
+    return torch.utils.data.DataLoader(ds, batch_size=batch, shuffle=False)
+
+
+def test_cli_checkpoint_validate_ok(tmp_path: Path):
+    device = "cpu"
+    m = nn.Sequential(nn.Linear(1, 4), nn.ReLU(), nn.Linear(4, 1)).to(device)
+    opt = torch.optim.SGD(m.parameters(), lr=1e-2)
+    cfg = UnifiedTrainingConfig(epochs=1, lr=1e-2, deterministic=True, device=device, checkpoint_dir=str(tmp_path))
+    run_unified_training(cfg, model=m, optimizer=opt, loss_fn=nn.MSELoss(), train_loader=_loader(device=device))
+    ck = tmp_path / "epoch-0000"
+    p = subprocess.run([sys.executable, "-m", "codex_ml", "checkpoint", "validate", str(ck)], capture_output=True, text=True)
+    assert p.returncode == 0
+    assert "OK: meta.json present & valid" in p.stdout
+
diff --git a/tests/cli/test_cli_manifest_notes.py b/tests/cli/test_cli_manifest_notes.py
new file mode 100644
index 0000000..0b77c78
--- /dev/null
+++ b/tests/cli/test_cli_manifest_notes.py
@@ -0,0 +1,37 @@
+import subprocess
+import sys
+from pathlib import Path
+
+
+def test_readme_badge_notes_ckpt_v2(tmp_path: Path, monkeypatch):
+    # Minimal repo with checkpoint core referencing schema_v2-ish tokens
+    ck = tmp_path / "src" / "codex_ml" / "checkpointing" / "checkpoint_core.py"
+    ck.parent.mkdir(parents=True, exist_ok=True)
+    ck.write_text("def save_checkpoint(): pass\n" "def load_checkpoint(): pass\n" "def calculate_digest(): pass\n" "schema_version=2\n" "has_scheduler=True\n", encoding="utf-8")
+    readme = tmp_path / "README.md"
+    readme.write_text("# Test\n", encoding="utf-8")
+    # run CLI
+    p = subprocess.run(
+        [sys.executable, "-m", "codex_ml", "hash", "manifest", "--write-readme", "--include-detectors"],
+        cwd=str(tmp_path),
+        capture_output=True,
+        text=True,
+    )
+    assert p.returncode == 0
+    txt = readme.read_text(encoding="utf-8")
+    assert "Notes: ckpt=v2" in txt
+
diff --git a/docs/resume_cookbook.md b/docs/resume_cookbook.md
new file mode 100644
index 0000000..f1af1d2
--- /dev/null
+++ b/docs/resume_cookbook.md
@@ -0,0 +1,148 @@
+# Resume Cookbook
+
+This page shows practical patterns for resuming training with the unified faÃ§ade. All flows are **local-first** and deterministic-friendly.
+
+## 1) Continuous run (no resume)
+```python
+cfg = UnifiedTrainingConfig(epochs=2, lr=1e-2, deterministic=True, device="cpu")
+hist = run_unified_training(cfg, model, optimizer, nn.MSELoss(), train_loader)
+```
+
+## 2) Epoch-boundary resume
+Save a checkpoint at the end of each epoch, then resume later:
+```python
+cfg1 = UnifiedTrainingConfig(epochs=1, lr=1e-2, deterministic=True, device="cpu", checkpoint_dir="ck")
+run_unified_training(cfg1, model, optimizer, nn.MSELoss(), train_loader)
+
+resume_dir = "ck/epoch-0000"
+cfg2 = UnifiedTrainingConfig(epochs=2, lr=1e-2, deterministic=True, device="cpu", resume_from=resume_dir)
+run_unified_training(cfg2, model, optimizer, nn.MSELoss(), train_loader)
+```
+
+## 3) Mid-epoch resume (step-level parity)
+Use unified naming under the epoch directory and skip completed batches on resume:
+```python
+cfgA = UnifiedTrainingConfig(
+    epochs=1, lr=1e-2, deterministic=True, device="cpu",
+    break_after_batches=3, mid_epoch_ckpt_dir="ck"
+)
+hist = run_unified_training(cfgA, model, optimizer, nn.MSELoss(), train_loader)
+resume_dir = hist["resume_path"]  # ck/epoch-0000/step-0002
+
+cfgB = UnifiedTrainingConfig(epochs=1, lr=1e-2, deterministic=True, device="cpu", resume_from=resume_dir)
+run_unified_training(cfgB, model, optimizer, nn.MSELoss(), train_loader)
+```
+
+## 4) Scheduler and AMP scaler state
+To preserve LR schedules and mixed-precision scaler dynamics across resume:
+```python
+opt = torch.optim.SGD(model.parameters(), lr=1e-2)
+sched = torch.optim.lr_scheduler.StepLR(opt, step_size=1, gamma=0.9)
+scaler = torch.cuda.amp.GradScaler(enabled=False)  # works on CPU too
+
+cfg = UnifiedTrainingConfig(epochs=2, lr=1e-2, deterministic=True, device="cpu", checkpoint_dir="ck")
+run_unified_training(cfg, model, opt, nn.MSELoss(), train_loader, scheduler=sched, scaler=scaler)
+```
+Checkpoints will include `scheduler` and `amp_scaler` states and set `has_scheduler` / `has_amp_scaler` in `meta.json`.
+
+## 5) Determinism posture (quick checklist)
+- `torch.use_deterministic_algorithms(True)` to reject nondeterministic kernels.
+- On CUDA, set `torch.backends.cudnn.deterministic=True` and `torch.backends.cudnn.benchmark=False`.
+- Fix seeds for model init and loaders.
+
+## 6) Validating checkpoints
+```bash
+python -m codex_ml checkpoint validate ck/epoch-0000
+```
+
+## 7) Troubleshooting
+- If a resume changes loss trajectory:
+  - Verify optimizer/scheduler/scaler states are restored.
+  - Confirm deterministic posture (and hardware parity).
+  - Ensure dataset ordering/sharding and seeds are identical.
diff --git a/docs/index.md b/docs/index.md
index 20b7f8a..0f4561b 100644
--- a/docs/index.md
+++ b/docs/index.md
@@ -1,8 +1,9 @@
 # Documentation Index
 - **Architecture**: `docs/architecture.md`
 - **Unified Training**: `docs/unified_training.md`
+- **Resume Cookbook**: `docs/resume_cookbook.md`
 - **Tokenizer Benchmark**: `tools/bench_tokenizer.py`
 - **Checkpoint Core**: `docs/checkpointing_core.md`
 - **Tracking & Observability**: `docs/observability.md`, `docs/observability_metrics.md`
 - **Tokenization API**: `docs/tokenization_api.md`
diff --git a/README.md b/README.md
index 2d8a3f4..6e8d1c2 100644
--- a/README.md
+++ b/README.md
@@ -54,6 +54,13 @@ See `docs/cli.md` for more.
 ### CLI snippets
+- **Validate checkpoints**
+```bash
+python -m codex_ml checkpoint validate ck/epoch-0000
+```
+
+- **Hash + badge (with detectors)**
 ```bash
 python -m codex_ml hash manifest --write-readme --include-detectors
````
@@ -68,6 +75,9 @@ See `docs/cli.md` for more.
* Fence validation: `python tools/validate_fences.py --paths README.md docs`
* NDJSON summarize: `python tools/ndjson_summarize.py run.ndjson -o run.csv`
+**Badge notes**
+When detectors find checkpoint **schema v2**, the README badge includes `Notes: ckpt=v2`.
+
See `docs/quality_gates.md` for details and references (NDJSON spec, PyTorch determinism, MLflow/W&B posture).
diff --git a/CHANGELOG_CODEX.md b/CHANGELOG_CODEX.md
index 58e9f0a..c7b0b92 100644
--- a/CHANGELOG_CODEX.md
+++ b/CHANGELOG_CODEX.md
@@ -1,5 +1,24 @@
# Codex Changelog
+## 2025-10-07 â CLI checkpoint validate, badge notes (ckpt=v2), resume cookbook
+
+### WHY
+- Provide a first-class CLI for **checkpoint validation**.
+- Surface detector insight in the README badge via **Notes: ckpt=v2**.
+- Document practical **resume patterns** (continuous, epoch, mid-epoch; scheduler/AMP).
+
+### Changes
+- CLI: `checkpoint validate` subcommand integrates the validator tool.
+- README: badge rendering supports notes; adds usage snippets.
+- Docs: new `docs/resume_cookbook.md`; index updated.
+- Tests: CLI validation smoke and badge notes assertions.
+
+### Risk
+- Low. Additive commands and docs; badge note appears only when detectors signal schema v2.
+
+### Rollback
+- Revert CLI changes and remove added tests/docs.
````
```diff
diff --git a/src/codex_ml/cli.py b/src/codex_ml/cli.py
index e6bd5a2..9f34ad7 100644
--- a/src/codex_ml/cli.py
+++ b/src/codex_ml/cli.py
@@ -135,6 +135,33 @@ def _cmd_tracking_decide(args: argparse.Namespace) -> int:
     env = os.environ.copy()
     decision = decide_mlflow_tracking_uri(environ=env)
     payload = {
         "ts": time.time(),
         "blocked": decision.blocked,
         "uri": decision.uri,
         "reason": decision.reason,
         "details": decision.details,
     }
+    # --- W&B posture (offline / disabled) with project-local override WANDB_ENABLE=1
+    wandb_mode = env.get("WANDB_MODE", "").strip().lower()
+    wandb_disabled = str(env.get("WANDB_DISABLED", "")).strip().lower() in {"1", "true", "yes"}
+    wandb_enable_override = str(env.get("WANDB_ENABLE", "")).strip() == "1"
+    wb_reason = None
+    effective_offline = False
+    effective_disabled = False
+    if wandb_enable_override:
+        wb_reason = "override:WANDB_ENABLE=1"
+        effective_offline = False
+        effective_disabled = False
+    else:
+        if wandb_mode == "disabled" or wandb_disabled:
+            effective_disabled = True
+            wb_reason = "WANDB_MODE=disabled or WANDB_DISABLED=true"
+        elif wandb_mode == "offline":
+            effective_offline = True
+            wb_reason = "WANDB_MODE=offline"
+        else:
+            wb_reason = "default-online"
+    payload["wandb"] = {
+        "mode": wandb_mode or None,
+        "disabled_env": wandb_disabled,
+        "effective_offline": effective_offline,
+        "effective_disabled": effective_disabled,
+        "reason": wb_reason,
+    }
     if args.summary:
         _append_ndjson(Path(args.summary), payload)
     if args.print:
         print(json.dumps(payload, ensure_ascii=False))
     return 0
diff --git a/tests/cli/test_cli_wandb_matrix.py b/tests/cli/test_cli_wandb_matrix.py
new file mode 100644
index 0000000..3f6bc7e
--- /dev/null
+++ b/tests/cli/test_cli_wandb_matrix.py
@@ -0,0 +1,91 @@
+import json
+import os
+import subprocess
+import sys
+import pytest
+
+
+def _run(env):
+    p = subprocess.run(
+        [sys.executable, "-m", "codex_ml", "tracking", "decide", "--print"],
+        capture_output=True,
+        text=True,
+        env=env,
+    )
+    assert p.returncode == 0
+    line = p.stdout.strip().splitlines()[-1]
+    return json.loads(line)
+
+
+@pytest.mark.parametrize(
+    "env_updates, expect_offline, expect_disabled",
+    [
+        ({"WANDB_MODE": "offline"}, True, False),
+        ({"WANDB_MODE": "disabled"}, False, True),
+        ({"WANDB_DISABLED": "true"}, False, True),
+        ({}, False, False),
+    ],
+)
+def test_wandb_offline_disabled_matrix(env_updates, expect_offline, expect_disabled):
+    env = os.environ.copy()
+    env.update(env_updates)
+    out = _run(env)
+    wb = out["wandb"]
+    assert wb["effective_offline"] is expect_offline
+    assert wb["effective_disabled"] is expect_disabled
+
+
+@pytest.mark.parametrize(
+    "env_updates",
+    [
+        {"WANDB_MODE": "offline", "WANDB_ENABLE": "1"},
+        {"WANDB_MODE": "disabled", "WANDB_ENABLE": "1"},
+        {"WANDB_DISABLED": "true", "WANDB_ENABLE": "1"},
+    ],
+)
+def test_wandb_enable_override_forces_online(env_updates):
+    env = os.environ.copy()
+    env.update(env_updates)
+    out = _run(env)
+    wb = out["wandb"]
+    assert wb["effective_offline"] is False
+    assert wb["effective_disabled"] is False
+    assert wb["reason"].startswith("override:")
diff --git a/tests/checkpointing/test_retention_and_digest.py b/tests/checkpointing/test_retention_and_digest.py
new file mode 100644
index 0000000..4f6d2a0
--- /dev/null
+++ b/tests/checkpointing/test_retention_and_digest.py
@@ -0,0 +1,77 @@
+from pathlib import Path
+import json
+import time
+import pytest
+
+torch = pytest.importorskip("torch")
+nn = torch.nn
+
+from codex_ml.checkpointing import save_checkpoint, load_checkpoint, calculate_digest
+
+
+def _touch(p: Path):
+    p.write_text(p.read_text(encoding="utf-8") + "\n# tamper", encoding="utf-8")
+
+
+def test_keep_last_k_and_digest(tmp_path: Path):
+    # create 4 checkpoints, keep only last 2
+    root = tmp_path / "ck"
+    root.mkdir(parents=True, exist_ok=True)
+    for e in range(4):
+        out = root / f"epoch-{e:04d}"
+        save_checkpoint(
+            out,
+            state={"model": {"w": e}, "optimizer": {"step": e}},
+            meta={"epoch": e, "loss": float(10 - e)},
+            keep_last_k=2,
+        )
+        # call retention at parent level; save_checkpoint is assumed to enforce retention per parent
+    # only epoch-0002 and epoch-0003 should remain
+    kids = sorted(p.name for p in root.iterdir() if p.is_dir())
+    assert kids == ["epoch-0002", "epoch-0003"]
+    # digest should be stable and change if files are tampered
+    d_before = calculate_digest(root / "epoch-0003")
+    assert isinstance(d_before, str) and len(d_before) >= 32
+    # Tamper with meta to force digest change
+    _touch(root / "epoch-0003" / "meta.json")
+    d_after = calculate_digest(root / "epoch-0003")
+    assert d_before != d_after
```

