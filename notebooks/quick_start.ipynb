{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e165f02",
   "metadata": {},
   "source": [
    "# Codex Quickâ€‘Start (Offline)\n",
    "\n",
    "This notebook trains a tiny model on a synthetic dataset and logs metrics to TensorBoard. It is designed to run **offline** on CPU in under two minutes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7ecdba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from transformers import DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
    "\n",
    "from codex_ml.modeling.codex_model_loader import load_model_with_optional_lora\n",
    "from codex_ml.tokenization.train_tokenizer import TrainTokenizerConfig\n",
    "from codex_ml.tokenization.train_tokenizer import run as train_tokenizer\n",
    "from interfaces.tokenizer import HFTokenizer\n",
    "\n",
    "random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "print(\"PyTorch\", torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454e9af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Build a tiny in-memory corpus\n",
    "texts = [\"hello world\", \"foo bar\", \"lorem ipsum\"] * 100\n",
    "Path(\"runs\").mkdir(exist_ok=True)\n",
    "with open(\"runs/tiny_corpus.txt\", \"w\", encoding=\"utf-8\") as fh:\n",
    "    fh.write(\"\\n\".join(texts))\n",
    "texts[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45f23a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Train a tokenizer from the corpus (offline)\n",
    "cfg = TrainTokenizerConfig(\n",
    "    input_file=\"runs/tiny_corpus.txt\", output_dir=\"runs/tokenizer\", vocab_size=50\n",
    ")\n",
    "train_tokenizer(cfg)\n",
    "tk = HFTokenizer(\n",
    "    name_or_path=None,\n",
    "    artifacts_dir=\"runs/tokenizer\",\n",
    "    max_length=64,\n",
    "    padding=\"max_length\",\n",
    "    truncation=True,\n",
    ")\n",
    "print(\"Vocab size\", tk.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93351af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Prepare datasets\n",
    "\n",
    "ds = Dataset.from_dict({\"text\": texts})\n",
    "\n",
    "\n",
    "def encode(batch):\n",
    "    ids = tk.batch_encode(batch[\"text\"])\n",
    "    return {\"input_ids\": ids, \"labels\": ids}\n",
    "\n",
    "\n",
    "tokenized = ds.map(encode, batched=True, remove_columns=[\"text\"])\n",
    "split = tokenized.train_test_split(test_size=0.2, seed=0)\n",
    "train_ds, val_ds = split[\"train\"], split[\"test\"]\n",
    "len(train_ds), len(val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b476e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) Load a tiny decoder-only model\n",
    "model = load_model_with_optional_lora(\n",
    "    \"decoder_only\",\n",
    "    model_config={\n",
    "        \"vocab_size\": tk.vocab_size,\n",
    "        \"d_model\": 32,\n",
    "        \"n_heads\": 4,\n",
    "        \"n_layers\": 2,\n",
    "        \"max_seq_len\": 64,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a151b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) Train with HF Trainer\n",
    "collator = DataCollatorForLanguageModeling(tk._tk, mlm=False)\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"runs/quickstart\",\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    max_steps=20,\n",
    "    eval_steps=10,\n",
    "    logging_steps=5,\n",
    "    save_steps=20,\n",
    "    report_to=[\"tensorboard\"],\n",
    "    logging_dir=\"runs/tb\",\n",
    "    remove_unused_columns=False,\n",
    ")\n",
    "trainer = Trainer(\n",
    "    model=model, args=args, train_dataset=train_ds, eval_dataset=val_ds, data_collator=collator\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7d1129",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6) Evaluate and log perplexity\n",
    "import csv\n",
    "\n",
    "metrics = trainer.evaluate()\n",
    "ppl = math.exp(metrics[\"eval_loss\"])\n",
    "with open(\"runs/eval.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"metric\", \"value\"])\n",
    "    writer.writerow([\"perplexity\", ppl])\n",
    "print(\"Perplexity\", ppl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "712cca79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7) Inspect generated checkpoints\n",
    "os.listdir(\"runs/quickstart\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61392d35",
   "metadata": {},
   "source": [
    "TensorBoard logs are written under `runs/tb`.\n",
    "Launch with:\n",
    "\n",
    "```\n",
    "tensorboard --logdir runs/tb\n",
    "```\n",
    "\n",
    "Remove the `runs/` directory to clean up.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
