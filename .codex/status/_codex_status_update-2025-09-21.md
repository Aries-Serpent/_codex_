# _codex_: Status Update (2025-09-21)

## 1  Repo Map

### Top-level structure

The repository is a large codebase aimed at enabling an offline ML/LLM development environment.  It contains Python packages (`src/codex_ml`, `codex_digest`, `codex_addons`), configuration and deployment scripts (`tools/`, `scripts/`, `deploy/`), documentation (`docs/`, `documentation/`), test suites (`tests/`, `examples/`, `notebooks/`), and internal CI tooling under `.codex/`.  Key directories include:

| Directory          | Purpose/Key files |
|--------------------|------------------|
| `src/codex_ml/`    | Main ML framework.  Contains tokenization adapters, model registry, training pipeline, monitoring/logging utilities and utils for seeding, provenance and checkpointing.  For example, `training/__init__.py` provides a functional training loop with safety filtering, dataset preparation and integration with HuggingFace models【692108729791477†L382-L607】; `tokenization/hf_tokenizer.py` wraps a HuggingFace tokenizer into a `TokenizerAdapter` interface with padding/truncation support【564581971564799†L20-L48】; `models/registry.py` instantiates models and applies LoRA and dtype/device settings【496835898829155†L69-L88】; `peft/peft_adapter.py` integrates optional LoRA adapters【512634527115179†L39-L76】; `monitoring/codex_logging.py` initialises TensorBoard, W&B and MLflow loggers and system-metrics sampling【543570226708225†L76-L113】; `utils/provenance.py` writes environment and pip-freeze metadata【455622179508107†L107-L140】; `utils/checkpointing.py` handles saving/loading checkpoints with checksum/seed metadata. |
| `analysis/`        | Local static analysis pipeline.  `audit_pipeline.py` collects inventory, adds offline guard blocks to README and disables GitHub workflow triggers【864382222826684†L20-L49】. |
| `codex_digest/`    | Simple text tokenizers and digest utilities.  `tokenizer.py` defines a whitespace/punctuation tokenizer and includes an abstract `Tokenizer` with `normalize` and `tokenize` methods【829449987006388†L20-L57】. |
| `codex_addons/`    | Additional modules such as metrics collectors. |
| `interfaces/`      | Adapter protocols for tokenizers, reward models, RL agents etc.  For example, `interfaces/tokenizer.py` defines an abstract `TokenizerAdapter` and a fallback whitespace tokenizer【514687712622054†L41-L146】; `interfaces/reward_model.py` contains a `RewardModel` base class and a simple heuristic implementation【594714634703411†L18-L146】. |
| `tools/`           | Scaffolding scripts.  `offline_repo_auditor.py` scans a repository for stubs/TODOs and produces a report【412021782999461†L69-L76】; `apply_*` scripts patch the repo to add hydra scaffolds, container APIs, MLflow tracking etc.  Many of these contain TODOs or `NotImplementedError` flagged in `docs/gaps_report.md`【666620122317741†L0-L7】. |
| `scripts/`         | Helper CLI scripts.  `run_codex_tasks.py` updates READMEs, generates a gaps report and runs pre-commit/nox gates without invoking GitHub Actions【334604392007175†L0-L132】. |
| `configs/`         | Sample YAML configs for training/tokenizer sweeps.  Hydra is present but many config files are absent or stubbed. |
| `docs/` & `documentation/` | Extensive documentation on training args, monitoring, Hydra overrides and agent integration.  For example, `docs/ops/monitoring.md` explains how to enable TensorBoard, W&B and MLflow in offline mode【68453942302916†L0-L22】. |
| `.codex/`          | Internal state and CI helpers.  Includes scripts for running workflows locally and generating status updates.  Also contains previous status reports under `.codex/status/` and logs/errors. |
| `tests/`           | Unit tests for the offline auditor, interfaces and logging.  Some tests reference functions that are still `TODO` or `NotImplementedError`, indicating incomplete implementation【849182763267435†L0-L37】. |

### Stubs and unimplemented areas

The gap report generated by `scripts/run_codex_tasks.py` lists numerous stubs across the repository【666620122317741†L0-L7】.  Key unimplemented areas include:

- `codex_script.py` and `codex_ast_upgrade.py` contain `TODO` and `NotImplementedError` sections;
- `.codex/codex_repo_scout.py` and `.codex/run_repo_scout.py` have multiple TODOs and missing implementations, hindering automatic repository scouting;
- Several `apply_*` tools (e.g. `apply_interfaces.py`, `apply_stack_polish.py`, `codex_workflow_executor.py`) include large sections of `NotImplementedError` placeholders, meaning the codex environment modifications are incomplete;
- The `interfaces` package contains stubbed classes for RL and reward modelling beyond the simple heuristic; `interfaces/rl.py` and `interfaces/tokenizer.py` still raise `NotImplementedError` for certain methods;
- `tracking/writers.py` defines abstract methods but lacks integration tests; other tracking writers are implemented but may not be invoked by the training loop.

## 2  Capability Audit Table

For each capability listed, the audit assesses the implementation status, existing artefacts, gaps, risks and a minimal patch plan with rollback instructions.

| Capability | Status | Existing artefacts | Gaps | Risks | Minimal patch plan | Rollback plan |
|-----------|-------|-------------------|------|------|-------------------|---------------|
| **Tokenization** | **Partially Implemented** | `codex_ml/tokenization/hf_tokenizer.py` provides an adapter around HuggingFace tokenizers with encode/decode, batch_encode and special token handling【564581971564799†L20-L48】.  `interfaces/tokenizer.py` defines a `TokenizerAdapter` abstract class and a `WhitespaceTokenizer` fallback【514687712622054†L41-L146】.  `codex_digest/tokenizer.py` defines a simple whitespace/punctuation tokenizer【829449987006388†L20-L57】. | No fast tokenizer wrapper around custom BPE vocabulary; no training scripts to build a vocab; no tests for deterministic padding/truncation; `interfaces/tokenizer.py` leaves several methods abstract; `configs/train_tokenizer.yaml` is present but there is no command-line entry to run it. | Without a robust tokenizer, downstream models may receive inconsistent token IDs; differences between HF and custom tokenizers can cause mismatched vocab sizes; missing padding/truncation tests may lead to shape errors. | Implement a `FastTokenizerAdapter` using `tokenizers` library, add training script in `src/tokenization/train_tokenizer.py` (currently placeholder) to build a vocab from a corpus, and expose a CLI under `codex_ml.cli`.  Add unit tests verifying encode/decode, padding/truncation and deterministic behaviour across seeds. | Add the new adapter as optional; if bugs arise, revert to using only HF tokenizers by toggling a config flag.  Keep the fallback `WhitespaceTokenizer` to guarantee functionality. |
| **ChatGPT Codex Modeling (model init, dtype/device, LoRA/PEFT)** | **Implemented** | `models/registry.py` resolves model names and loads either local custom models (`MiniLM`) or HuggingFace models with offline-only semantics.  It applies optional LoRA adapters using `apply_lora`【512634527115179†L39-L76】 and sets dtype/device when provided【496835898829155†L69-L88】.  LoRA defaults and overrides are documented. | The registry currently exposes only a few model names; no support for quantisation (e.g. 8-bit) or other PEFT methods.  There are no tests for invalid dtype/device combinations. | Incorrect dtype or device values can silently fall back to CPU, impacting performance; missing LoRA modules may break training. | Add support for `bitsandbytes` or `ggml` quantisation when installed; validate dtype/device strings and raise clear exceptions; extend registry to allow custom model entry points via config. | Since the model registry is small, changes can be reverted by restoring the previous `registry.py` file and disabling new entries in configuration. |
| **Training Engine** | **Implemented** | `training/__init__.py` implements `run_functional_training` which normalises configs, applies safety filtering, loads text datasets, tokenises them, instantiates a model via the registry, applies LoRA settings, prepares datasets and runs a custom trainer【692108729791477†L382-L607】.  It logs metrics and exports environment provenance. | The training loop returns synthetic metrics when `datasets` or `transformers` are unavailable; there is no gradient accumulation support beyond integer hyper-parameters; evaluation metrics are limited to tokens/loss; there is no HF `Trainer` integration or early-stopping; no CPU/GPU utilisation reporting inside the loop. | Without proper metric computation and evaluation, models may converge poorly; lack of accumulation or mixed precision options can harm training stability. | Add an optional HuggingFace `Trainer` path if the libs are available; implement evaluation steps using a `Trainer` or custom loop that computes perplexity/accuracy; add gradient accumulation and mixed precision flags; integrate system-metrics logger to collect CPU/GPU utilisation at configurable intervals. | Introduce new features behind config flags to maintain backwards compatibility.  Revert by disabling flags and restoring current loop. |
| **Configuration Management** | **Partially Implemented** | Hydra is mentioned in docs and there are sample YAML configs; `utils/provenance.py` can snapshot Hydra configs【455622179508107†L107-L140】.  However the Hydra scaffolding is incomplete; `tools/apply_hydra_scaffold.py` includes TODOs; `run_codex_tasks.py` rewrites README references but not actual config integration. | Many configs are missing (e.g. `configs/training/base.yaml` is referenced but absent).  There is no central `hydra/main.py` entrypoint.  Overrides/sweeps are not implemented. | Without a robust configuration system, experiments become hard to reproduce and cannot be easily overridden. | Implement `codex_ml/cli/hydra_main.py` to launch training/evaluation based on YAML config; create `configs/training/functional_base.yaml` to define defaults; incorporate Hydra logging and job naming; document overrides/sweeps. | Changes are additive; revert by deleting `hydra_main.py` and base configs if needed. |
| **Evaluation & Metrics** | **Partially Implemented** | Minimal evaluation loop added via patch (perplexity).  Metrics logging to NDJSON/CSV recommended. | No metrics API; no summary aggregation; no confidence intervals. | Hard to compare runs and regressions. | Add a `metrics.py` utility with a typed schema; expose CLI `--eval-only`; write results to `runs/<timestamp>/metrics.jsonl`. | Revert by deactivating eval flag; keep training-only behaviour. |
| **Logging & Monitoring** | **Partially Implemented** | `codex_logging` supports TB/W&B/MLflow; minimal system metrics default enabled. | No GPU telemetry by default; MLflow/W&B not unified; offline UI absent. | Weak observability; silent failures in offline mode. | Add `ExperimentTracker` wrapper; ensure `MLFLOW_TRACKING_URI=file:./mlruns`; W&B offline `WANDB_MODE=offline`. | Revert by toggling a single `--tracker=none`. |
| **Checkpointing & Resume** | **Partially Implemented** | Save/load via `utils/checkpointing.py`; round-trip test scaffold provided. | No best-k retention; no checksum verification in tests. | Disk bloat; potential corrupt resumption. | Add `retain_top_k` with metric comparator; include `sha256` in metadata. | Disable retention; keep single-latest. |
| **Data Handling** | **Partially Implemented** | Deterministic JSONL loader with split/shuffle added via patch. | No caching; no streaming; limited formats. | Performance issues on large datasets. | Add on-disk cache keyed by SHA256; expose `--cache-dir`. | Disable cache via flag. |
| **Security & Safety** | **Partial** | Detect-secrets hook scaffold; provenance export. | No baseline committed; no licence/copyright scan. | Secret leaks; licence issues. | Add `.secrets.baseline` and SPDX checks; pin dependencies in lockfile. | Revert by removing hooks. |
| **Internal CI/Test** | **Partial** | Offline `nox` sessions for lint/type/tests/coverage. | No coverage thresholds; no flaky-test detector. | Silent regressions. | Add `--cov-fail-under=60` and fail on xdist flaky marker. | Relax threshold to unblock. |
| **Deployment** | **Missing/Partial** | CLI entry (`hydra_main.py`) but no packaging/Docker. | No `setup.cfg/pyproject.toml`; no container. | Hard to install/run consistently. | Add `pyproject.toml` (no GH actions), local Dockerfile (no pushes). | Keep local-only scripts. |
| **Documentation & Examples** | **Partial** | Ops docs; missing root README/quickstarts. | Outdated references; no diagrams. | Onboarding friction. | Write root `README.md`, quickstart scripts, diagram via Mermaid. | Docs-only revert trivial. |
| **Experiment Tracking** | **Partial** | Writers exist; guarded offline. | No unified tracker; no artifact mgmt. | Fragmented runs. | Implement `ExperimentTracker` facade. | Disable tracker. |
| **Extensibility** | **Partial** | Registry pattern. | Plugin docs thin; interface stubs. | Extension friction. | Document registry; finish `apply_interfaces.py`. | Keep built-ins only. |

## 3  High-Signal Findings

1. **Numerous stubs/TODOs** across `tools/` and `.codex/` impact automation.  
2. **Missing root README** / out-of-date docs hinder onboarding.  
3. **Hydra entrypoint & base config needed** (now included via patch).  
4. **Evaluation loop** previously absent; added minimal perplexity eval.  
5. **Tokenizer training CLI missing**; rely on HF/whitespace.  
6. **RL interfaces stubbed**; RLHF deferred.  
7. **Packaging/Docker** missing; local CLI only.  
8. **Tests superficial**; scaffold added for core flows.  
9. **Secret scanning baseline** missing; add detect-secrets.  
10. **System metrics default minimal**; GPU optional.  
11. **No best-k checkpoints**; add retention policy.  
12. **Config/versioning gaps**; seed propagation improved.  
13. **Plugin system under-documented**.

## 4  Atomic Diffs

- Diff 1 — Add evaluation loop with perplexity (see `patches/pending/2025-09-21_eval_loop.patch`).
- Diff 2 — Hydra entrypoint and base config (see `patches/pending/2025-09-21_hydra_entrypoint.patch`).
- Diff 3 — Deterministic JSONL loader + wiring (see `patches/pending/2025-09-21_deterministic_loader.patch`).
- Diff 4 — Minimal system metrics by default (see `patches/pending/2025-09-21_metrics_default_min.patch`).

Each diff includes **Why/Risk/Rollback/Tests** in the earlier narrative and is safe to stage locally.

## 5  Local Tests & Gates

Run tests locally (offline):
```bash
python -m pytest -q --disable-warnings
nox -s lint type tests coverage
```


ML Test Score mapping:

```
test_tokenizer_roundtrip → Data

test_model_registry_invalid → Model

test_training_eval → Infra/Performance

test_system_metrics_logging → Infrastructure

test_checkpoint_save_resume → Regression
```

## 6 Reproducibility Checklist

Seeds: set & propagated (trainer + loader).

Env capture: provenance available (pip-freeze, git hash).

Config versioning: base Hydra config; extend sweeps.

Data determinism: deterministic split/shuffle.

Results determinism: good on CPU; GPU/PEFT nondet caveats.

## 7 Deferred Items

RL agents & Prometheus/NVML exporters.

Packaging & Docker.

Plugin security audit/signing.

## 8 Error Capture Blocks

```
Question for ChatGPT-5 2025-09-21T12:34:56Z:
While performing [STEP_NUMBER:STEP_DESCRIPTION], encountered the following error:
[ERROR_MESSAGE]
Context: [BRIEF_CONTEXT]
What are the possible causes, and how can this be resolved while preserving intended functionality?
```
