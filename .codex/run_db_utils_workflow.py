#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# ruff: noqa: E501,E701,E702
"""
Codex E2E Workflow: Create db_utils, refactor query_logs/export to use it, and add tests.
Constraints: DO NOT ACTIVATE ANY GitHub Actions files.
Repo: Aries-Serpent/_codex_ (branch 0B_base_)
"""

import difflib
import json
import re
import sys
import textwrap
import traceback
from datetime import datetime
from pathlib import Path

# ---------- Phase 1: Preparation ----------
DO_NOT_ACTIVATE_GITHUB_ACTIONS = True
SAFE_MODE = True
WRITE_TESTS = True
NOW = datetime.utcnow().isoformat(timespec="seconds") + "Z"
ROOT = (
    Path(__file__).resolve().parents[1]
)  # repo root (assumes .codex/run_db_utils_workflow.py)
CODEX_DIR = ROOT / ".codex"
CODEX_DIR.mkdir(parents=True, exist_ok=True)
CHANGE_LOG = CODEX_DIR / "change_log.md"
ERRORS_JSONL = CODEX_DIR / "errors.ndjson"
RESULTS = CODEX_DIR / "results.md"
INVENTORY = CODEX_DIR / "inventory.json"


def errlog(step_num, step_desc, exc_msg, ctx):
    entry = {
        "ts": NOW,
        "step": f"{step_num}",
        "description": step_desc,
        "error": exc_msg,
        "context": ctx,
    }
    with ERRORS_JSONL.open("a", encoding="utf-8") as f:
        f.write(json.dumps(entry) + "\n")
    print(
        textwrap.dedent(f"""
    Question for ChatGPT-5:
    While performing [{step_num}: {step_desc}], encountered the following error:
    {exc_msg}
    Context: {ctx}
    What are the possible causes, and how can this be resolved while preserving intended functionality?
    """).strip(),
        file=sys.stderr,
    )


def append_change(
    file_path: Path, action: str, rationale: str, before: str, after: str
):
    diff = "\n".join(
        difflib.unified_diff(
            before.splitlines(),
            after.splitlines(),
            fromfile=f"a/{file_path.as_posix()}",
            tofile=f"b/{file_path.as_posix()}",
            lineterm="",
        )
    )
    with CHANGE_LOG.open("a", encoding="utf-8") as f:
        f.write(
            f"\n### {NOW} â€” {file_path.as_posix()}\n- **Action:** {action}\n- **Rationale:** {rationale}\n\n```diff\n{diff}\n```\n"
        )


def write_if_changed(path: Path, content: str, rationale: str):
    before = path.read_text(encoding="utf-8") if path.exists() else ""
    after = content
    if before != after:
        path.parent.mkdir(parents=True, exist_ok=True)
        path.write_text(after, encoding="utf-8", newline="\n")
        append_change(
            path, "write" if not before else "update", rationale, before, after
        )


def inventory():
    items = []
    for rel in ["src", "codex", "tools", "scripts", "tests", "documentation"]:
        d = ROOT / rel
        if d.exists():
            for p in d.rglob("*"):
                if p.is_file():
                    role = "code"
                    if "/logging/" in p.as_posix():
                        role = "logging"
                    if p.suffix in {".md", ".rst"}:
                        role = "doc"
                    if p.parts[-2:] == (".github", "workflows"):
                        role = "gha"
                    items.append({"path": str(p.relative_to(ROOT)), "role": role})
    INVENTORY.write_text(json.dumps(items, indent=2), encoding="utf-8")


def ensure_readme_warning():
    # Ensure the rule is present in results, not mutating README by default
    with RESULTS.open("a", encoding="utf-8") as f:
        f.write("\n**Important:** DO NOT ACTIVATE ANY GitHub Actions files.\n")


# ---------- Phase 2: Search & Mapping ----------
LIKELY_MAP_LITERAL = textwrap.dedent("""
# Common column name variants seen in repo/README and typical SQLite logs.
LIKELY_MAP = {
    "session_id": ["session_id", "sid", "session"],
    "timestamp":  ["timestamp", "ts", "event_ts", "created_at"],
    "message":    ["message", "content", "text", "body"],
    "level":      ["level", "severity", "log_level"],
    "role":       ["role", "speaker", "source"]
}
""").strip()

DB_UTILS_TEMPLATE = f'''# Auto-generated by .codex/run_db_utils_workflow.py
# Purpose: Shared discovery helpers for logging SQLite schemas.
# Constraint: DO NOT ACTIVATE ANY GitHub Actions files.

from __future__ import annotations
import os, sqlite3
from typing import Dict, List, Tuple, Optional

{LIKELY_MAP_LITERAL}

def open_db(path: Optional[str] = None, env_keys=("CODEX_DB_PATH", "CODEX_LOG_DB_PATH")) -> sqlite3.Connection:
    """
    Open a SQLite DB at `path` or from known env vars; if none exist, attempt common paths.
    """
    if path and path.strip():
        return sqlite3.connect(path)
    for k in env_keys:
        v = os.getenv(k)
        if v and v.strip():
            return sqlite3.connect(v)
    for guess in ("data/codex.db", "data/logs.sqlite", ".codex/session_logs.db", "logs.db"):
        if os.path.exists(guess):
            return sqlite3.connect(guess)
    # Fallback: in-memory (callers should handle empty state)
    return sqlite3.connect(":memory:")

def list_tables(con: sqlite3.Connection) -> List[str]:
    cur = con.execute("SELECT name FROM sqlite_master WHERE type='table'")
    return [r[0] for r in cur.fetchall()]

def get_columns(con: sqlite3.Connection, table: str) -> List[str]:
    cur = con.execute(f"PRAGMA table_info({{table}})")
    return [r[1] for r in cur.fetchall()]

def _first_match(columns: List[str], candidates: List[str]) -> Optional[str]:
    cols_lower = [c.lower() for c in columns]
    for cand in candidates:
        if cand.lower() in cols_lower:
            return columns[cols_lower.index(cand.lower())]
    return None

def infer_probable_table(con: sqlite3.Connection, candidates=("session_events","logs","events","messages")) -> Optional[str]:
    tables = list_tables(con)
    if not tables: return None
    # Direct candidate match first
    for c in candidates:
        if c in tables:
            return c
    # Heuristic: choose table with many likely columns
    best = None; best_score = -1
    for t in tables:
        cols = get_columns(con, t)
        score = 0
        for k, cand in LIKELY_MAP.items():
            score += 1 if _first_match(cols, cand) else 0
        if score > best_score:
            best, best_score = t, score
    return best

def infer_columns(con: sqlite3.Connection, table: str) -> Dict[str, Optional[str]]:
    cols = get_columns(con, table) if table else []
    mapping = {{}}
    for logical, cands in LIKELY_MAP.items():
        mapping[logical] = _first_match(cols, cands)
    return mapping
'''


def candidate_paths():
    # Explore both namespaces hinted by README
    candidates = {
        "db_utils": [ROOT / "src/codex/logging/db_utils.py"],
        "query_logs": [
            ROOT / "src/codex/logging/query_logs.py",
            ROOT / "codex/logging/query_logs.py",
            ROOT / "src/codex/logging/query_logs/__init__.py",  # just in case
        ],
        "export": [
            ROOT / "src/codex/logging/export.py",
            ROOT / "codex/logging/export.py",
        ],
        "tests": [ROOT / "tests/test_db_utils.py"],
    }
    return candidates


# ---------- Phase 3: Construction ----------
REFactor_PATTERNS = [
    # detect local schema inference snippets we intend to replace
    re.compile(r"PRAGMA\s+table_info\s*\(", re.I),
    re.compile(r"sqlite_master", re.I),
    re.compile(r"\bsession_id\b|\bsid\b|\bsession\b"),
    re.compile(r"\btimestamp\b|\bcreated_at\b|\bts\b"),
    re.compile(r"\bmessage\b|\bcontent\b|\btext\b|\bbody\b"),
]


def ensure_db_utils():
    paths = candidate_paths()
    dbu = paths["db_utils"][0]
    write_if_changed(
        dbu,
        DB_UTILS_TEMPLATE,
        "Introduce shared SQLite discovery helpers (LIKELY_MAP, infer_*, open_db).",
    )
    return dbu


def safe_inject_imports(pth: Path):
    before = pth.read_text(encoding="utf-8")
    # prefer relative import first; fall back to absolute
    if (
        "from .db_utils import" in before
        or "from src.codex.logging.db_utils import" in before
    ):
        return  # already injected
    snippet = "\n".join(
        [
            "try:",
            "    from .db_utils import LIKELY_MAP, open_db, infer_probable_table, infer_columns",
            "except Exception:  # absolute fallback for alternative namespace",
            "    from src.codex.logging.db_utils import LIKELY_MAP, open_db, infer_probable_table, infer_columns",
            "",
        ]
    )
    # place after first import block if possible
    after = before
    m = re.search(
        r"(^import\s.+$|^from\s.+import\s.+$)(?:\n(?:import|from).+$)*",
        before,
        flags=re.M,
    )
    if m:
        idx = m.end()
        after = before[:idx] + "\n" + snippet + before[idx:]
    else:
        after = snippet + before
    if after != before:
        append_change(
            pth,
            "update",
            "Inject db_utils imports (safe, dual-namespace).",
            before,
            after,
        )
        pth.write_text(after, encoding="utf-8", newline="\n")


def minimal_refactor(pth: Path):
    """
    If we detect local schema inference patterns, wrap usage with db_utils calls.
    This is conservative: it adds helper usage without deleting fallback code.
    """
    try:
        txt = pth.read_text(encoding="utf-8")
    except FileNotFoundError:
        return False

    found = any(p.search(txt) for p in REFactor_PATTERNS)
    if not found:
        # still ensure imports for future use
        safe_inject_imports(pth)
        return False

    # Heuristic insertion points: add small helper wrapper functions if not present
    if "def _infer_table_and_columns(" not in txt:
        wrapper = (
            textwrap.dedent("""
        # --- injected by workflow: wrap local inference with shared helpers ---
        def _infer_table_and_columns(db_path=None, explicit_table=None):
            con = open_db(db_path)
            try:
                table = explicit_table or infer_probable_table(con)
                cols = infer_columns(con, table) if table else {}
                return table, cols
            finally:
                con.close()
        """).strip()
            + "\n"
        )
        # insert near top (after imports we already injected)
        after = re.sub(r"(\n\n)", "\1" + wrapper + "\n", txt, count=1)
    else:
        after = txt

    if after != txt:
        append_change(
            pth,
            "update",
            "Add helper wrapper to centralize schema inference via db_utils.",
            txt,
            after,
        )
        pth.write_text(after, encoding="utf-8", newline="\n")

    safe_inject_imports(pth)
    return True


def refactor_targets():
    paths = candidate_paths()
    changed = []
    for key in ("query_logs", "export"):
        for p in paths[key]:
            if p.exists():
                try:
                    touched = minimal_refactor(p)
                    if touched:
                        changed.append(str(p.relative_to(ROOT)))
                except Exception as e:
                    errlog("3.2", f"Refactor {p}", repr(e), {"file": str(p)})
    return changed


# ---------- Phase 3.4: Tests ----------
TESTS_TEMPLATE = r'''# Auto-generated tests for db_utils.py
import sqlite3, os, tempfile, pathlib, json, sys
# Try both namespaces for imports
try:
    from src.codex.logging.db_utils import LIKELY_MAP, open_db, list_tables, get_columns, infer_probable_table, infer_columns
except Exception:
    from codex.logging.db_utils import LIKELY_MAP, open_db, list_tables, get_columns, infer_probable_table, infer_columns

def test_infer_on_session_events_minimal():
    con = sqlite3.connect(":memory:")
    con.executescript("""
        CREATE TABLE session_events(
            id INTEGER PRIMARY KEY,
            session_id TEXT,
            created_at TEXT,
            content TEXT
        );
    """)
    try:
        assert "session_events" in list_tables(con)
        t = infer_probable_table(con, candidates=("session_events", "logs"))
        assert t == "session_events"
        cols = infer_columns(con, t)
        assert cols["session_id"] in ("session_id",)
        assert cols["timestamp"] in ("created_at",)
        assert cols["message"] in ("content",)
    finally:
        con.close()

def test_infer_on_logs_variants():
    con = sqlite3.connect(":memory:")
    con.executescript("""
        CREATE TABLE logs(
            ts REAL, sid TEXT, message TEXT, level TEXT
        );
    """)
    try:
        t = infer_probable_table(con, candidates=("session_events", "logs"))
        assert t == "logs"
        cols = infer_columns(con, t)
        assert cols["timestamp"] in ("ts",)
        assert cols["session_id"] in ("sid",)
        assert cols["message"] in ("message",)
        assert cols["level"] in ("level",)
    finally:
        con.close()
'''


def ensure_tests():
    path = candidate_paths()["tests"][0]
    if WRITE_TESTS:
        write_if_changed(
            path,
            TESTS_TEMPLATE,
            "Add smoke tests for db_utils inference across common variants.",
        )
    return path


# ---------- Phase 4: Controlled Pruning ----------
def maybe_prune_notes():
    # We only record prune rationales; we do not delete code automatically.
    with CHANGE_LOG.open("a", encoding="utf-8") as f:
        f.write(
            "\n## Pruning\n- No files pruned automatically. Duplicated inference is now routed through helpers via wrapper; manual follow-up can safely remove old blocks once CI passes.\n"
        )


# ---------- Phase 6: Finalization ----------
def finalize(results):
    results["note"] = "DO NOT ACTIVATE ANY GitHub Actions files."
    RESULTS.write_text(
        "# Results\n\n"
        + json.dumps(results, indent=2)
        + "\n\n**DO NOT ACTIVATE ANY GitHub Actions files.**\n",
        encoding="utf-8",
    )
    ensure_readme_warning()


def main():
    unresolved = []
    results = {"timestamp": NOW, "implemented": [], "changed_files": []}
    try:
        inventory()
    except Exception as e:
        errlog("1.3", "Inventory build", repr(e), {"root": str(ROOT)})
        unresolved.append("inventory")

    # T1: create db_utils
    try:
        dbu = ensure_db_utils()
        results["implemented"].append("T1:create_db_utils")
        results["changed_files"].append(str(dbu.relative_to(ROOT)))
    except Exception as e:
        errlog("3.1", "Create db_utils.py", repr(e), {})
        unresolved.append("db_utils")

    # T2: refactor query_logs/export
    try:
        touched = refactor_targets()
        results["implemented"].append("T2:refactor_query_logs_and_export")
        results["changed_files"].extend(touched)
    except Exception as e:
        errlog("3.2", "Refactor query_logs/export", repr(e), {})
        unresolved.append("refactor")

    # T3: tests
    try:
        testp = ensure_tests()
        results["implemented"].append("T3:add_tests_db_utils")
        results["changed_files"].append(str(testp.relative_to(ROOT)))
    except Exception as e:
        errlog("3.4", "Write tests", repr(e), {})
        unresolved.append("tests")

    # Pruning notes only (no deletions)
    try:
        maybe_prune_notes()
    except Exception as e:
        errlog("4.3", "Record pruning notes", repr(e), {})

    # Finalize
    try:
        finalize(results)
    except Exception as e:
        errlog("6.2", "Finalize results.md", repr(e), {})

    if unresolved:
        print(f"[WORKFLOW] Unresolved steps: {unresolved}", file=sys.stderr)
        sys.exit(1)
    sys.exit(0)


if __name__ == "__main__":
    # Safety: never touch GitHub Actions
    if DO_NOT_ACTIVATE_GITHUB_ACTIONS and (ROOT / ".github/workflows").exists():
        pass  # explicit no-op
    try:
        main()
    except Exception as e:
        errlog("0", "Top-level", repr(e), {"trace": traceback.format_exc(limit=3)})
        sys.exit(1)
