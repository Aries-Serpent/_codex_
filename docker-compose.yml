version: "3.9"
services:
  codex-cpu:
    build: .
    image: ${CODEX_IMAGE:-codex:local}
    command: ["codex-infer", "--prompt", "hello from codex"]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      start_period: 10s
      interval: 30s
      timeout: 2s
      retries: 6
    environment:
      MODEL_NAME: ${MODEL_NAME:-sshleifer/tiny-gpt2}
      TOKENIZER_NAME: ${TOKENIZER_NAME:-sshleifer/tiny-gpt2}
      MAX_NEW_TOKENS: ${MAX_NEW_TOKENS:-20}
      API_RATE_LIMIT: 50
    volumes:
      - ./data:/data
      - ./artifacts:/artifacts
  # codex-gpu service is optional; uncomment the lines below if NVIDIA GPUs are available
  # codex-gpu:
  #   build: .
  #   image: ${CODEX_IMAGE:-codex:local}
  #   deploy:
  #     resources:
  #       reservations:
  #         devices:
  #           - capabilities: [gpu]
  #   command: ["codex-infer", "--device", "cuda", "--prompt", "hello gpu"]
  #   environment:
  #     MODEL_NAME: ${MODEL_NAME:-sshleifer/tiny-gpt2}
  #     TOKENIZER_NAME: ${TOKENIZER_NAME:-sshleifer/tiny-gpt2}
  #     MAX_NEW_TOKENS: ${MAX_NEW_TOKENS:-20}
  #   volumes:
  #     - ./data:/data
  #     - ./artifacts:/artifacts
