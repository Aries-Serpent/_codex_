# Experiment: Smoke Training Loop Baseline
- **Date:** 2025-01-15
- **Owner(s):** Automation
- **Related PRs / Issues:** _n/a_
- **Run IDs:** smoke-train-0001
- **Seed(s):** 123

## Goals
- Exercise the functional training loop end-to-end with the new run logging stack.
- Verify dataset split manifests and checksum artefacts are generated for smoke datasets.

## Configuration
- **CLI invocation:** `python -m codex_ml.train_loop --epochs 1 --grad-accum 2`
- **Config snapshot:** See `runs/smoke-train-0001/params.ndjson` (schema `run_params.schema.json`). Key options:
  - learning rate: `3e-4`
  - batch size: `4`
  - gradient accumulation: `2`
- **Derived parameters:**
  - effective batch size: `8`
  - warmup steps: `0`

## Data & Provenance
- **Datasets:** `examples/smoke_train.jsonl` (release `smoke-v1`).
- **Checksums:**
  - `runs/smoke-train-0001/split_manifest.json`
  - `runs/smoke-train-0001/split_checksums.json`
- **Data validation:** schema validation (âœ…) and 40 example rows discovered during ingest.
- **Input revisions:** repository commit `abc1234` (working tree at time of run).

## Results
| Split | Metric | Value |
|-------|--------|-------|
| train | loss   | 1.02  |
| train | acc    | 0.41  |
| val   | loss   | 0.98  |
| val   | acc    | 0.44  |

Full metrics available in `runs/smoke-train-0001/metrics.ndjson`.

## Analysis
- The run confirms metrics logging succeeds with schema validation.
- Checksum manifest hashes match across repeated runs with the same seed.
- No anomalies observed in logs or tensorboard summaries.

## Follow-ups
- [ ] Capture additional validation metrics (BLEU, ROUGE) for smoke runs.
- [ ] Wire smoke run outputs into regression dashboards.

## Artifacts
- `runs/smoke-train-0001/params.ndjson`
- `runs/smoke-train-0001/metrics.ndjson`
- `runs/smoke-train-0001/split_manifest.json`
- `runs/smoke-train-0001/split_checksums.json`
